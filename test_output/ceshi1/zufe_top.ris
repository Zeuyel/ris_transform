TY  - JOUR
AU  - Zhao, Z.
AU  - Cao, L.
AU  - Yu, P.S.
TI  - Out-of-distribution detection by regaining lost clues
PY  - 2025
T2  - Artificial Intelligence
VL  - 339
C7  - 104275
DO  - 10.1016/j.artint.2024.104275
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211984006&doi=10.1016%2fj.artint.2024.104275&partnerID=40&md5=2e5db732f88b746df9f93019d7c884cb
AB  - Out-of-distribution (OOD) detection identifies samples in the test phase that are drawn from distributions distinct from that of training in-distribution (ID) samples for a trained network. According to the information bottleneck, networks that classify tabular data tend to extract labeling information from features with strong associations to ground-truth labels, discarding less relevant labeling cues. This behavior leads to a predicament in which OOD samples with limited labeling information receive high-confidence predictions, rendering the network incapable of distinguishing between ID and OOD samples. Hence, exploring more labeling information from ID samples, which makes it harder for an OOD sample to obtain high-confidence predictions, can address this over-confidence issue on tabular data. Accordingly, we propose a novel transformer chain (TC), which comprises a sequence of dependent transformers that iteratively regain discarded labeling information and integrate all the labeling information to enhance OOD detection. The generalization bound theoretically reveals that TC can balance ID generalization and OOD detection capabilities. Experimental results demonstrate that TC significantly surpasses state-of-the-art methods for OOD detection in tabular data. © 2024 The Author(s)
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, X.
AU  - Liu, X.
AU  - Wang, L.
AU  - Wu, S.
AU  - Su, J.
AU  - Wu, H.
TI  - A simple yet effective self-debiasing framework for transformer models
PY  - 2025
T2  - Artificial Intelligence
VL  - 339
C7  - 104258
DO  - 10.1016/j.artint.2024.104258
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210642199&doi=10.1016%2fj.artint.2024.104258&partnerID=40&md5=90d8f21ff67cceeefae70a2f776eba58
AB  - Current Transformer-based natural language understanding (NLU) models heavily rely on dataset biases, while failing to handle real-world out-of-distribution (OOD) instances. Many methods have been proposed to deal with this issue, but they ignore the fact that the features learned in different layers of Transformer-based NLU models are different. In this paper, we first conduct preliminary studies to obtain two conclusions: 1) both low- and high-layer sentence representations encode common biased features during training; 2) the low-layer sentence representations encode fewer unbiased features than the high-layer ones. Based on these conclusions, we propose a simple yet effective self-debiasing framework for Transformer-based NLU models. Concretely, we first stack a classifier on a selected low layer. Then, we introduce a residual connection that feeds the low-layer sentence representation to the top-layer classifier. In this way, the top-layer sentence representation will be trained to ignore the common biased features encoded by the low-layer sentence representation and focus on task-relevant unbiased features. During inference, we remove the residual connection and directly use the top-layer sentence representation to make predictions. Extensive experiments and in-depth analyses on NLU tasks demonstrate the superiority of our framework, achieving a new state-of-the-art (SOTA) on three OOD test sets. © 2024 Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Shao, R.
AU  - Wu, T.
AU  - Liu, Z.
TI  - Robust Sequential DeepFake Detection
PY  - 2025
T2  - International Journal of Computer Vision
DO  - 10.1007/s11263-024-02339-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214021905&doi=10.1007%2fs11263-024-02339-6&partnerID=40&md5=613ec431319e7c4c05f3e015e159d422
AB  - Since photorealistic faces can be readily generated by facial manipulation technologies nowadays, potential malicious abuse of these technologies has drawn great concerns. Numerous deepfake detection methods are thus proposed. However, existing methods only focus on detecting one-step facial manipulation. As the emergence of easy-accessible facial editing applications, people can easily manipulate facial components using multi-step operations in a sequential manner. This new threat requires us to detect a sequence of facial manipulations, which is vital for both detecting deepfake media and recovering original faces afterwards. Motivated by this observation, we emphasize the need and propose a novel research problem called Detecting Sequential DeepFake Manipulation (Seq-DeepFake). Unlike the existing deepfake detection task only demanding a binary label prediction, detecting Seq-DeepFake manipulation requires correctly predicting a sequential vector of facial manipulation operations. To support a large-scale investigation, we construct the first Seq-DeepFake dataset, where face images are manipulated sequentially with corresponding annotations of sequential facial manipulation vectors. Based on this new dataset, we cast detecting Seq-DeepFake manipulation as a specific image-to-sequence (e.g., image captioning) task and propose a concise yet effective Seq-DeepFake Transformer (SeqFakeFormer). To better reflect real-world deepfake data distributions, we further apply various perturbations on the original Seq-DeepFake dataset and construct the more challenging Sequential DeepFake dataset with perturbations (Seq-DeepFake-P). To exploit deeper correlation between images and sequences when facing Seq-DeepFake-P, a dedicated Seq-DeepFake Transformer with Image-Sequence Reasoning (SeqFakeFormer++) is devised, which builds stronger correspondence between image-sequence pairs for more robust Seq-DeepFake detection. Moreover, we build a comprehensive benchmark and set up rigorous evaluation protocols and metrics for this new research problem. Extensive quantitative and qualitative experiments demonstrate the effectiveness of SeqFakeFormer and SeqFakeFormer++. Several valuable observations are also revealed to facilitate future research in broader deepfake detection problems. The code has been released at https://github.com/rshaojimmy/SeqDeepFake/. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Xiao, Z.
AU  - Zhang, W.
AU  - Wang, T.
AU  - Loy, C.C.
AU  - Lin, D.
AU  - Pang, J.
TI  - Position-Guided Point Cloud Panoptic Segmentation Transformer
PY  - 2025
T2  - International Journal of Computer Vision
VL  - 133
IS  - 1
SP  - 275
EP  - 290
DO  - 10.1007/s11263-024-02162-z
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199919384&doi=10.1007%2fs11263-024-02162-z&partnerID=40&md5=4da9b8eee2a64d03759ed30e0872162d
AB  - DEtection TRansformer (DETR) started a trend that uses a group of learnable queries for unified visual perception. This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline. Although the naive adaptation obtains fair results, the instance segmentation performance is noticeably inferior to previous works. By diving into the details, we observe that instances in the sparse point clouds are relatively small to the whole scene and often have similar geometry but lack distinctive appearance for segmentation, which are rare in the image domain. Considering instances in 3D are more featured by their positional information, we emphasize their roles during the modeling and design a robust Mixed-parameterized Positional Embedding (MPE) to guide the segmentation process. It is embedded into backbone features and later guides the mask prediction and query update processes iteratively, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel the queries to attend to specific regions and identify various instances. The method, named Position-guided Point cloud Panoptic segmentation transFormer (P3Former), outperforms previous state-of-the-art methods by 2.7% and 1.2% PQ on SemanticKITTI and nuScenes datasets, respectively. The source code and models are available at https://github.com/OpenRobotLab/P3Former. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Jang, D.-K.
AU  - Yang, D.
AU  - Jang, D.-Y.
AU  - Choi, B.
AU  - Lee, S.-H.
AU  - Shin, D.
TI  - ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling
PY  - 2024
T2  - ACM Transactions on Graphics
VL  - 43
IS  - 6
C7  - 237
DO  - 10.1145/3687991
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210026526&doi=10.1145%2f3687991&partnerID=40&md5=ff026318a01a2bdf2ba938513a8587d9
AB  - This paper introduces ELMO, a real-time upsampling motion capture framework designed for a single LiDAR sensor. Modeled as a conditional autoregressive transformer-based upsampling motion generator, ELMO achieves 60 fps motion capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is the coupling of the self-attention mechanism with thoughtfully designed embedding modules for motion and point clouds, significantly elevating the motion quality. To facilitate accurate motion capture, we develop a one-time skeleton calibration model capable of predicting user skeleton off-sets from a single-frame point cloud. Additionally, we introduce a novel data augmentation technique utilizing a LiDAR simulator, which enhances global root tracking to improve environmental understanding. To demonstrate the effectiveness of our method, we compare ELMO with state-of-the-art methods in both image-based and point cloud-based motion capture. We further conduct an ablation study to validate our design principles. ELMO's fast inference time makes it well-suited for real-time applications, exemplified in our demo video featuring live streaming and interactive gaming scenarios. Furthermore, we contribute a high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects performing a range of motions, which can serve as a valuable resource for future research. The dataset and evaluation code are available at https://movin3d.github.io/ELMO_SIGASIA2024/. © 2024 Association for Computing Machinery. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Jang, I.
AU  - Choi, S.
AU  - Hong, S.
AU  - Kim, C.
AU  - Noh, J.
TI  - Geometry-Aware Retargeting for Two-Skinned Characters Interaction
PY  - 2024
T2  - ACM Transactions on Graphics
VL  - 43
IS  - 6
C7  - 12-ART203
DO  - 10.1145/3687962
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210168684&doi=10.1145%2f3687962&partnerID=40&md5=19532ca3c2b0dbb8975b7a7b81cb3433
AB  - Interactive motion between multiple characters is widely utilized in games and movies. However, the method for generating interactive motions considering the character's diverse mesh shape has yet to be studied. We propose a Spatio Cooperative Transformer (SCT) to retarget the interacting motions of two characters having arbitrary mesh connectivity. SCT predicts the residual of root position and joint rotations considering the shape difference between the source and target of interacting characters. In addition, we introduce an anchor loss function for SCT to maintain the geometric distance between the interacting characters when they are retargeted. We also propose a motion augmentation method with deformation-based adaptation to prepare a source-target paired dataset with an identical mesh connectivity for training. In experiments, our method achieved higher accuracy for semantic preservation and produced less artifacts of inter-penetration between the interacting characters for unseen characters and motions than the baselines. Moreover, we conducted a user evaluation using characters with various shapes, spanning low-to-high interaction levels to prove better semantic preservation of our method compared to previous studies.  © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, B.
AU  - Hu, Y.
AU  - Guerrero, P.
AU  - Hasan, M.
AU  - Shi, L.
AU  - Deschaintre, V.
AU  - Matusik, W.
TI  - Procedural Material Generation with Reinforcement Learning
PY  - 2024
T2  - ACM Transactions on Graphics
VL  - 43
IS  - 6
C7  - 280
DO  - 10.1145/3687979
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209940404&doi=10.1145%2f3687979&partnerID=40&md5=4ae6d263bd0c0fed57e9388655a1b53b
AB  - Modern 3D content creation heavily relies on procedural assets. In particular, procedural materials are ubiquitous in the industry, but their manipulation remains challenging. Previous work [Hu et al. 2023] conditionally generates procedural graphs that match a given input image. However, the parameter generation step limits how accurately the generated graph matches the input image, due to a reliance on supervision with scarcely available procedural data. We propose to improve parameter prediction accuracy for image-conditioned procedural material generation by leveraging reinforcement learning (RL) and present the first RL approach for procedural materials. RL circumvents the limited availability of procedural data, the domain gap between real and synthetic materials, and the need for end-to-end differentiable loss functions. Given a target image, we retrieve a procedural material and use an RL-trained transformer model to predict a set of parameters that reconstruct the target image as closely as possible. We show that using RL significantly improves parameter prediction to match a given target image compared to supervised methods on both synthetic and real target images. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, L.
AU  - Lu, J.
AU  - Zheng, S.
AU  - Zhao, X.
AU  - Zhu, X.
AU  - Fu, Y.
AU  - Xiang, T.
AU  - Feng, J.
AU  - Torr, P.H.S.
TI  - Vision Transformers: From Semantic Segmentation to Dense Prediction
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 12
SP  - 6142
EP  - 6162
DO  - 10.1007/s11263-024-02173-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198706168&doi=10.1007%2fs11263-024-02173-w&partnerID=40&md5=61db404e1df8d690484e40ff46b676d0
AB  - The emergence of vision transformers (ViTs) in image classification has shifted the methodologies for visual representation learning. In particular, ViTs learn visual representation at full receptive field per layer across all the image patches, in comparison to the increasing receptive fields of CNNs across layers and other alternatives (e.g., large kernels and atrous convolution). In this work, for the first time we explore the global context learning potentials of ViTs for dense visual prediction (e.g., semantic segmentation). Our motivation is that through learning global context at full receptive field layer by layer, ViTs may capture stronger long-range dependency information, critical for dense prediction tasks. We first demonstrate that encoding an image as a sequence of patches, a vanilla ViT without local convolution and resolution reduction can yield stronger visual representation for semantic segmentation. For example, our model, termed as SEgmentation TRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the test leaderboard on the day of submission) and performs competitively on Cityscapes. However, the basic ViT architecture falls short in broader dense prediction applications, such as object detection and instance segmentation, due to its lack of a pyramidal structure, high computational demand, and insufficient local context. For tackling general dense visual prediction tasks in a cost-effective manner, we further formulate a family of Hierarchical Local-Global (HLG) Transformers, characterized by local attention within windows and global-attention across windows in a pyramidal architecture. Extensive experiments show that our methods achieve appealing performance on a variety of dense prediction tasks (e.g., object detection and instance segmentation and semantic segmentation) as well as image classification. Our code and models are available at https://github.com/fudan-zvg/SETR. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zou, J.
AU  - Sun, A.
AU  - Long, C.
AU  - Kanoulas, E.
TI  - Knowledge-Enhanced Conversational Recommendation via Transformer-Based Sequential Modeling
PY  - 2024
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 6
C7  - 162
DO  - 10.1145/3677376
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208364514&doi=10.1145%2f3677376&partnerID=40&md5=f35159b0f131527029df81886810cbb7
AB  - In conversational recommender systems (CRSs), conversations usually involve a set of items and item-related entities or attributes, e.g., director is a related entity of a movie. These items and item-related entities are often mentioned along the development of a dialog, leading to potential sequential dependencies among them. However, most of existing CRSs neglect these potential sequential dependencies7. In this article, we first propose a Transformer-based sequential conversational recommendation method, named TSCR, to model the sequential dependencies in the conversations to improve CRS. In TSCR, we represent conversations by items and the item-related entities, and construct user sequences to discover user preferences by considering both the mentioned items and item-related entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Meanwhile, in certain domains, knowledge graphs formed by the items and their related entities are readily available, which provide various different kinds of associations among them. Given that TSCR does not benefit from such knowledge graphs, we then propose a knowledge graph enhanced version of TSCR, called TSCRKG. In specific, we leverage the knowledge graph to offline initialize our model TSCRKG, and augment the user sequence of conversations (i.e., sequence of the mentioned items and item-related entities in the conversation) with multi-hop paths in the knowledge graph. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines, and the enhanced version TSCRKG further improves recommendation performance on top of TSCR.  © 2024 Copyright held by the owner/author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Zou2024Knowledge-Enhanced
ER  -

TY  - JOUR
AU  - Zhuo, X.
AU  - Qian, S.
AU  - Hu, J.
AU  - Dai, F.
AU  - Lin, K.
AU  - Wu, G.
TI  - Multi-Hop Multi-View Memory Transformer for Session-Based Recommendation
PY  - 2024
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 6
C7  - 144
DO  - 10.1145/3663760
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202975023&doi=10.1145%2f3663760&partnerID=40&md5=8002ba0e5a391d07f3d42d7f91db7725
AB  - A Session-Based Recommendation (SBR) seeks to predict users' future item preferences by analyzing their interactions with previously clicked items. In recent approaches, Graph Neural Networks (GNNs) have been commonly applied to capture item relations within a session to infer user intentions. However, these GNN-based methods typically struggle with feature ambiguity between the sequential session information and the item conversion within an item graph, which may impede the model's ability to accurately infer user intentions. In this article, we propose a novel Multi-hop Multi-view Memory Transformer (M3T) to effectively integrate the sequence-view information and relation conversion (graph-view information) of items in a session. First, we propose a Multi-view Memory Transformer (M2T) module to concurrently obtain multi-view information of items. Then, a set of trainable memory matrices are employed to store sharable item features, which mitigates cross-view item feature ambiguity. To comprehensively capture latent user intentions, an M3T framework is designed to integrate user intentions across different hops of an item graph. Specifically, a k-order power method is proposed to manage the item graph to alleviate the over-smoothing problem when obtaining high-order relations of items. Extensive experiments conducted on three real-world datasets demonstrate the superiority of our method.  © 2024 Copyright held by the owner/author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Zhuo2024Multi-Hop
ER  -

TY  - JOUR
AU  - Lu, J.
AU  - Zhang, J.
AU  - Zhu, X.
AU  - Feng, J.
AU  - Xiang, T.
AU  - Zhang, L.
TI  - Softmax-Free Linear Transformers
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 8
SP  - 3355
EP  - 3374
DO  - 10.1007/s11263-024-02035-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187698369&doi=10.1007%2fs11263-024-02035-5&partnerID=40&md5=ebe1416cc393719f08ac2e899db660a9
AB  - Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of SOftmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under low-rank matrix decomposition. For computational robustness, we estimate the Moore–Penrose inverse using an iterative Newton–Raphson method in the forward process only, while calculating its theoretical gradients only once in the backward process. To further expand applicability (e.g., dense prediction tasks), an efficient symmetric normalization technique is introduced. Extensive experiments on ImageNet, COCO and ADE20K show that our SOFT significantly improves the computational efficiency of existing ViT variants. With linear complexity, much longer token sequences are permitted by SOFT, resulting in superior trade-off between accuracy and complexity. Code and models are available at https://github.com/fudan-zvg/SOFT. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Hu, X.
AU  - Sun, F.
AU  - Sun, J.
AU  - Wang, F.
AU  - Li, H.
TI  - Cross-Modal Fusion and Progressive Decoding Network for RGB-D Salient Object Detection
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 8
SP  - 3067
EP  - 3085
DO  - 10.1007/s11263-024-02020-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186486414&doi=10.1007%2fs11263-024-02020-y&partnerID=40&md5=5e49dbef5d6552cededaedc906419dd3
AB  - Most existing RGB-D salient object detection (SOD) methods tend to achieve higher performance by integrating additional modules, such as feature enhancement and edge generation. There is no doubt that these modules will inevitably produce feature redundancy and performance degradation. To this end, we exquisitely design a cross-modal fusion and progressive decoding network (termed CPNet) to achieve RGB-D SOD tasks. The designed network structure only includes three indispensable parts: feature encoding, feature fusion and feature decoding. Specifically, in the feature encoding part, we adopt a two-stream Swin Transformer encoder to extract multi-level and multi-scale features from RGB images and depth images respectively to model global information. In the feature fusion part, we design a cross-modal attention fusion module, which can leverage the attention mechanism to fuse multi-modality and multi-level features. In the feature decoding part, we design a progressive decoder to gradually fuse low-level features and filter noise information to accurately predict salient objects. Extensive experimental results on 6 benchmarks demonstrated that our network surpasses 12 state-of-the-art methods in terms of four metrics. In addition, it is also verified that for the RGB-D SOD task, the addition of the feature enhancement module and the edge generation module is not conducive to improving the detection performance under this framework, which provides new insights into the salient object detection task. Our codes are available at https://github.com/hu-xh/CPNet. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 18
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Qin, Y.
AU  - Ju, W.
AU  - Wu, H.
AU  - Luo, X.
AU  - Zhang, M.
TI  - Learning Graph ODE for Continuous-Time Sequential Recommendation
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 7
SP  - 3224
EP  - 3236
DO  - 10.1109/TKDE.2024.3349397
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181576796&doi=10.1109%2fTKDE.2024.3349397&partnerID=40&md5=65b8961219d46a6e2fbb3575cd3dfb1a
AB  - Sequential recommendation aims at understanding user preference by capturing successive behavior correlations, which are usually represented as the item purchasing sequences based on their past interactions. Existing efforts generally predict the next item via modeling the sequential patterns. Despite effectiveness, there exist two natural deficiencies: (i) user preference is dynamic in nature, and the evolution of collaborative signals is often ignored; and (ii) the observed interactions are often irregularly-sampled, while existing methods model item transitions assuming uniform intervals. Thus, how to effectively model and predict the underlying dynamics for user preference becomes a critical research problem. To tackle the above challenges, in this paper, we focus on continuous-time sequential recommendation and propose a principled graph ordinary differential equation framework named GDERec. Technically, GDERec is characterized by an autoregressive graph ordinary differential equation consisting of two components, which are parameterized by two tailored graph neural networks (GNNs) respectively to capture user preference from the perspective of hybrid dynamical systems. On the one hand, we introduce a novel ordinary differential equation based GNN to implicitly model the temporal evolution of the user-item interaction graph. On the other hand, an attention-based GNN is proposed to explicitly incorporate collaborative attention to interaction signals when the interaction graph evolves over time. The two customized GNNs are trained alternately in an autoregressive manner to track the evolution of the underlying system from irregular observations, and thus learn effective representations of users and items beneficial to the sequential recommendation. Extensive experiments on five benchmark datasets demonstrate the superiority of our model over various state-of-the-art recommendation methods.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 9
C2  - CCF:A期刊; FMS:B; 
LB  - Qin2024Learning
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Laskar, Z.
AU  - Melekhov, I.
AU  - Li, X.
AU  - Zhao, Y.
AU  - Tolias, G.
AU  - Kannala, J.
TI  - HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 7
SP  - 2530
EP  - 2550
DO  - 10.1007/s11263-023-01982-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187172970&doi=10.1007%2fs11263-023-01982-9&partnerID=40&md5=2fb763d5af199d447ae657f86076a2ee
AB  - Visual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization, state-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently, deep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and thus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous environment, learning such a regression task directly can be difficult for a single network. In this work, we present a new hierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-fine manner from a single RGB image. The proposed method, which is an extension of HSCNet, allows us to train compact models which scale robustly to large environments. It sets a new state-of-the-art for single-image localization on the 7-Scenes, 12-Scenes, Cambridge Landmarks datasets, and the combined indoor scenes. © The Author(s) 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhuo, X.
AU  - Wu, G.
AU  - Zhang, Z.
AU  - Wu, X.
TI  - Geometric-Contextual Mutual Infomax Path Aggregation for Relation Reasoning on Knowledge Graph
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 7
SP  - 3076
EP  - 3090
DO  - 10.1109/TKDE.2024.3360258
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184314522&doi=10.1109%2fTKDE.2024.3360258&partnerID=40&md5=255a893844a6b62b54a74f2bcaa70083
AB  - Relation reasoning in Knowledge Graph Completion (KGC) aims at predicting missing relations between entities. Recently, effective KGC methods have usually focused on exploring the path pattern between entities, such as reward-based path walking and path context mining, to complete target relations. However, these methods typically suffer from two challenges: 1) They have difficulty in handling the individual representation limitation of candidate paths when there are no paths that directly represent latent relations between entities; 2) They overlook the biases of path context induction, which leads to unreasonable information interfering with the model's reasoning. To manage these challenges, a Geometric-Contextual Mutual Infomax (GCMI) path aggregator is proposed for relation reasoning. First, we design an attentive path aggregator with a shared Transformer encoder to capture the contexts from several candidate paths parallelly and integrate these contexts to sufficiently represent the latent relations of each entity pair for reasoning. Then, the GCMI modules are proposed to constrain the local and global biases of path context induction in the Transformer encoder and the path aggregator, respectively, by a straightforward geometric rule. Extensive experiments on 32 real-world relation reasoning tasks demonstrate that our method significantly outperforms 8 state-of-the-art baselines in terms of AP and AUC. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Zhuo2024Geometric-Contextual
ER  -

TY  - JOUR
AU  - Yu, X.
AU  - Qin, C.
AU  - Shen, D.
AU  - Ma, H.
AU  - Zhang, L.
AU  - Zhang, X.
AU  - Zhu, H.
AU  - Xiong, H.
TI  - RDGT: Enhancing Group Cognitive Diagnosis With Relation-Guided Dual-Side Graph Transformer
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 7
SP  - 3429
EP  - 3442
DO  - 10.1109/TKDE.2024.3352640
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182917696&doi=10.1109%2fTKDE.2024.3352640&partnerID=40&md5=9d673f532bbc4267a2fe1ec884027ac6
AB  - Cognitive diagnosis has been widely recognized as a crucial task in the field of computational education, which is capable of learning the knowledge profiles of students and predicting their future exercise performance. Indeed, considerable research efforts have been made in this direction over the past decades. However, most of the existing studies only focus on individual-level diagnostic modeling, while the group-level cognitive diagnosis still lacks an in-depth exploration, which is more compatible with realistic collaborative learning environments. To this end, in this paper, we propose a Relation-guided Dual-side Graph Transformer (RDGT) model for achieving effective group-level cognitive diagnosis. Specifically, we first construct the dual-side relation graphs (i.e., student-side and exercise-side) from the group-student-exercise heterogeneous interaction data for explicitly modeling associations between students and exercises, respectively. In particular, the edge weight between two nodes is defined based on the similarity of corresponding student-exercise interactions. Then, we introduce two relation-guided graph transformers to learn the representations of students and exercises by integrating the whole graph information, including both nodes and edge weights. Meanwhile, the inter-group information has been incorporated into the student-side relation graph to further enhance the representations of students. Along this line, we design a cognitive diagnosis module for learning the groups' proficiency in specific knowledge concepts, which includes an attention-based aggregation strategy to obtain the final group representation and a hybrid loss for optimizing the performance prediction of both group and student. Finally, extensive experiments on 5 real-world datasets clearly demonstrate the effectiveness of our model as well as some interesting findings (e.g., the representative groups and potential collaborations among students). © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; FMS:B; 
LB  - Yu2024RDGT
ER  -

TY  - JOUR
AU  - Xu, H.
AU  - Usuyama, N.
AU  - Bagga, J.
AU  - Zhang, S.
AU  - Rao, R.
AU  - Naumann, T.
AU  - Wong, C.
AU  - Gero, Z.
AU  - González, J.
AU  - Gu, Y.
AU  - Xu, Y.
AU  - Wei, M.
AU  - Wang, W.
AU  - Ma, S.
AU  - Wei, F.
AU  - Yang, J.
AU  - Li, C.
AU  - Gao, J.
AU  - Rosemon, J.
AU  - Bower, T.
AU  - Lee, S.
AU  - Weerasinghe, R.
AU  - Wright, B.J.
AU  - Robicsek, A.
AU  - Piening, B.
AU  - Bifulco, C.
AU  - Wang, S.
AU  - Poon, H.
TI  - A whole-slide foundation model for digital pathology from real-world data
PY  - 2024
T2  - Nature
VL  - 630
IS  - 8015
SP  - 181
EP  - 188
DO  - 10.1038/s41586-024-07441-w
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193847602&doi=10.1038%2fs41586-024-07441-w&partnerID=40&md5=0b1e7085075e609243f9d856aa33ac1c
AB  - Digital pathology poses unique computational challenges, as a standard gigapixel slide may comprise tens of thousands of image tiles1–3. Prior models have often resorted to subsampling a small portion of tiles for each slide, thus missing the important slide-level context4. Here we present Prov-GigaPath, a whole-slide pathology foundation model pretrained on 1.3 billion 256 × 256 pathology image tiles in 171,189 whole slides from Providence, a large US health network comprising 28 cancer centres. The slides originated from more than 30,000 patients covering 31 major tissue types. To pretrain Prov-GigaPath, we propose GigaPath, a novel vision transformer architecture for pretraining gigapixel pathology slides. To scale GigaPath for slide-level learning with tens of thousands of image tiles, GigaPath adapts the newly developed LongNet5 method to digital pathology. To evaluate Prov-GigaPath, we construct a digital pathology benchmark comprising 9 cancer subtyping tasks and 17 pathomics tasks, using both Providence and TCGA data6. With large-scale pretraining and ultra-large-context modelling, Prov-GigaPath attains state-of-the-art performance on 25 out of 26 tasks, with significant improvement over the second-best method on 18 tasks. We further demonstrate the potential of Prov-GigaPath on vision–language pretraining for pathology7,8 by incorporating the pathology reports. In sum, Prov-GigaPath is an open-weight foundation model that achieves state-of-the-art performance on various digital pathology tasks, demonstrating the importance of real-world data and whole-slide modelling. © The Author(s) 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 38
C2  - ZUFE:TOP; 
ER  -

TY  - JOUR
AU  - Tuli, S.
AU  - Casale, G.
AU  - Jennings, N.R.
TI  - PreGAN+: Semi-Supervised Fault Prediction and Preemptive Migration in Dynamic Mobile Edge Environments
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 6
SP  - 6881
EP  - 6895
DO  - 10.1109/TMC.2023.3330679
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177039424&doi=10.1109%2fTMC.2023.3330679&partnerID=40&md5=2709f784ec3f1649ff6d3037b4148fef
AB  - Typical mobile edge computing infrastructures have to contend with unreliable computing devices at their end-points. The limited resource capacities of mobile edge devices gives rise to frequent contentions, node overloads or failures. This is exacerbated by the strict deadlines of modern applications. To avoid failures, fault-tolerant approaches utilize preemptive migration to transfer active tasks across nodes and prevent nodes running at capacity. However, prior work struggles to dynamically adapt in settings with highly volatile workloads or even accurately detect and diagnose anomalies for optimal remediation. To meet the strict service level objectives of contemporary workloads, there is a need for dynamic fault-tolerant methods that can quickly adapt to changes in edge environments while having parsimonious remediation in the form of preemptive migration to avoid stressing the system network. This work proposes PreGAN, featuring a Generative Adversarial Network (GAN) based approach to predict contentions, pinpoint specific resource types with high chance of overload, and generate migration decisions to proactively avoid system downtime. PreGAN leverages coupled-simulations to train the GAN model at run-time and a few-shot fault classifier to update decisions of an underpinning scheduler. We also extend it to PreGAN+ that also periodically tunes the decision model using semi-supervised training and a Transformer based neural network for low tuning time, albeit with higher memory overheads. Experiments on a Raspberry-Pi based edge environment demonstrate that both models outperform state-of-the-art baselines in fault detection and diagnosis scores by up to 12.5% and 31.2% respectively. This also translates in improvements in Quality of Service against baseline approaches.  © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Shi, C.
AU  - Ren, P.
AU  - Fu, D.
AU  - Xin, X.
AU  - Yang, S.
AU  - Cai, F.
AU  - Ren, Z.
AU  - Chen, Z.
TI  - Diversifying Sequential Recommendation with Retrospective and Prospective Transformers
PY  - 2024
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 5
C7  - 132
DO  - 10.1145/3653016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195044918&doi=10.1145%2f3653016&partnerID=40&md5=84698a0b5729805ae7f001191a5a74fa
AB  - Previous studies on sequential recommendation (SR) have predominantly concentrated on optimizing recommendation accuracy. However, there remains a significant gap in enhancing recommendation diversity, particularly for short interaction sequences. The limited availability of interaction information in short sequences hampers the recommender's ability to comprehensively model users' intents, consequently affecting both the diversity and accuracy of recommendation. In light of the above challenge, we propose reTrospective and pRospective Transformers for dIversified sEquential Recommendation (TRIER). The TRIER addresses the issue of insufficient information in short interaction sequences by first retrospectively learning to predict users' potential historical interactions, thereby introducing additional information and expanding short interaction sequences, and then capturing users' potential intents from multiple augmented sequences. Finally, the TRIER learns to generate diverse recommendation lists by covering as many potential intents as possible.To evaluate the effectiveness of TRIER, we conduct extensive experiments on three benchmark datasets. The experimental results demonstrate that TRIER significantly outperforms state-of-the-art methods, exhibiting diversity improvement of up to 11.36% in terms of intra-list distance (ILD@5) on the Steam dataset, 3.43% ILD@5 on the Yelp dataset and 3.77% in terms of category coverage (CC@5) on the Beauty dataset. As for accuracy, on the Yelp dataset, we observe notable improvement of 7.62% and 8.63% in HR@5 and NDCG@5, respectively. Moreover, we found that TRIER reveals more significant accuracy and diversity improvement for short interaction sequences.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Shi2024Diversifying
ER  -

TY  - JOUR
AU  - Si, J.
AU  - Yang, J.
AU  - Xiang, Y.
AU  - Wang, H.
AU  - Li, L.
AU  - Zhang, R.
AU  - Tu, B.
AU  - Chen, X.
TI  - TrajBERT: BERT-Based Trajectory Recovery With Spatial-Temporal Refinement for Implicit Sparse Trajectories
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 5
SP  - 4849
EP  - 4860
DO  - 10.1109/TMC.2023.3297115
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165421373&doi=10.1109%2fTMC.2023.3297115&partnerID=40&md5=4b90df61afc3497d51aca4fb3db55020
AB  - In the realm of human mobility data analysis, a multitude of constraints result in the publication of sparse, non-uniform implicit trajectories without explicit location information, such as coordinates. Researchers have dedicated substantial efforts towards trajectory recovery, aiming to densify trajectories and gain a more comprehensive understanding of human mobility. However, existing trajectory recovery methods focus on explicit trajectories, and require extensive historical data to capture users' mobility patterns. Nevertheless, implicit trajectories are usually more sparse than explicit trajectories. Addressing these challenges, we propose TrajBERT, an innovative BERT-based trajectory recovery method with spatial-temporal refinement. TrajBERT employs the Transformer encoder to learn mobility patterns bi-directionally and enhances the predictions by cross-stage temporal refinement. Subsequently, we design an output layer with global spatial refinement with a novel spatial-temporal aware loss function. To evaluate the performance of TrajBERT, we conduct a series of experiments on real-world datasets. Remarkably, TrajBERT yields at least 8.2% performance improvement compared to the state-of-the-art trajectory recovery approachs. Furthermore, TrajBERT successfully mitigates the cold start problem commonly experienced with new users lacking historical trajectories. It also shows superior robustness when faced with extremely sparse trajectories, thus demonstrating its potential as a practical tool in the field of human mobility analysis. © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, D.
AU  - Lin, Y.
AU  - Tang, J.
AU  - Cheng, K.-T.
TI  - CAE-GReaT: Convolutional-Auxiliary Efficient Graph Reasoning Transformer for Dense Image Predictions
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 5
SP  - 1502
EP  - 1520
DO  - 10.1007/s11263-023-01928-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178280838&doi=10.1007%2fs11263-023-01928-1&partnerID=40&md5=4ab5d2908ee65b3e3e0fe77a36c2a020
AB  - Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) are two primary frameworks for current semantic image recognition tasks in the community of computer vision. The general consensus is that both CNNs and ViT have their latent strengths and weaknesses, e.g., CNNs are good at extracting local features but difficult to aggregate long-range feature dependencies, while ViT is good at aggregating long-range feature dependencies but poorly represents in local features. In this paper, we propose an auxiliary and integrated network architecture, named Convolutional-Auxiliary Efficient Graph Reasoning Transformer (CAE-GReaT), which joints strengths of both CNNs and ViT into a uniform framework. CAE-GReaT stands on the shoulders of the advanced graph reasoning transformer and employs an internal auxiliary convolutional branch to enrich the local feature representations. Besides, to reduce the computational costs in graph reasoning, we also propose an efficient information diffusion strategy. Compared to the existing ViT models, CAE-GReaT not only has the advantage of a purposeful interaction pattern (via the graph reasoning branch), but also can capture fine-grained heterogeneous feature representations (via the auxiliary convolutional branch). Extensive experiments are implemented on three challenging dense image prediction tasks, i.e., semantic segmentation, instance segmentation, and panoptic segmentation. Results demonstrate that CAE-GReaT can achieve consistent performance gains on the state-of-the-art baselines with a slightly computational cost. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Pu, Y.
AU  - Liu, F.
AU  - Shi, R.
AU  - Yuan, H.
AU  - Chen, R.
AU  - Peng, T.
AU  - Wu, W.
TI  - ELAKT: Enhancing Locality for Attentive Knowledge Tracing
PY  - 2024
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 4
C7  - 112
DO  - 10.1145/3652601
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193526605&doi=10.1145%2f3652601&partnerID=40&md5=6ecfaa047c840db40785ab65645fa456
AB  - Knowledge tracing models based on deep learning can achieve impressive predictive performance by leveraging attention mechanisms. However, there still exist two challenges in attentive knowledge tracing (AKT): First, the mechanism of classical models of AKT demonstrates relatively low attention when processing exercise sequences with shifting knowledge concepts (KC), making it difficult to capture the comprehensive state of knowledge across sequences. Second, classical models do not consider stochastic behaviors, which negatively affects models of AKT in terms of capturing anomalous knowledge states. This article proposes a model of AKT, called Enhancing Locality for Attentive Knowledge Tracing (ELAKT), that is a variant of the deep KT model. The proposed model leverages the encoder module of the transformer to aggregate knowledge embedding generated by both exercises and responses over all timesteps. In addition, it uses causal convolutions to aggregate and smooth the states of local knowledge. The ELAKT model uses the states of comprehensive KCs to introduce a prediction correction module to forecast the future responses of students to deal with noise caused by stochastic behaviors. The results of experiments demonstrated that the ELAKT model consistently outperforms state-of-the-art baseline KT models.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Pu2024ELAKT
ER  -

TY  - JOUR
AU  - Zhao, S.
AU  - Xu, T.
AU  - Wu, X.-J.
AU  - Kittler, J.
TI  - A Spatio-Temporal Robust Tracker with Spatial-Channel Transformer and Jitter Suppression
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 5
SP  - 1645
EP  - 1658
DO  - 10.1007/s11263-023-01902-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177818438&doi=10.1007%2fs11263-023-01902-x&partnerID=40&md5=074629d29aca57ef5f8fa25b6df63a21
AB  - The robustness of visual object tracking is reflected not only in the accuracy of the target localisation in every single frame, but also in the smoothness of the predicted motion of the tracked object across consecutive frames. From the perspective of appearance modelling, the success of the state-of-the-art Transformer-based trackers derives from their ability to adaptively associate the representations of related spatial regions. However, the absence of attention in the channel dimension hinders the realisation of their potential tracking capacity. To cope with the commonly occurring misalignment of the spatial scale between the template and a search patch, we propose a novel cross channel correlation mechanism. Accordingly, the relevance of multi-channel features in the channel Transformer is modelled using two different sources of information. The result is a novel spatial-channel Transformer, which integrates information conveyed by features along both, the spatial and channel directions. For temporal modelling, to quantify the temporal smoothness, we propose a jitter metric that measures the cross-frame variation of the predicted bounding boxes as a function of the parameters such as centre displacement, area, and aspect ratio. As the changes of an object between consecutive frames are limited, the proposed jitter loss can be used to monitor the temporal behaviour of the tracking results and penalise erroneus predictions during the training stage, thus enhancing the temporal stability of an appearance-based tracker. Extensive experiments on several well-known benchmarking datasets demonstrate the robustness of the proposed tracker. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Tan, G.
AU  - Gou, C.
TI  - Cascaded Iterative Transformer for Jointly Predicting Facial Landmark, Occlusion Probability and Head Pose
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 4
SP  - 1242
EP  - 1257
DO  - 10.1007/s11263-023-01935-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175858330&doi=10.1007%2fs11263-023-01935-2&partnerID=40&md5=0e549c1476dcab60fc3cf878354e408d
AB  - Landmark detection under large pose with occlusion has been one of the challenging problems in the field of facial analysis. Recently, many works have predicted pose or occlusion together in the multi-task learning (MTL) paradigm, trying to tap into their dependencies and thus alleviate this issue. However, such implicit dependencies are weakly interpretable and inconsistent with the way humans exploit inter-task coupling relations, i.e., accommodating the induced explicit effects. This is one of the essentials that hinders their performance. To this end, in this paper, we propose a Cascaded Iterative Transformer (CIT) to jointly predict facial landmark, occlusion probability, and pose. The proposed CIT, besides implicitly mining task dependencies in a shared encoder, innovatively employs a cost-effective and portability-friendly strategy to pass the decoders’ predictions as prior knowledge to human-like exploit the coupling-induced effects. Moreover, to the best of our knowledge, no dataset contains all these task annotations simultaneously, so we introduce a new dataset termed MERL-RAV-FLOP based on the MERL-RAV dataset. We conduct extensive experiments on several challenging datasets (300W-LP, AFLW2000-3D, BIWI, COFW, and MERL-RAV-FLOP) and achieve remarkable results. The code and dataset can be accessed in https://github.com/Iron-LYK/CIT. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Radhakrishnan, A.
AU  - Beaglehole, D.
AU  - Pandit, P.
AU  - Belkin, M.
TI  - Mechanism for feature learning in neural networks and backpropagation-free machine learning models
PY  - 2024
T2  - Science
VL  - 383
IS  - 6690
SP  - 1461
EP  - 1467
DO  - 10.1126/science.adi5639
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189181874&doi=10.1126%2fscience.adi5639&partnerID=40&md5=c85e1a2a01f1934f1e8b1f34f720f021
AB  - Understanding how neural networks learn features, or relevant patterns in data, for prediction is necessary for their reliable use in technological and scientific applications. In this work, we presented a unifying mathematical mechanism, known as average gradient outer product (AGOP), that characterized feature learning in neural networks. We provided empirical evidence that AGOP captured features learned by various neural network architectures, including transformer-based language models, convolutional networks, multilayer perceptrons, and recurrent neural networks. Moreover, we demonstrated that AGOP, which is backpropagation-free, enabled feature learning in machine learning models, such as kernel machines, that a priori could not identify task-specific features. Overall, we established a fundamental mechanism that captured feature learning in neural networks and enabled feature learning in general machine learning models. © 2024 American Association for the Advancement of Science. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - ZUFE:TOP; 
ER  -

TY  - JOUR
AU  - He, J.
AU  - Zhou, X.
AU  - Xu, B.
AU  - Zhang, T.
AU  - Kim, K.
AU  - Yang, Z.
AU  - Thung, F.
AU  - Irsan, I.C.
AU  - Lo, D.
TI  - Representation Learning for Stack Overflow Posts: How Far Are We?
PY  - 2024
T2  - ACM Transactions on Software Engineering and Methodology
VL  - 33
IS  - 3
C7  - 69
DO  - 10.1145/3635711
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192010921&doi=10.1145%2f3635711&partnerID=40&md5=a4824e042d89d61c26287b630880a1e4
AB  - The tremendous success of Stack Overflow has accumulated an extensive corpus of software engineering knowledge, thus motivating researchers to propose various solutions for analyzing its content. The performance of such solutions hinges significantly on the selection of representation models for Stack Overflow posts. As the volume of literature on Stack Overflow continues to burgeon, it highlights the need for a powerful Stack Overflow post representation model and drives researchers' interest in developing specialized representation models that can adeptly capture the intricacies of Stack Overflow posts. The state-of-the-art (SOTA) Stack Overflow post representation models are Post2Vec and BERTOverflow, which are built upon neural networks such as convolutional neural network and transformer architecture (e.g., BERT). Despite their promising results, these representation methods have not been evaluated in the same experimental setting. To fill the research gap, we first empirically compare the performance of the representation models designed specifically for Stack Overflow posts (Post2Vec and BERTOverflow) in a wide range of related tasks (i.e., tag recommendation, relatedness prediction, and API recommendation). The results show that Post2Vec cannot further improve the SOTA techniques of the considered downstream tasks, and BERTOverflow shows surprisingly poor performance. To find more suitable representation models for the posts, we further explore a diverse set of transformer-based models, including (1) general domain language models (RoBERTa, Longformer, and GPT2) and (2) language models built with software engineering related textual artifacts (CodeBERT, GraphCodeBERT, seBERT, CodeT5, PLBart, and CodeGen). This exploration shows that models like CodeBERT and RoBERTa are suitable for representing Stack Overflow posts. However, it also illustrates the "No Silver Bullet"concept, as none of the models consistently wins against all the others. Inspired by the findings, we propose SOBERT, which employs a simple yet effective strategy to improve the representation models of Stack Overflow posts by continuing the pre-training phase with the textual artifact from Stack Overflow. The overall experimental results demonstrate that SOBERT can consistently outperform the considered models and increase the SOTA performance significantly for all the downstream tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Lai, B.
AU  - Liu, M.
AU  - Ryan, F.
AU  - Rehg, J.M.
TI  - In the Eye of Transformer: Global–Local Correlation for Egocentric Gaze Estimation and Beyond
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 3
SP  - 854
EP  - 871
DO  - 10.1007/s11263-023-01879-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174399982&doi=10.1007%2fs11263-023-01879-7&partnerID=40&md5=2a6ba4b3df4495c982e41f9ea3966cfc
AB  - Predicting human’s gaze from egocentric videos serves as a critical role for human intention understanding in daily activities. In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel global–local correlation module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets – EGTEA Gaze + and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds the previous state-of-the-art model by a large margin. We also apply our model to a novel gaze saccade/fixation prediction task and the traditional action recognition problem. The consistent gains suggest the strong generalization capability of our model. We also provide additional visualizations to support our claim that global–local correlation serves a key representation for predicting gaze fixation from egocentric videos. More details can be found in our website (https://bolinlai.github.io/GLC-EgoGazeEst). © The Author(s) 2023.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Guo, Y.
AU  - Feng, W.
AU  - Yin, F.
AU  - Liu, C.-L.
TI  - SignParser: An End-to-End Framework for Traffic Sign Understanding
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 3
SP  - 805
EP  - 821
DO  - 10.1007/s11263-023-01912-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174252366&doi=10.1007%2fs11263-023-01912-9&partnerID=40&md5=a8f9cfd18ad98b706a2b01b9fecd5a63
AB  - In intelligent transportation systems, parsing traffic signs and transmitting traffic information to humans is an urgent need. However, despite the success achieved in the detection and recognition of low-level circular or triangular traffic signs, parsing the more complex and informative rectangular traffic signs remains unexplored and challenging. Our work is devoted to the topic called “Traffic Sign Understanding (TSU)”, which is aimed to parse various traffic signs and generate semantic descriptions for them. To achieve this goal, we propose an end-to-end framework that integrates component detection, content reasoning, and semantic description generation. The component detection module first detects initial components in the sign image. Then the content reasoning module acquires the detailed content of the sign, including final components, their relations, and layout category, which provide local and global information for the subsequent module. In the end, the semantic description generation module mines relational attributes and text semantic attributes from the preceding results, embeds them with the layout categories, and transforms them into semantic descriptions through a dynamic prediction transformer. The three modules are trained jointly in an end-to-end manner for optimizing the overall performance. This method achieves state-of-the-art performance not only in the final semantic description generation stage but also on multiple subtasks of the CASIA-Tencent CTSU Dataset. Abundant ablation experiments are provided to prove the effectiveness of this method. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Vuong, T.
AU  - Ruotsalo, T.
TI  - Predicting Representations of Information Needs from Digital Activity Context
PY  - 2024
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 4
C7  - 95
DO  - 10.1145/3639819
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193535810&doi=10.1145%2f3639819&partnerID=40&md5=839d30be3866597f23394789f5fd8282
AB  - Information retrieval systems often consider search-session and immediately preceding web-browsing history as the context for predicting users' present information needs. However, such context is only available when a user's information needs originate from web context or when users have issued preceding queries in the search session. Here, we study the effect of more extensive context information recorded from users' everyday digital activities by monitoring all information interacted with and communicated using personal computers. Twenty individuals were recruited for 14 days of 24/7 continuous monitoring of their digital activities, including screen contents, clicks, and operating system logs on Web and non-Web applications. Using this data, a transformer architecture is applied to model the digital activity context and predict representations of personalized information needs. Subsequently, the representations of information needs are used for query prediction, query auto-completion, selected search result prediction, and Web search re-ranking. The predictions of the models are evaluated against the ground truth data obtained from the activity recordings. The results reveal that the models accurately predict representations of information needs improving over the conventional search session and web-browsing contexts. The results indicate that the present practice for utilizing users' contextual information is limited and can be significantly extended to achieve improved search interaction support and performance. © 2024 Copyright held by the owner/author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; FMS:B; 
LB  - Vuong2024Predicting
ER  -

TY  - JOUR
AU  - Wang, Q.
AU  - Cao, X.
AU  - Wang, J.
AU  - Zhang, W.
TI  - Knowledge-Aware Collaborative Filtering With Pre-Trained Language Model for Personalized Review-Based Rating Prediction
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 3
SP  - 1170
EP  - 1182
DO  - 10.1109/TKDE.2023.3301884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167819218&doi=10.1109%2fTKDE.2023.3301884&partnerID=40&md5=05c99653ec921ee3fbd9631c06e0b39c
AB  - Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover, KCF-PLM integrates the transformer network and the pre-trained language models through representation propagation on the knowledge graph and user-item guided attention of the aspect representations. Thus KCF-PLM combines review text, aspect, knowledge graph, and pre-trained language models together for review-based rating prediction. We conduct comprehensive experiments on several public datasets, demonstrating the effectiveness of KCF-PLM.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; FMS:B; 
LB  - Wang2024Knowledge-Aware
ER  -

TY  - JOUR
AU  - Tufano, R.
AU  - Dabic, O.
AU  - Mastropaolo, A.
AU  - Ciniselli, M.
AU  - Bavota, G.
TI  - Code Review Automation: Strengths and Weaknesses of the State of the Art
PY  - 2024
T2  - IEEE Transactions on Software Engineering
VL  - 50
IS  - 2
SP  - 338
EP  - 353
DO  - 10.1109/TSE.2023.3348172
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181565883&doi=10.1109%2fTSE.2023.3348172&partnerID=40&md5=13391e83302cf2a373ff6293d05a7c05
AB  - The automation of code review has been tackled by several researchers with the goal of reducing its cost. The adoption of deep learning in software engineering pushed the automation to new boundaries, with techniques imitating developers in generative tasks, such as commenting on a code change as a reviewer would do or addressing a reviewer's comment by modifying code. The performance of these techniques is usually assessed through quantitative metrics, e.g., the percentage of instances in the test set for which correct predictions are generated, leaving many open questions on the techniques' capabilities. For example, knowing that an approach is able to correctly address a reviewer's comment in 10% of cases is of little value without knowing what was asked by the reviewer: What if in all successful cases the code change required to address the comment was just the removal of an empty line? In this paper we aim at characterizing the cases in which three code review automation techniques tend to succeed or fail in the two above-described tasks. The study has a strong qualitative focus, with ∼105 man-hours of manual inspection invested in manually analyzing correct and wrong predictions generated by the three techniques, for a total of 2,291 inspected predictions. The output of this analysis are two taxonomies reporting, for each of the two tasks, the types of code changes on which the experimented techniques tend to succeed or to fail, pointing to areas for future work. A result of our manual analysis was also the identification of several issues in the datasets used to train and test the experimented techniques. Finally, we assess the importance of researching in techniques specialized for code review automation by comparing their performance with ChatGPT, a general purpose large language model, finding that ChatGPT struggles in commenting code as a human reviewer would do. © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; FMS:B; 
LB  - Tufano2024Code
ER  -

TY  - JOUR
AU  - Wu, F.
AU  - Lyu, F.
AU  - Ren, J.
AU  - Yang, P.
AU  - Qian, K.
AU  - Gao, S.
AU  - Zhang, Y.
TI  - Characterizing Internet Card User Portraits for Efficient Churn Prediction Model Design
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 2
SP  - 1735
EP  - 1752
DO  - 10.1109/TMC.2023.3241206
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148432161&doi=10.1109%2fTMC.2023.3241206&partnerID=40&md5=17d1e6b596a1004aa3f25e8bf16db94b
AB  - Cellular Internet card (IC) as a new business model emerges, which penetrates rapidly and holds the potential to foster a great business market. However, with the explosive growth of IC users, the user churn problem becomes severe, affecting the IC business significantly, while there is lacking appropriate techniques in the literature to deal with the issue. In this article, we take the lead to study one large-scale data set from a provincial network operator of China, which contains about 4 million IC users and 22 million traditional card (TC) users. We first justify the IC user churn issue with data, and categorize the user churning reasons. Then, we shed light on understanding user portraits, which is the building block to enable efficient model design. Particularly, we conduct a systematical analytics on usage data by studying the difference of two types of users, examining the impact of user properties, and characterizing the user Internet using behaviors. Finally, by using the IC user portraits and usage patterns, we propose an IC user Churn Prediction model, named ICCP, which consists of a feature extraction component and a learning-based churn prediction architecture design. For feature extraction, both the static portrait features and temporal sequential features are captured. In the learning architecture, we devise the principal component analysis (PCA) block and the embedding/transformer layers to learn the respective information of two types of features, which are collectively fed into the classification multilayer perceptron layer (MPL) for churn prediction. A reference implementation of ICCP is conducted within the telecom system and extensive experiments corroborate the efficiency of ICCP.  © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 16
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Pugoy, R.A.
AU  - Kao, H.-Y.
TI  - NEAR: Non-Supervised Explainability Architecture for Accurate Review-Based Collaborative Filtering
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 2
SP  - 750
EP  - 765
DO  - 10.1109/TKDE.2022.3226189
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144056063&doi=10.1109%2fTKDE.2022.3226189&partnerID=40&md5=ac4113543c833ff37040791633bec419
AB  - There is a critical issue in explainable recommender systems that compounds the challenges of explainability yet is rarely tackled: the lack of ground-truth explanation texts for training. It is unrealistic to expect every user-item pair in a dataset to have a corresponding target explanation. Hence, we pioneer the first non-supervised explainability architecture for review-based collaborative filtering (called NEAR) as our novel contribution to the theory of explanation construction in recommender systems. While maintaining excellent recommendation performance, our approach reformulates explainability as a non-supervised (i.e., unsupervised and self-supervised) explanation generation task. We formally define two explanation types, both of which NEAR can produce. An invariant explanation, fixed for all users, is based on the unsupervised extractive summary of an item's reviews via embedding clustering. Meanwhile, a variant explanation, personalized for a specific user, is a sentence-level text generated by our customized Transformer conditioned on every user-item-rating tuple and artificial ground-truth (self-supervised label) from one of the invariant explanation's sentences. Our empirical evaluation illustrates that NEAR's rating prediction accuracy is better than the other state-of-the-art baselines. Moreover, experiments and assessments show that NEAR-generated variant explanations are more personalized and distinct than those from other Transformer-based models, and our invariant explanations are preferred over those from other contemporary models in real life.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Pugoy2024NEAR
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Zhang, D.
AU  - Keuper, M.
AU  - Khoreva, A.
TI  - Intra- & Extra-Source Exemplar-Based Style Synthesis for Improved Domain Generalization
PY  - 2024
T2  - International Journal of Computer Vision
VL  - 132
IS  - 2
SP  - 446
EP  - 465
DO  - 10.1007/s11263-023-01878-8
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169894761&doi=10.1007%2fs11263-023-01878-8&partnerID=40&md5=0f4847009f3da3a4421e1e490cd6ea7e
AB  - The generalization with respect to domain shifts, as they frequently appear in applications such as autonomous driving, is one of the remaining big challenges for deep learning models. Therefore, we propose an exemplar-based style synthesis pipeline to improve domain generalization in semantic segmentation. Our method is based on a novel masked noise encoder for StyleGAN2 inversion. The model learns to faithfully reconstruct the image, preserving its semantic layout through noise prediction. Random masking of the estimated noise enables the style mixing capability of our model, i.e. it allows to alter the global appearance without affecting the semantic layout of an image. Using the proposed masked noise encoder to randomize style and content combinations in the training set, i.e., intra-source style augmentation (ISSA) effectively increases the diversity of training data and reduces spurious correlation. As a result, we achieve up to 12.4 % mIoU improvements on driving-scene semantic segmentation under different types of data shifts, i.e., changing geographic locations, adverse weather conditions, and day to night. ISSA is model-agnostic and straightforwardly applicable with CNNs and Transformers. It is also complementary to other domain generalization techniques, e.g., it improves the recent state-of-the-art solution RobustNet by 3 % mIoU in Cityscapes to Dark Zürich. In addition, we demonstrate the strong plug-n-play ability of the proposed style synthesis pipeline, which is readily usable for extra-source exemplars e.g., web-crawled images, without any retraining or fine-tuning. Moreover, we study a new use case to indicate neural network’s generalization capability by building a stylized proxy validation set. This application has significant practical sense for selecting models to be deployed in the open-world environment. Our code is available at https://github.com/boschresearch/ISSA . © 2023, The Author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yan, W.
AU  - Ma, H.
AU  - Yang, Z.
TI  - Segmented Sequence Prediction using Variable-Order Markov Model Ensemble
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
DO  - 10.1109/TKDE.2024.3522975
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213447578&doi=10.1109%2fTKDE.2024.3522975&partnerID=40&md5=48c4f80445faea3f7e8edf5acd527263
AB  - In recent years, sequence prediction, particularly in natural language processing tasks, has made significant progress due to advanced neural network architectures like Transformer and enhanced computing power. However, challenges persist in modeling and analyzing certain types of sequence data, such as human daily activities and competitive ball games. These segmented sequence data are characterized by short length, varying local dependencies, and coarse-grained unit states. These characteristics limit the effectiveness of conventional probabilistic graphical models and attention-based or recurrent neural networks in modeling and analyzing segmented sequence data. To address this gap, we introduce a novel generative model for segmented sequences, employing an ensemble of multiple variable-order Markov models (VOMMs) to flexibly represent state transition dependencies. Our approach integrates probabilistic graphical models with neural networks, surpassing the representation capabilities of single high-order or variable-order Markov models. Compared to end-to-end deep learning models, our method offers improved interpretability and reduces overfitting in short segments. We demonstrate the efficacy of our proposed method in two tasks: predicting tennis shot types and forecasting daily action sequences. These applications highlight the broad applicability of our segmented sequence modeling approach across diverse domains. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Yan2024Segmented
ER  -

TY  - JOUR
AU  - Shen, G.
AU  - Ouyang, Y.
AU  - Lu, J.
AU  - Yang, Y.
AU  - Sanchez, V.
TI  - Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6865
EP  - 6880
DO  - 10.1109/TIP.2024.3512369
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211979572&doi=10.1109%2fTIP.2024.3512369&partnerID=40&md5=b78bca1d788a54d3cf485b3146af98de
AB  - Despite the prevailing transition from single-task to multi-task approaches in video anomaly detection, we observe that many adopt sub-optimal frameworks for individual proxy tasks. Motivated by this, we contend that optimizing single-task frameworks can advance both single- and multi-task approaches. Accordingly, we leverage middle-frame prediction as the primary proxy task, and introduce an effective hybrid framework designed to generate accurate predictions for normal frames and flawed predictions for abnormal frames. This hybrid framework is built upon a bi-directional structure that seamlessly integrates both vision transformers and ConvLSTMs. Specifically, we utilize this bi-directional structure to fully analyze the temporal dimension by predicting frames in both forward and backward directions, significantly boosting the detection stability. Given the transformer's capacity to model long-range contextual dependencies, we develop a convolutional temporal transformer that efficiently associates feature maps from all context frames to generate attention-based predictions for target frames. Furthermore, we devise a layer-interactive ConvLSTM bridge that facilitates the smooth flow of low-level features across layers and time-steps, thereby strengthening predictions with fine details. Anomalies are eventually identified by scrutinizing the discrepancies between target frames and their corresponding predictions. Several experiments conducted on public benchmarks affirm the efficacy of our hybrid framework, whether used as a standalone single-task approach or integrated as a branch in a multi-task approach. These experiments also underscore the advantages of merging vision transformers and ConvLSTMs for video anomaly detection. The implementation of our hybrid framework is available at https://github.com/SHENGUODONG19951126/ConvTTrans-ConvLSTM.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhao, J.
AU  - Xue, B.
AU  - Zhang, M.
TI  - SALENet: Structure-Aware Lighting Estimations From a Single Image for Indoor Environments
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6806
EP  - 6820
DO  - 10.1109/TIP.2024.3512381
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211970360&doi=10.1109%2fTIP.2024.3512381&partnerID=40&md5=02d9a498acd19ef153c22bbbca340cc1
AB  - High Dynamic Range (HDR) lighting plays a pivotal role in modern augmented and mixed-reality (AR/MR) applications, facilitating immersive experiences through realistic object insertion and dynamic relighting. However, the acquisition of precise HDR environment maps remains cost-prohibitive and impractical when using standard devices. To bridge this gap, this paper introduces SALENet, a novel deep network for estimating global lighting conditions from a single image, to effectively mitigate the need for resource-intensive acquisition methods. In contrast to earlier studies, we focus on exploring the inherent structural relationships within the lighting distribution. We design a hierarchical transformer-based neural network architecture with a proposed cross-attention mechanism between different resolution lighting source representations, optimizing the spatial distribution of lighting sources simultaneously for enhanced consistency. To further improve accuracy, a structure-based contrastive learning method is proposed to select positive-negative pairs based on lighting distribution similarity. By harnessing the synergy of hierarchical transformers and structure-based contrastive learning, our framework yields a significant enhancement in lighting prediction accuracy, enabling high-fidelity augmented and mixed reality to achieve cost-effectively immersive and realistic lighting effects.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhu, R.
AU  - Tu, Z.
AU  - Liu, J.
AU  - Bovik, A.C.
AU  - Fan, Y.
TI  - MWFormer: Multi-Weather Image Restoration Using Degradation-Aware Transformers
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6790
EP  - 6805
DO  - 10.1109/TIP.2024.3501855
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210985531&doi=10.1109%2fTIP.2024.3501855&partnerID=40&md5=9104b7b721233ab71b2e75dfb15b90bb
AB  - Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, S.
AU  - Yang, Q.
AU  - Ruan, S.
AU  - Long, C.
AU  - Yuan, Y.
AU  - Li, Q.
AU  - Yuan, Z.
AU  - Bao, J.
AU  - Zheng, Y.
TI  - Spatial Meta Learning With Comprehensive Prior Knowledge Injection for Service Time Prediction
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
DO  - 10.1109/TKDE.2024.3512582
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211975200&doi=10.1109%2fTKDE.2024.3512582&partnerID=40&md5=dafe55a252d5ab4594dffd426113e139
AB  - Intelligent logistics relies on accurately predicting the service time, which is a part of time cost in the last-mile delivery. However, service time prediction (STP) is non-trivial given complex delivery circumstances, location heterogeneity, and skewed observations in space, which are not well-handled by existing solutions. In our prior work, we treat STP at each location as a learning task to keep the location heterogeneity, propose a prior knowledge-enhanced meta-learning to tackle skewed observations, and introduce a Transformer-based representation module to encode complex delivery circumstances. Maintaining the design principles of prior work, in this extended paper, we propose MetaSTP+. In addition to fusing the prior knowledge after the meta-learning process, MetaSTP+ also injects the prior knowledge before and during the meta-learning process to better tackle skewed observations. More specifically, MetaSTP+ completes the support set of tasks with scarce samples from other tasks based on prior knowledge and is equipped with a prior knowledge-aware historical observation encoding module to achieve those purposes accordingly. Experiments show MetaSTP+ outperforms the best baseline by 11.2% and 8.4% on two real-world datasets. Finally, an intelligent waybill assignment system based on MetaSTP+ is deployed in JD Logistics.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Wang2024Spatial
ER  -

TY  - JOUR
AU  - Lin, Z.
AU  - Sun, N.
AU  - Bhattacharya, P.
AU  - Feng, X.
AU  - Feng, L.
AU  - Owens, J.D.
TI  - Towards Universal Performance Modeling for Machine Learning Training on Multi-GPU Platforms
PY  - 2024
T2  - IEEE Transactions on Parallel and Distributed Systems
DO  - 10.1109/TPDS.2024.3507814
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210928340&doi=10.1109%2fTPDS.2024.3507814&partnerID=40&md5=43decfbc461b4ebbcd0701a6e8672d66
AB  - Characterizing and predicting the training performance of modern machine learning (ML) workloads on compute systems with compute and communication spread between CPUs, GPUs, and network devices is not only the key to optimization and planning but also a complex goal to achieve. The primary challenges include the complexity of synchronization and load balancing between CPUs and GPUs, the variance in input data distribution, and the use of different communication devices and topologies (e.g., NVLink, PCIe, network cards) that connect multiple compute devices, coupled with the desire for flexible training configurations. Built on top of our prior work for single- GPU platforms, we address these challenges and enable multi-GPU performance modeling1 by incorporating (1) data-distributionaware performance models for embedding table lookup, and (2) data movement prediction of communication collectives, into our upgraded performance modeling pipeline equipped with inter-and intra-rank synchronization for ML workloads trained on multi- GPU platforms. Beyond accurately predicting the per-iteration training time of deep learning recommendation models (DLRM) models with random configurations with a geomean error of 5.21% on two multi-GPU platforms, our prediction pipeline generalizes well to other types of ML workloads, such as Transformer-based natural language processing (NLP) models with a geomean error of 3.00%. Moreover, even without actually running ML workloads like DLRMs on the hardware, it is capable of generating insights such as quickly selecting the fastest embedding table sharding configuration (with a success rate of 85%).  © 1990-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ma, J.
AU  - Lou, J.
AU  - Jiang, B.
AU  - Ye, H.
AU  - Yu, W.
AU  - Chen, X.
AU  - Zhou, K.
AU  - Zheng, Y.
TI  - Neural Orthodontic Staging: Predicting Teeth Movements with a Transformer
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
DO  - 10.1109/TVCG.2024.3504866
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210008188&doi=10.1109%2fTVCG.2024.3504866&partnerID=40&md5=67f09bf75cc9e3a97172610a2fda93e5
AB  - We present a novel learning-based method for predicting tooth movements in orthodontic treatment path planning (orthodontic staging). Recognizing the multi-solution nature of orthodontic staging, our approach involves generating the staging sequence progressively with a dedicated Transformer model. This model predicts teeth movements within a predefined number of steps (e.g., 10 or 20), targeting alignment in problematic dentition. The Transformer refines its predictions iteratively, building on previous outcomes until reaching a state that aligns with the target within an acceptable distance. This mirrors real-life scenarios where orthodontists dynamically adjust staging plans based on treatment outcomes. Our Transformer model is tailored to incorporate spatial and temporal attentions, addressing inter-tooth and inter-step interactions, respectively. These attentions are further refined with relative positional encoding. Recognizing the significant influence of tooth shape on the alignment process, we propose integrating a tooth-wise shape encoder to extract morphological features from the 3D teeth point cloud. These features are then fused into the Transformer, facilitating the capture of inter-tooth dynamics during staging, in collaboration with spatial attention. We validate the proposed method on a large-scale dataset that contains 10K real-life orthodontic cases. The results show that our method outperforms the state-of-the-art, and orthodontists favor its predictions.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Shi, F.
AU  - Li, D.
AU  - Wang, X.
AU  - Li, B.
AU  - Wu, X.
TI  - TGformer: A Graph Transformer Framework for Knowledge Graph Embedding
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
DO  - 10.1109/TKDE.2024.3486747
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208930487&doi=10.1109%2fTKDE.2024.3486747&partnerID=40&md5=9efbf4ecc1b3a60bd18525f6c3458c8c
AB  - Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Shi2024TGformer
ER  -

TY  - JOUR
AU  - Choi, H.
AU  - Jin, S.
AU  - Han, K.
TI  - ICEv2: Interpretability, Comprehensiveness, and Explainability in Vision Transformer
PY  - 2024
T2  - International Journal of Computer Vision
DO  - 10.1007/s11263-024-02290-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210359163&doi=10.1007%2fs11263-024-02290-6&partnerID=40&md5=650d581b714f676623d56f4932bbe83f
AB  - Vision transformers use [CLS] token to predict image classes. Their explainability visualization has been studied using relevant information from the [CLS] token or focusing on attention scores during self-attention. However, such visualization is challenging because of the dependence of the interpretability of a vision transformer on skip connections and attention operators, the instability of non-linearities in the learning process, and the limited reflection of self-attention scores on relevance. We argue that the output patch embeddings in a vision transformer preserve the image information of each patch location, which can facilitate the prediction of an image class. In this paper, we propose ICEv2 (ICEv2: I̲nterpretability, C̲omprehensiveness, and E̲xplainability in Vision Transformer), an explainability visualization method that addresses the limitations of ICE (i.e., high dependence of hyperparameters on performance and the inability to preserve the model’s properties) by minimizing the number of training encoder layers, redesigning the MLP layer, and optimizing hyperparameters along with various model size. Overall, ICEv2 shows higher efficiency, performance, robustness, and scalability than ICE. On the ImageNet-Segmentation dataset, ICEv2 outperformed all explainability visualization methods in all cases depending on the model size. On the Pascal VOC dataset, ICEv2 outperformed both self-supervised and supervised methods on Jaccard similarity. In the unsupervised single object discovery, where untrained classes are present in the images, ICEv2 effectively distinguished between foreground and background, showing performance comparable to the previous state-of-the-art. Lastly, ICEv2 can be trained with significantly lower training computational complexity. © The Author(s) 2024.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Yang, K.
AU  - Lin, J.
AU  - Yuan, J.
AU  - Li, Z.
AU  - Li, S.
TI  - PVPUFormer: Probabilistic Visual Prompt Unified Transformer for Interactive Image Segmentation
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6455
EP  - 6468
DO  - 10.1109/TIP.2024.3492713
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209934258&doi=10.1109%2fTIP.2024.3492713&partnerID=40&md5=8e0c25d6bc5d09b0a000caf1a2fa1211
AB  - Integration of diverse visual prompts like clicks, scribbles, and boxes in interactive image segmentation significantly facilitates users' interaction as well as improves interaction efficiency. However, existing studies primarily encode the position or pixel regions of prompts without considering the contextual areas around them, resulting in insufficient prompt feedback, which is not conducive to performance acceleration. To tackle this problem, this paper proposes a simple yet effective Probabilistic Visual Prompt Unified Transformer (PVPUFormer) for interactive image segmentation, which allows users to flexibly input diverse visual prompts with the probabilistic prompt encoding and feature post-processing to excavate sufficient and robust prompt features for performance boosting. Specifically, we first propose a Probabilistic Prompt-unified Encoder (PPuE) to generate a unified one-dimensional vector by exploring both prompt and non-prompt contextual information, offering richer feedback cues to accelerate performance improvement. On this basis, we further present a Prompt-to-Pixel Contrastive (P2C) loss to accurately align both prompt and pixel features, bridging the representation gap between them to offer consistent feature representations for mask prediction. Moreover, our approach designs a Dual-cross Merging Attention (DMA) module to implement bidirectional feature interaction between image and prompt features, generating notable features for performance improvement. A comprehensive variety of experiments on several challenging datasets demonstrates that the proposed components achieve consistent improvements, yielding state-of-the-art interactive segmentation performance. Our code is available at https://github.com/XuZhang1211/PVPUFormer. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Sun, T.
AU  - Yuan, X.
AU  - Li, S.
AU  - Ni, W.
TI  - Minimizing Adversarial Training Samples for Robust Image Classifiers: Analysis and Adversarial Example Generator Design
PY  - 2024
T2  - IEEE Transactions on Information Forensics and Security
DO  - 10.1109/TIFS.2024.3474973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207150871&doi=10.1109%2fTIFS.2024.3474973&partnerID=40&md5=b6b5e71fcb5ae963a1a69bfc5067be35
AB  - Training deep neural networks (DNNs) with altered data, known as adversarial training, is essential for improving their robustness. A significant challenge emerges as the robustness strengthened during training often diminishes during inference, resulting in drops in robust pronounced accuracy. Contemporary strategies either necessitate excessively large training data or risk compromising the natural accuracy of non-adversarial images. Our analysis identifies that the inherent vulnerability of DNNs to adversarial attacks stems from certain input space segments that are inadequately populated by training data, leading to decision-making voids with incorrect predictions. The minimum number of training samples required for successful adversarial training can be attained by maximizing the representativeness of the samples. In light of this, we put forth an advanced training data augmentation method anchored on a Generative Adversarial Network. The generated samples are evaluated by the image classifier during training and selected based on their confidence scores. Evaluations on public datasets, such as Tiny-ImageNet, MS COCO, and CIFAR-100, using various deep neural networks (DNNs), including Vision Transformer, MobileNet, and WideResNet, under recent attacks like DifAttack, SQBA, and AutoAttack, confirm that our method significantly enhances the adversarial robustness of DNN image classifiers. Our method outperforms state-of-the-art adversarial training methods by 35.66% on Tiny-ImageNet, 13.53% on MS COCO, and 13.06% on CIFAR-100.  © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wu, D.
AU  - Wu, P.
AU  - Zhang, M.
AU  - Wang, F.
TI  - MANSY: Generalizing Neural Adaptive Immersive Video Streaming With Ensemble and Representation Learning
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
DO  - 10.1109/TMC.2024.3487175
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208701441&doi=10.1109%2fTMC.2024.3487175&partnerID=40&md5=362e385ad313074cbbcbccf69a9d51f6
AB  - The popularity of immersive videos has prompted extensive research into neural adaptive tile-based streaming to optimize video transmission over networks with limited bandwidth. However, the diversity of users' viewing patterns and Quality of Experience (QoE) preferences has not been fully addressed yet by existing neural adaptive approaches for viewport prediction and bitrate selection. Their performance can significantly deteriorate when users' actual viewing patterns and QoE preferences differ considerably from those observed during the training phase, resulting in poor generalization. In this paper, we propose MANSY, a novel streaming system that embraces user diversity to improve generalization. Specifically, to accommodate users' diverse viewing patterns, we design a Transformer-based viewport prediction model with an efficient multi-viewport trajectory input output architecture based on implicit ensemble learning. Besides, we for the first time combine the advanced representation learning and deep reinforcement learning to train the bitrate selection model to maximize diverse QoE objectives, enabling the model to generalize across users with diverse preferences. Extensive experiments demonstrate that MANSY outperforms state-of-the-art approaches in viewport prediction accuracy and QoE improvement on both trained and unseen viewing patterns and QoE preferences, achieving better generalization.  © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Kong, L.
AU  - Li, W.
AU  - Yang, H.
AU  - Zhang, Y.
AU  - Guan, J.
AU  - Zhou, S.
TI  - CausalFormer: An Interpretable Transformer for Temporal Causal Discovery
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
DO  - 10.1109/TKDE.2024.3484461
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207459156&doi=10.1109%2fTKDE.2024.3484461&partnerID=40&md5=df32960e417e3180a80de34210ce7238
AB  - Temporal causal discovery is a crucial task aimed at uncovering the causal relations within time series data. The latest temporal causal discovery methods usually train deep learning models on prediction tasks to uncover the causality between time series. They capture causal relations by analyzing the parameters of some components of the trained models, e.g., attention weights and convolution weights. However, this is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components, e.g., fully connected layers and activation functions, that are also significant for causal discovery. To facilitate the utilization of the whole deep learning models in temporal causal discovery, we proposed an interpretable transformer-based causal discovery model termed CausalFormer, which consists of the causality-aware transformer and the decomposition-based causality detector. The causality-aware transformer learns the causal representation of time series data using a prediction task with the designed multi-kernel causal convolution which aggregates each input time series along the temporal dimension under the temporal priority constraint. Then, the decomposition-based causality detector interprets the global structure of the trained causality-aware transformer with the proposed regression relevance propagation to identify potential causal relations and finally construct the causal graph. Experiments on synthetic, simulated, and real datasets demonstrate the state-of-the-art performance of CausalFormer on discovering temporal causality. Our code is available at https://github.com/lingbai-kong/CausalFormer. © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Kong2024CausalFormer
ER  -

TY  - JOUR
AU  - Huang, N.
AU  - Yang, Y.
AU  - Xi, R.
AU  - Zhang, Q.
AU  - Han, J.
AU  - Huang, J.
TI  - Salient Object Detection from Arbitrary Modalities
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6268
EP  - 6282
DO  - 10.1109/TIP.2024.3486225
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208373494&doi=10.1109%2fTIP.2024.3486225&partnerID=40&md5=742ca25545a5fd88205d186a9b54f874
AB  - Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection. Our code and AM-XD dataset will be released on https://github.com/nexiakele/AMSODFirst.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Xu, Y.
AU  - Zhang, M.
AU  - Yang, X.
AU  - Xu, C.
TI  - Exploring Multi-Modal Contextual Knowledge for Open-Vocabulary Object Detection
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 6253
EP  - 6267
DO  - 10.1109/TIP.2024.3485518
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208111265&doi=10.1109%2fTIP.2024.3485518&partnerID=40&md5=a972985024105a9ad3c6f0ff3470edd3
AB  - We explore multi-modal contextual knowledge learned through multi-modal masked language modeling to provide explicit localization guidance for novel classes in open-vocabulary object detection (OVD). Intuitively, a well-modeled and correctly predicted masked concept word should effectively capture the textual contexts, visual contexts, and the cross-modal correspondence between texts and regions, thereby automatically activating high attention on corresponding regions. In light of this, we propose a multi-modal contextual knowledge distillation framework, MMC-Det, to explicitly supervise a student detector with the context-aware attention of the masked concept words in a teacher fusion transformer. The teacher fusion transformer is trained with our newly proposed diverse multi-modal masked language modeling (D-MLM) strategy, which significantly enhances the fine-grained region-level visual context modeling in the fusion transformer. The proposed distillation process provides additional contextual guidance to the concept-region matching of the detector, thereby further improving the OVD performance. Extensive experiments performed upon various detection datasets show the effectiveness of our multi-modal context learning strategy.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Nguyen, T.T.
AU  - Nguyen, T.T.
AU  - Weidlich, M.
AU  - Jo, J.
AU  - Nguyen, Q.V.H.
AU  - Yin, H.
AU  - Liew, A.W.-C.
TI  - Handling Low Homophily in Recommender Systems with Partitioned Graph Transformer
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
DO  - 10.1109/TKDE.2024.3485880
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208390157&doi=10.1109%2fTKDE.2024.3485880&partnerID=40&md5=303f14c3664eaf14ea69f337d26ed864
AB  - Modern recommender systems derive predictions from an interaction graph that links users and items. To this end, many of today's state-of-the-art systems use graph neural networks (GNNs) to learn effective representations of these graphs under the assumption of homophily, i.e., the idea that similar users will sit close to each other in the graph. However, recent studies have revealed that real-world recommendation graphs are often heterophilous, i.e., dissimilar users will also often sit close to each other. One of the reasons for this heterophilia is shilling attacks that obscure the inherent characteristics of the graph and make the derived recommendations less accurate as a consequence. Hence, to cope with low homophily in recommender systems, we propose a recommendation model called PGT4Rec that is based on a Partitioned Graph Transformer. The model integrates label information into the learning process, which allows discriminative neighbourhoods of users to be generated. As such, the framework can both detect shilling attacks and predict user ratings for items. Extensive experiments on real and synthetic datasets show PGT4Rec as not only providing superior performance in these two tasks but also significant robustness to a range of adversarial conditions.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Nguyen2024Handling
ER  -

TY  - JOUR
AU  - Huang, J.
AU  - Zhou, D.
AU  - Liu, J.
AU  - Shi, L.
AU  - Chen, S.
TI  - IFAST: Weakly Supervised Interpretable Face Anti-Spoofing from Single-Shot Binocular NIR Images
PY  - 2024
T2  - IEEE Transactions on Information Forensics and Security
VL  - 19
SP  - 9270
EP  - 9284
DO  - 10.1109/TIFS.2024.3465930
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205012096&doi=10.1109%2fTIFS.2024.3465930&partnerID=40&md5=04bf9f49f0d3a214269565c011ce8a6b
AB  - Single-shot face anti-spoofing (FAS) is a key technique for securing face recognition systems, relying solely on static images as input. However, single-shot FAS remains a challenging and under-explored problem due to two reasons: 1) On the data side, learning FAS from RGB images is largely context-dependent, and single-shot images without additional annotations contain limited semantic information. 2) On the model side, existing single-shot FAS models struggle to provide proper evidence for their decisions, and FAS methods based on depth estimation require expensive per-pixel annotations. To address these issues, we construct and release a large binocular NIR image dataset named BNI-FAS, which contains more than 300,000 real face and plane attack images, and propose an Interpretable FAS Transformer (IFAST) that requires only weak supervision to produce interpretable predictions. Our IFAST generates pixel-wise disparity maps using the proposed disparity estimation Transformer with Dynamic Matching Attention (DMA) blocks. Besides, we design a confidence map generator to work in tandem with a dual-teacher distillation module to obtain the final discriminant results. Comprehensive experiments show that our IFAST achieves state-of-the-art performance on BNI-FAS, verifying its effectiveness of single-shot FAS on binocular NIR images. The project page is available at https://ifast-bni.github.io/.  © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Luo, Y.
AU  - Du, B.
TI  - UVaT: Uncertainty Incorporated View-Aware Transformer for Robust Multi-View Classification
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 5129
EP  - 5143
DO  - 10.1109/TIP.2024.3451931
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204510337&doi=10.1109%2fTIP.2024.3451931&partnerID=40&md5=b59fe2ee85a89a739d7e4993e1bbd54d
AB  - Existing multi-view classification algorithms usually assume that all examples have observations on all views, and the data in different views are clean. However, in real-world applications, we are often provided with data that have missing representations or contain noise on some views (i.e., missing or noise views). This may lead to significant performance degeneration, and thus many algorithms are proposed to address the incomplete view or noisy view issues. However, most of existing algorithms deal with the two issues separately, and hence may fail when both missing and noisy views exist. They are also usually not flexible in that the view or feature significance cannot be adaptively identified. Besides, the view missing patterns may vary in the training and test phases, and such difference is often ignored. To remedy these drawbacks, we propose a novel multi-view classification framework that is simultaneously robust to both incomplete and noisy views. This is achieved by integrating early fusion and late fusion in a single framework. Specifically, in our early fusion module, we propose a view-aware transformer to mask the missing views and adaptively explore the relationships between views and target tasks to deal with missing views. Considering that view missing patterns may change from the training to the test phase, we also design single-view classification and category-consistency constraints to reduce the dependence of our model on view-missing patterns. In our late fusion module, we quantify the view uncertainty in an ensemble way to estimate the noise level of that view. Then the uncertainty and prediction logits of different views are integrated to make our model robust to noisy views. The framework is trained in an end-to-end manner. Experimental results on diverse datasets demonstrate the robustness and effectiveness of our model for both incomplete and noisy views. Codes are available at https://github.com/li-yapeng/UVaT.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Qu, J.
AU  - Zhang, P.
AU  - Che, E.
AU  - Chen, Y.
AU  - Ling, H.
TI  - Graph Transformer for Label Placement
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
DO  - 10.1109/TVCG.2024.3456141
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204240598&doi=10.1109%2fTVCG.2024.3456141&partnerID=40&md5=ae33f845c2750086bfe7261f1e2e6888
AB  - Placing text labels is a common way to explain key elements in a given scene. Given a graphic input and original label information, how to place labels to meet both geometric and aesthetic requirements is an open challenging problem. Geometry-wise, traditional rule-driven solutions struggle to capture the complex interactions between labels, let alone consider graphical/appearance content. In terms of aesthetics, training/evaluation data ideally require nontrivial effort and expertise in design, thus resulting in a lack of decent datasets for learning-based methods. To address the above challenges, we formulate the task with a graph representation, where nodes correspond to labels and edges to interactions between labels, and treat label placement as a node position prediction problem. With this novel representation, we design a Label Placement Graph Transformer (LPGT) to predict label positions. Specifically, edge-level attention, conditioned on node representations, is introduced to reveal potential relationships between labels. To integrate graphic/image information, we design a feature aligning strategy that extracts deep features for nodes and edges efficiently. Next, to address the dataset issue, we collect commercial illustrations with professionally designed label layouts from household appliance manuals, and annotate them with useful information to create a novel dataset named the Appliance Manual Illustration Labels (AMIL) dataset. In the thorough evaluation on AMIL, our LPGT solution achieves promising label placement performance compared with popular baselines. Our algorithm and dataset are available at https://github.com/JingweiQu/LPGT.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, H.
AU  - Zhang, Y.-F.
AU  - Zhang, Z.
AU  - Wen, Q.
AU  - Wang, L.
TI  - LogoRA: Local-Global Representation Alignment for Robust Time Series Classification
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 12
SP  - 8718
EP  - 8729
DO  - 10.1109/TKDE.2024.3459908
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204247974&doi=10.1109%2fTKDE.2024.3459908&partnerID=40&md5=4cd846fc8537f8cce5c856ea91371429
AB  - Unsupervised domain adaptation (UDA) of time series aims to teach models to identify consistent patterns across various temporal scenarios, disregarding domain-specific differences, which can maintain their predictive accuracy and effectively adapt to new domains. However, existing UDA methods struggle to adequately extract and align both global and local features in time series data. To address this issue, we propose the Local-Global Representation Alignment framework (LogoRA), which employs a two-branch encoder-comprising a multi-scale convolutional branch and a patching transformer branch. The encoder enables the extraction of both local and global representations from time series. A fusion module is then introduced to integrate these representations, enhancing domain-invariant feature alignment from multi-scale perspectives. To achieve effective alignment, LogoRA employs strategies like invariant feature learning on the source domain, utilizing triplet loss for fine alignment and dynamic time warping-based feature alignment. Additionally, it reduces source-target domain gaps through adversarial training and per-class prototype alignment. Our evaluations on four time-series datasets demonstrate that LogoRA outperforms strong baselines by up to 12.52%, showcasing its superiority in time series UDA tasks.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Zhang2024LogoRA
ER  -

TY  - JOUR
AU  - Paltenghi, M.
AU  - Pandita, R.
AU  - Henley, A.Z.
AU  - Ziegler, A.
TI  - Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration
PY  - 2024
T2  - IEEE Transactions on Software Engineering
VL  - 50
IS  - 10
SP  - 2568
EP  - 2582
DO  - 10.1109/TSE.2024.3445338
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201757668&doi=10.1109%2fTSE.2024.3445338&partnerID=40&md5=69ceca00c920953c22da0d44f7d5ea84
AB  - Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47% accuracy. This outperforms the baseline prediction accuracy of 42.3%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration. © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Paltenghi2024Follow-Up
ER  -

TY  - JOUR
AU  - Jin, Y.
AU  - Zhang, W.
AU  - Xu, Z.
AU  - Wang, F.
AU  - Liu, X.
TI  - Privacy-Preserving Gaze-Assisted Immersive Video Streaming
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 12
SP  - 15098
EP  - 15113
DO  - 10.1109/TMC.2024.3452510
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202771442&doi=10.1109%2fTMC.2024.3452510&partnerID=40&md5=88ec60b0fbb2b3f83374bb8ca53f82f5
AB  - Immersive videos, also known as 360° videos, have gained significant attention in recent years due to their ability to provide an interactive and engaging experience. However, the development of immersive video streaming faces several challenges, including privacy concerns, the need for accurate viewport prediction, and efficient bandwidth allocation. In this paper, we propose a comprehensive system that integrates three specialized modules: the Privacy Protection module, the Viewport Prediction module, and the Bitrate Allocation module. The Privacy Protection module introduces a novel approach to differential privacy tailored for immersive video environments, considering the spatial and temporal correlations in viewport and gaze motion data. The Viewport Prediction module leverages a crossmodal attention mechanism based on the transformer to predict user viewport movements by analyzing the complex interactions between historical data, video content, and gaze patterns. The Bitrate Allocation module employs an adaptive tile-based bitrate allocation strategy using an exponential decay function to optimize video quality and maximize user quality of experience. Experimental results demonstrate that our proposed framework outperforms three state-of-the-art integrated frameworks, achieving an average QoE improvement of 21.61%. This paper offers substantial novelty in addressing privacy concerns, leveraging gaze information for viewport prediction, and utilizing underlying correlations between different features. © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Gao, S.
AU  - Zhou, H.
AU  - Chen, T.
AU  - He, M.
AU  - Xu, R.
AU  - Li, J.
TI  - PE-Attack: On the Universal Positional Embedding Vulnerability in Transformer-Based Models
PY  - 2024
T2  - IEEE Transactions on Information Forensics and Security
VL  - 19
SP  - 9359
EP  - 9373
DO  - 10.1109/TIFS.2024.3442617
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201269815&doi=10.1109%2fTIFS.2024.3442617&partnerID=40&md5=b8fbad55d18d180e44a17f7f0b432349
AB  - The Transformer model has gained significant recognition for its remarkable computational capabilities and versatility, positioning itself as a fundamental component in numerous practical applications. However, the robustness of the Transformer model, specifically its stability and reliability under various types of adversarial attacks, is of utmost importance for its practical applicability. Furthermore, it offers valuable insights for the design of more efficient and secure models. In contrast with conventional investigations into adversarial robustness, our study focuses on the analysis of Positional Embeddings (PEs), a crucial component that sets the Transformer model apart from previous model architectures. Theoretical analysis of PEs has been limited due to previous predominantly empirical design, which includes features such as sinusoidal or linear patterns, learned or fixed characteristics, and absolute or relative measurements. Our investigation delves deep into potential vulnerabilities within PEs. Initially, we develop a set of input infection techniques that can be universally applied to exploit vulnerabilities present in the Transformer architecture and its variants. In addition, we propose a novel adversarial attack that manipulates the model by providing it with incorrect positional information, enabling an evasion attack. Significantly, in contrast to previous attacks that were limited to a single task, our conducted experiments involving time-series analysis, natural language processing, and computer vision indicate that the susceptibility of PEs could be universal and transferable. This finding serves as a significant warning for future Transformer-based model design, urging researchers to consider potential security risks inherent in the model's structure.  © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, T.-B.
AU  - Liu, A.-A.
AU  - Song, D.
AU  - Li, W.-H.
AU  - Zhang, J.
AU  - Wei, Z.-Q.
AU  - Su, Y.-T.
TI  - Multi-Task Spatial-Temporal Transformer for Multi-Variable Meteorological Forecasting
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 12
SP  - 8876
EP  - 8888
DO  - 10.1109/TKDE.2024.3432599
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199524783&doi=10.1109%2fTKDE.2024.3432599&partnerID=40&md5=afaf30c4991aa6205e4ff63c14906f17
AB  - This study delves into multi-variable meteorological spatial-temporal prediction, focusing on the simultaneous forecasting of key meteorological parameters such as temperature, wind speed, and atmospheric pressure. The core challenge of this task lies in identifying commonalities across different variables while capturing their unique features and the interactions among them. To address this, we propose a novel multi-task learning framework tailored for multi-variable meteorological forecasting. Our framework integrates a convolutional variable-specific visual representation module and a variable-interactive spatial-temporal inference module. The former extracts distinct variable information independently for each variable, while the latter employs a tri-level attention mechanism across space, time, and variables to uncover both commonalities and interactions among the variables. An adaptive multi-loss optimization strategy and a local information aggregation module are introduced to balance task optimization complexities and enhance representation stability. Comprehensive experiments across various meteorological prediction tasks confirm the effectiveness of our methods, showcasing superior performance over existing approaches. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Li2024Multi-Task
ER  -

TY  - JOUR
AU  - Xu, G.
AU  - Jia, W.
AU  - Wu, T.
AU  - Chen, L.
AU  - Gao, G.
TI  - HAFormer: Unleashing the Power of Hierarchy-Aware Features for Lightweight Semantic Segmentation
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 4202
EP  - 4214
DO  - 10.1109/TIP.2024.3425048
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199110338&doi=10.1109%2fTIP.2024.3425048&partnerID=40&md5=3ae6658a93387bcb5c239aaa5ad1c7ac
AB  - Both Convolutional Neural Networks (CNNs) and Transformers have shown great success in semantic segmentation tasks. Efforts have been made to integrate CNNs with Transformer models to capture both local and global context interactions. However, there is still room for enhancement, particularly when considering constraints on computational resources. In this paper, we introduce HAFormer, a model that combines the hierarchical features extraction ability of CNNs with the global dependency modeling capability of Transformers to tackle lightweight semantic segmentation challenges. Specifically, we design a Hierarchy-Aware Pixel-Excitation (HAPE) module for adaptive multi-scale local feature extraction. During the global perception modeling, we devise an Efficient Transformer (ET) module streamlining the quadratic calculations associated with traditional Transformers. Moreover, a correlation-weighted Fusion (cwF) module selectively merges diverse feature representations, significantly enhancing predictive accuracy. HAFormer achieves high performance with minimal computational overhead and compact model size, achieving 74.2% mIoU on Cityscapes and 71.1% mIoU on CamVid test datasets, with frame rates of 105FPS and 118FPS on a single 2080Ti GPU. The source codes are available at https://github.com/XU-GITHUB-curry/HAFormer.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ye, M.
AU  - Yu, Y.
AU  - Shen, Z.
AU  - Yu, W.
AU  - Zeng, Q.
TI  - Cross-Feature Interactive Tabular Data Modeling With Multiplex Graph Neural Networks
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 12
SP  - 7851
EP  - 7864
DO  - 10.1109/TKDE.2024.3440654
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200798036&doi=10.1109%2fTKDE.2024.3440654&partnerID=40&md5=0eb7ad6ff9f4b2ba43ecc19e6b259135
AB  - The rising popularity of tabular data in data science applications has led to a surge of interest in utilizing deep neural networks (DNNs) to address tabular problems. Existing deep neural network methods are not effective in handling two fundamental challenges that are inherent in tabular data: permutation invariance (where the labels remain unchanged regardless of element order) and local dependency (where predictive labels are solely determined by local features). Furthermore, given the inherent heterogeneity among elements in tabular data, effectively capturing heterogeneous feature interactions remains unresolved. In this paper, we propose a novel Multiplex Cross-Feature Interaction Network (MPCFIN) by explicitly and systematically modeling feature relations with interactive graph neural networks. Specifically, MPCFIN first learns the most relevant features associated with individual features, and merges them to form cross-feature embedding. Subsequently, we design a multiplex graph neural network to learn enhanced representation for each sample. Comprehensive experiments on seven datasets demonstrate that MPCFIN exhibits superior performance over deep neural network methods in modeling the tabular data, showcasing consistent interpretability in its cross-feature embedding module for medical diagnosis applications. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Ye2024Cross-Feature
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Bi, Y.
AU  - Yuan, X.
AU  - Niyato, D.
AU  - Yang, K.
AU  - Chen, X.
AU  - Zhao, L.
TI  - A Novel Multimodal Long-Term Trajectory Prediction Scheme for Heterogeneous User Behavior Patterns
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 12
SP  - 13275
EP  - 13291
DO  - 10.1109/TMC.2024.3427862
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199106019&doi=10.1109%2fTMC.2024.3427862&partnerID=40&md5=91f9676f0f5263db04e43eaa497784a1
AB  - The prediction of user trajectories is a fundamental component to support urban traffic management and various advanced transportation applications, such as traffic optimization and location-based services. Trajectory data typically contains multiple behavioral patterns and contexts, including different travel purposes, modes of transportation, time intervals, and geographic regions. These complex factors collectively influence the prediction of user trajectories. However, trajectory prediction models face challenges in effectively distinguishing between these various patterns. In this paper, we propose a novel stack Transformer-based multimodal long-term trajectory prediction (SMTTP) scheme for heterogeneous user behavior patterns. First, a learnable trajectory similarity measure method is proposed to estimate the relative distance between multi-attribute variable-length trajectories. Then, to address the instability of trajectory clustering caused by random initialization, a cluster head initialization algorithm based on high confidence nodes is developed to improve clustering stability and reduce convergence time. In addition, a Transformer-based trajectory prediction model with multi-dimensional feature fusion is proposed to achieve accurate and efficient long-term trajectory prediction. Experimental results on the real telecom dataset in Shanghai, China show that the proposed SMTTP scheme can achieve improved performance in trajectory prediction in terms of prediction error, and also has high accuracy and stability in unsupervised trajectory clustering.  © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, X.
AU  - Wang, Z.
AU  - Du, B.
AU  - Wu, J.
AU  - Zhang, X.
AU  - Meng, E.
TI  - Deep Session Heterogeneity-Aware Network for Click Through Rate Prediction
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 12
SP  - 7927
EP  - 7939
DO  - 10.1109/TKDE.2024.3421594
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197541228&doi=10.1109%2fTKDE.2024.3421594&partnerID=40&md5=4bb64f8feca8c521798141a7eff41e4a
AB  - CTR (Click-Through Rate) prediction plays an essential role in online advertising systems. Most existing works attempt to capture users' interests from sessions by assuming that behaviors within a session are homogeneous. However, user interest may change frequently. Thus it is hard to guarantee that behaviors in a session are homogeneous, resulting in users' interests extracted from sessions being biased. In this paper, we propose a model named Deep Session Heterogeneity-aware Network (DSHN) by learning the relationships of behaviors within sessions and the relevance between the session and target item to alleviate the influence of irrelevant or heterogeneous sessions. We design a heterogeneity-aware mechanism to learn the heterogeneity of items within a session. Then we further design two modules: the Session Heterogeneity Learning module and the Relevance Inference module. The Session Heterogeneity Learning module weighs each session by summarizing the variation of session interest with and without any behavior. The relevance Inference module learns the relevance between the target item and each session in a similar way by learning session interest with and without the target item. Extensive experiments on four datasets demonstrate that our proposed DSHN achieves better results compared to the state-of-the-art. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Zhang2024Deep
ER  -

TY  - JOUR
AU  - Li, Z.
AU  - Wang, X.
AU  - Liu, X.
AU  - Jiang, J.
TI  - BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 3964
EP  - 3976
DO  - 10.1109/TIP.2024.3416065
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197099615&doi=10.1109%2fTIP.2024.3416065&partnerID=40&md5=9e06bfabb64c2a92f87d1ed31bd99ab1
AB  - Monocular depth estimation (MDE) is a fundamental task in computer vision and has drawn increasing attention. Recently, some methods reformulate it as a classification-regression task to boost the model performance, where continuous depth is estimated via a linear combination of predicted probability distributions and discrete bins. In this paper, we present a novel framework called BinsFormer, tailored for the classification-regression-based depth estimation. It mainly focuses on two crucial components in the specific task: 1) proper generation of adaptive bins; and 2) sufficient interaction between probability distribution and bins predictions. To specify, we employ a Transformer decoder to generate bins, novelly viewing it as a direct set-to-set prediction problem. We further integrate a multi-scale decoder structure to achieve a comprehensive understanding of spatial geometry information and estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene understanding query is proposed to improve the estimation accuracy, which turns out that models can implicitly learn useful information from the auxiliary environment classification task. Extensive experiments on the KITTI, NYU, and SUN RGB-D datasets demonstrate that BinsFormer surpasses state-of-the-art MDE methods with prominent margins. Code and pretrained models are made publicly available at https://github.com/zhyever/ Monocular-Depth-Estimation-Toolbox/tree/main/configs/ binsformer. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yang, H.
AU  - Li, Z.
AU  - Luo, C.
AU  - Wei, B.
AU  - Xu, W.
TI  - InaudibleKey2.0: Deep Learning-Empowered Mobile Device Pairing Protocol Based on Inaudible Acoustic Signals
PY  - 2024
T2  - IEEE/ACM Transactions on Networking
VL  - 32
IS  - 5
SP  - 4160
EP  - 4174
DO  - 10.1109/TNET.2024.3407783
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195363386&doi=10.1109%2fTNET.2024.3407783&partnerID=40&md5=2fe9ef5a4a53a669a91027e9b658150a
AB  - The increasing proliferation of Internet-of-Things (IoT) devices in daily life has rendered secure Device-to-Device (D2D) communication increasingly crucial. Achieving secure D2D communication necessitates key agreement between various IoT devices without prior knowledge. Despite existing literature proposing numerous approaches, they exhibit limitations such as low key generation rates and short pairing distances. In this paper, we present InaudibleKey2.0, an inaudible acoustic signal based key generation protocol for mobile devices. Based on acoustic channel reciprocity, InaudibleKey2.0 exploits the acoustic channel frequency response of two legitimate devices as a shared secret for key generation. To significantly enhance performance, InaudibleKey2.0 incorporates novel technologies, including a deep learning-enabled channel prediction model for improved channel reciprocity, a quantization model for increased key generation rates, and a transformer-based reconciliation method for augmented key agreement rates. We conduct comprehensive experiments to evaluate InaudibleKey2.0 in diverse real-world environments. In comparison to state-of-the-art solutions, InaudibleKey2.0 achieves 1.3-9.1 times improvement in key generation rates, 3.2-44 times extension in pairing distances, and 1.2-16 times reduction in information reconciliation counts. Security analysis substantiates that InaudibleKey2.0 is resilient to numerous malicious attacks. Furthermore, we implement InaudibleKey2.0 on modern smartphones and resource-limited IoT devices. The results indicate that it is energy-efficient and can operate on both powerful and resource-limited IoT devices without causing excessive resource consumption.  © 1993-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Wang, Q.
AU  - Yang, J.
AU  - Chen, B.
AU  - Xiong, H.
AU  - Du, S.
TI  - Learning Discriminative Features for Crowd Counting
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 3749
EP  - 3764
DO  - 10.1109/TIP.2024.3408609
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370084&doi=10.1109%2fTIP.2024.3408609&partnerID=40&md5=2cdb907a6a54e844cc3abe3c92f5a227
AB  - Crowd counting models in highly congested areas confront two main challenges: weak localization ability and difficulty in differentiating between foreground and background, leading to inaccurate estimations. The reason is that objects in highly congested areas are normally small and high-level features extracted by convolutional neural networks are less discriminative to represent small objects. To address these problems, we propose a learning discriminative features framework for crowd counting, which is composed of a masked feature prediction module (MPM) and a supervised pixel-level contrastive learning module (CLM). The MPM randomly masks feature vectors in the feature map and then reconstructs them, allowing the model to learn about what is present in the masked regions and improving the model's ability to localize objects in high-density regions. The CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background. Additionally, the proposed modules can be beneficial in various computer vision tasks, such as crowd counting and object detection, where dense scenes or cluttered environments pose challenges to accurate localization. The proposed two modules are plug-and-play, incorporating the proposed modules into existing models can potentially boost their performance in these scenarios.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, W.
AU  - Dong, X.
TI  - Perception-Aware Texture Similarity Prediction
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
C7  - 10542647
SP  - 3536
EP  - 3549
DO  - 10.1109/TIP.2024.3404854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194886920&doi=10.1109%2fTIP.2024.3404854&partnerID=40&md5=54427481add7aa8e480e28ddb7d1457a
AB  - Texture similarity plays important roles in texture analysis and material recognition. However, perceptually-consistent fine-grained texture similarity prediction is still challenging. The discrepancy between the texture similarity data obtained using algorithms and human visual perception has been demonstrated. This dilemma is normally attributed to the texture representation and similarity metric utilised by the algorithms, which are inconsistent with human perception. To address this challenge, we introduce a Perception-Aware Texture Similarity Prediction Network (PATSP-Net). This network comprises a Bilinear Lateral Attention Transformer network (BiLAViT) and a novel loss function, namely, RSLoss. The BiLAViT contains a Siamese Feature Extraction Subnetwork (SFEN) and a Metric Learning Subnetwork (MLN), designed on top of the mechanisms of human perception. On the other hand, the RSLoss measures both the ranking and the scaling differences. To our knowledge, either the BiLAViT or the RSLoss has not been explored for texture similarity tasks. The PATSP-Net performs better than, or at least comparably to, its counterparts on three data sets for different fine-grained texture similarity prediction tasks. We believe that this promising result should be due to the joint utilization of the BiLAViT and RSLoss, which is able to learn the perception-aware texture representation and similarity metric.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Han, L.
AU  - Ye, H.-J.
AU  - Zhan, D.-C.
TI  - The Capacity and Robustness Trade-Off: Revisiting the Channel Independent Strategy for Multivariate Time Series Forecasting
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 11
SP  - 7129
EP  - 7142
DO  - 10.1109/TKDE.2024.3400008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193234295&doi=10.1109%2fTKDE.2024.3400008&partnerID=40&md5=56b9a4293d759c437505adeeaf60024a
AB  - Multivariate time series data comprises various channels of variables. The multivariate forecasting models need to capture the relationship between the channels to accurately predict future values. However, recently, there has been an emergence of methods that employ the Channel Independent (CI) strategy. These methods view multivariate time series data as separate univariate time series and disregard the correlation between channels. Surprisingly, our empirical results have shown that models trained with the CI strategy outperform those trained with the Channel Dependent (CD) strategy, usually by a significant margin. Nevertheless, the reasons behind this phenomenon have not yet been thoroughly explored in the literature. This paper provides comprehensive empirical and theoretical analyses of the characteristics of multivariate time series datasets and the CI/CD strategy. Our results conclude that the CD approach has higher capacity but often lacks robustness to accurately predict distributionally drifted time series. In contrast, the CI approach trades capacity for robust prediction. Practical measures inspired by these analyses are proposed to address the capacity and robustness dilemma, including a modified CD method called Predict Residuals with Regularization (PRReg) that can surpass the CI strategy. We hope our findings can raise awareness among researchers about the characteristics of multivariate time series and inspire the construction of better forecasting models.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; FMS:B; 
LB  - Han2024Capacity
ER  -

TY  - JOUR
AU  - Hu, J.
AU  - Lu, Y.
AU  - Zhang, S.
AU  - Cao, L.
TI  - ISTR: Mask-Embedding-Based Instance Segmentation Transformer
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 2895
EP  - 2907
DO  - 10.1109/TIP.2024.3385980
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190340907&doi=10.1109%2fTIP.2024.3385980&partnerID=40&md5=f111903244212494570cc176bd10b935
AB  - Transformer-based instance-level recognition has attracted increasing research attention recently due to the superior performance. However, although attempts have been made to encode masks as embeddings into Transformer-based frameworks, how to combine mask embeddings and spatial information for a transformer-based approach is still not fully explored. In this paper, we revisit the design of mask-embedding-based pipelines and propose an Instance Segmentation TRansformer (ISTR) with Mask Meta-Embeddings (MME), leveraging the strengths of transformer models in encoding embedding information and incorporating spatial information from mask embeddings. ISTR incorporates a recurrent refining head that consists of a Dynamic Box Predictor (DBP), a Mask Information Generator (MIG), and a Mask Meta-Decoder (MMD). To improve the quality of mask embeddings, MME interprets the mask encoding-decoding processes as a mutual information maximization problem, which unifies the objective functions of different decoding schemes such as Principal Component Analysis (PCA) and Discrete Cosine Transform (DCT) with a meta-formulation. Under the meta-formulation, a learnable Spatial Mask Tuner (SMT) is further proposed, which fuses the spatial and embedding information produced from MIG and can significantly boost the segmentation performance. The resulting varieties, i.e., ISTR-PCA, ISTR-DCT, and ISTR-SMT, demonstrate the effectiveness and efficiency of incorporating mask embeddings with the query-based instance segmentation pipelines. On the COCO dataset, ISTR surpasses all predominant mask-embedding-based models by a large margin, and achieves competitive performance compared to concurrent state-of-the-art models. On the Cityscapes dataset, ISTR also outperforms several strong baselines. Our code has been made available at: https://github.com/hujiecpp/ISTR.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Bai, J.
AU  - Qin, H.
AU  - Lai, S.
AU  - Guo, J.
AU  - Guo, Y.
TI  - GLPanoDepth: Global-to-Local Panoramic Depth Estimation
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 2936
EP  - 2949
DO  - 10.1109/TIP.2024.3386403
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190733092&doi=10.1109%2fTIP.2024.3386403&partnerID=40&md5=e7b64bb058b5faa36a7617ef033af748
AB  - Depth estimation is a fundamental task in many vision applications. With the popularity of omnidirectional cameras, it becomes a new trend to tackle this problem in the spherical space. In this paper, we propose a learning-based method for predicting dense depth values of a scene from a monocular omnidirectional image. An omnidirectional image has a full field-of-view, providing much more complete descriptions of the scene than perspective images. However, fully-convolutional networks that most current solutions rely on fail to capture rich global contexts from the panorama. To address this issue and also the distortion of equirectangular projection in the panorama, we propose Cubemap Vision Transformers (CViT), a new transformer-based architecture that can model long-range dependencies and extract distortion-free global features from the panorama. We show that cubemap vision transformers have a global receptive field at every stage and can provide globally coherent predictions for spherical signals. As a general architecture, it removes any restriction that has been imposed on the panorama in many other monocular panoramic depth estimation methods. To preserve important local features, we further design a convolution-based branch in our pipeline (dubbed GLPanoDepth) and fuse global features from cubemap vision transformers at multiple scales. This global-to-local strategy allows us to fully exploit useful global and local features in the panorama, achieving state-of-the-art performance in panoramic depth estimation.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Liu, Y.
AU  - Li, B.
AU  - Wang, X.
AU  - Sammut, C.
AU  - Yao, L.
TI  - Attention-Aware Social Graph Transformer Networks for Stochastic Trajectory Prediction
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 11
SP  - 5633
EP  - 5646
DO  - 10.1109/TKDE.2024.3390765
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190774864&doi=10.1109%2fTKDE.2024.3390765&partnerID=40&md5=00f3dc739b1adb9ec716400cd3ec9e5f
AB  - Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; FMS:B; 
LB  - Liu2024Attention-Aware
ER  -

TY  - JOUR
AU  - Liu, D.
AU  - Wang, Y.
AU  - Liu, C.
AU  - Yuan, X.
AU  - Wang, K.
AU  - Yang, C.
TI  - Scope-Free Global Multi-Condition-Aware Industrial Missing Data Imputation Framework via Diffusion Transformer
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 11
SP  - 6977
EP  - 6988
DO  - 10.1109/TKDE.2024.3392897
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191355452&doi=10.1109%2fTKDE.2024.3392897&partnerID=40&md5=789f0811e199ce2c080d03c50fbce95b
AB  - Missing data is a common phenomenon in the industrial field. The recovery of missing data is crucial to enhance the reliability of subsequent data-driven monitoring and control of industrial processes. Most existing methods are limited by the confined scope of feature extraction, which makes it impossible to rely on global information to impute missing data. In addition, they usually assume that industrial data is a uniform distribution across all working conditions, ignoring the differences in data evolution patterns across different conditions. To address these issues, this paper proposes an innovative scope-free global multi-condition-aware imputation framework based on diffusion transformer (SGMCAI-DiT). First, it extends the diffusion model by introducing conditional probability to capture the condition distribution of the entire data. Then, a noise prediction model is designed based on a novel double-weighted attention mechanism (DW-SA) to broaden the horizons of feature extraction. By discerning the inter-conditional interactions and the intra-conditional local information, the missing data imputation performance can be improved. Finally, the effectiveness and suitability of the proposed SGMCAI-DiT are verified on four real datasets sourced from industrial processes and two public non-industrial datasets. Extensive experimental results demonstrate that the proposed method outperforms several state-of-the-art methods in different missing data scenarios.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; FMS:B; 
LB  - Liu2024Scope-Free
ER  -

TY  - JOUR
AU  - Li, G.
AU  - Cheng, D.
AU  - Wang, N.
AU  - Li, J.
AU  - Gao, X.
TI  - Neighbor-Guided Pseudo-Label Generation and Refinement for Single-Frame Supervised Temporal Action Localization
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 2419
EP  - 2430
DO  - 10.1109/TIP.2024.3378477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188892680&doi=10.1109%2fTIP.2024.3378477&partnerID=40&md5=6cbd190f01c5d7c83dea3df73e0dffd3
AB  - Due to the sparse single-frame annotations, current Single-Frame Temporal Action Localization (SF-TAL) methods generally employ threshold-based pseudo-label generation strategies. However, these approaches suffer from inefficient data utilization, as only parts of unlabeled frames with confidence scores surpassing a predefined threshold are selected for training. Moreover, the variability of single-frame annotations and unreliable model predictions introduce pseudo-label noise. To address these challenges, we propose two strategies by using the relationship of the video segments with their neighbors': 1) temporal neighbor-guided soft pseudo-label generation (TNPG); and 2) semantic neighbor-guided pseudo-label refinement (SNPR). TNPG utilizes a local-global self-attention mechanism in a transformer encoder to capture temporal neighbor information while focusing on the whole video. Then the generated self-attention map is multiplied by the network predictions to propagate information between labeled and unlabeled frames, and produce soft pseudo-label for all segments. Despite this, label noise persists due to unreliable model predictions. To mitigate this, SNPR refines pseudo-labels based on the assumption that predictions should resemble their semantic nearest neighbors'. Specifically, we search for semantic nearest neighbors of each video segment by cosine similarity in the feature space. Then the refined soft pseudo-labels can be obtained by a weight combination of the original pseudo-label and the semantic nearest neighbors'. Finally, the model can be trained with the refined pseudo-labels, and the performance has been greatly improved. Comprehensive experimental results on different benchmarks show that we achieve state-of-the-art performances on THUMOS14, ActivityNet1.2, and ActivityNet1.3 datasets.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Huang, H.
AU  - Zhan, W.
AU  - Min, G.
AU  - Duan, Z.
AU  - Peng, K.
TI  - Mobility-Aware Computation Offloading With Load Balancing in Smart City Networks Using MEC Federation
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 11
SP  - 10411
EP  - 10428
DO  - 10.1109/TMC.2024.3376377
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188728003&doi=10.1109%2fTMC.2024.3376377&partnerID=40&md5=88b0bc0585559bbc7bec9c25b2c4ad35
AB  - Internet-of-Things (IoT) has played a critical role in developing sustainable smart cities and emerging numerous latency-sensitive IoT applications. Mobile edge computing (MEC) federation has the capability to incorporate a transparent resource management approach, which enables the sharing and utilization of MEC services from edge infrastructure providers (EIPs) and provides agile access services to mobile devices (MDs). In this paper, we investigate the joint optimization problem of computation offloading, task migration, and resource allocation in the MEC federation. The objective is to minimize the weighted sum of latency and energy consumption while maintaining load balancing under the constraint of the long-term migration cost budget of EIPs. To address the problem, we decompose it into two sub-problems: 1) the MDs clustering sub-problem and 2) the sub-problem of joint computation offloading, task migration, and resource allocation. First, an MDs clustering matching (MDCM) algorithm is proposed to cluster the MDs in edge servers (ESs) according to the differences in channel gains. Afterward, the second sub-problem is simplified by the Lyapunov optimization technique, and then we propose a Transformer-based mobility prediction model and a decentralized deep deterministic policy gradient (DDPG)-based framework to solve it. Extensive simulation results demonstrate the cost-efficiency of the proposed algorithm.  © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Su, H.
AU  - Wang, S.
AU  - Yang, S.
AU  - Huang, T.
AU  - Ren, X.
TI  - Reducing Traffic Wastage in Video Streaming via Bandwidth-Efficient Bitrate Adaptation
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 11
SP  - 10361
EP  - 10377
DO  - 10.1109/TMC.2024.3373498
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187329503&doi=10.1109%2fTMC.2024.3373498&partnerID=40&md5=dfec8a0262dded8b42ed14294e584560
AB  - Bitrate adaptation (also known as ABR) is a crucial technique to improve the quality of experience (QoE) for video streaming applications. However, existing ABR algorithms suffer from severe traffic wastage, which refers to the traffic cost of downloading the video segments that users do not finally consume, for example, due to early departure or video skipping. In this paper, we carefully formulate the dynamics of buffered data volume (BDV), a strongly correlated indicator of traffic wastage, which, to the best of our knowledge, is the first time to rigorously clarify the effect of downloading plans on potential wastage. To reduce wastage while keeping a high QoE, we present a bandwidth-efficient bitrate adaptation algorithm (named BE-ABR), achieving consistently low BDV without distinct QoE losses. Specifically, we design a precise, time-aware transmission delay prediction model over the Transformer architecture, and develop a fine-grained buffer control scheme. Through extensive experiments conducted on emulated and real network environments including WiFi, 4G, and 5G, we demonstrate that BE-ABR performs well in both QoE and bandwidth savings, enabling a 60.87% wastage reduction and a comparable, or even better, QoE, compared to the state-of-the-art methods. © 2024 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Liang, J.
AU  - Cao, J.
AU  - Fan, Y.
AU  - Zhang, K.
AU  - Ranjan, R.
AU  - Li, Y.
AU  - Timofte, R.
AU  - Van Gool, L.
TI  - VRT: A Video Restoration Transformer
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 2171
EP  - 2182
DO  - 10.1109/TIP.2024.3372454
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187318275&doi=10.1109%2fTIP.2024.3372454&partnerID=40&md5=75479a177a7485f651cc0a7615a9ead0
AB  - Video restoration aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which are restricted by frame-by-frame restoration. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction ability. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal reciprocal self attention (TRSA) and parallel warping. TRSA divides the video into small clips, on which reciprocal attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on five tasks, including video super-resolution, video deblurring, video denoising, video frame interpolation and space-time video super-resolution, demonstrate that VRT outperforms the state-of-the-art methods by large margins (up to 2.16dB) on fourteen benchmark datasets. The codes are available at https://github.com/JingyunLiang/VRT.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 33
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Deng, J.
AU  - Chen, X.
AU  - Jiang, R.
AU  - Yang, Y.
AU  - Song, X.
AU  - Tsang, I.W.
TI  - Disentangling Structured Components: Towards Adaptive, Interpretable and Scalable Time Series Forecasting
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 8
SP  - 3783
EP  - 3800
DO  - 10.1109/TKDE.2024.3371931
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187014950&doi=10.1109%2fTKDE.2024.3371931&partnerID=40&md5=3e6617d9885f5f26c2a9e7cef0ad1d23
AB  - Multivariate time-series (MTS) forecasting is a paramount and fundamental problem in many real-world applications. The core issue in MTS forecasting is how to effectively model complex spatial-temporal patterns. In this paper, we develop a adaptive, interpretable and scalable forecasting framework, which seeks to individually model each component of the spatial-temporal patterns. We name this framework SCNN, as an acronym of Structured Component-based Neural Network. SCNN works with a pre-defined generative process of MTS, which arithmetically characterizes the latent structure of the spatial-temporal patterns. In line with its reverse process, SCNN decouples MTS data into structured and heterogeneous components and then respectively extrapolates the evolution of these components, the dynamics of which are more traceable and predictable than the original MTS. Extensive experiments are conducted to demonstrate that SCNN can achieve superior performance over state-of-the-art models on three real-world datasets. Additionally, we examine SCNN with different configurations and perform in-depth analyses of the properties of SCNN.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - CCF:A期刊; FMS:B; 
LB  - Deng2024Disentangling
ER  -

TY  - JOUR
AU  - Chai, Z.
AU  - Qin, H.
TI  - Dynamic Motion Transition: A Hybrid Data-driven and Model-driven Method for Human Pose Transitions
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
SP  - 1
EP  - 14
DO  - 10.1109/TVCG.2024.3372421
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187380431&doi=10.1109%2fTVCG.2024.3372421&partnerID=40&md5=aa98f4d42e8895708790d138bb08fd07
AB  - The rapid, accurate, and robust computation of virtual human figures&#x0027; &#x201D;in-between&#x201D; pose transitions from available and sometimes sparse inputs is of fundamental significance to 3D interactive graphics and computer animation. Various methods have been proposed to produce natural lifelike transitions of human pose automatically in recent decades. Nevertheless, conventional pure model-driven methods require heuristic knowledge (e.g., least motion guided by physics laws) and ad-hoc clues (e.g., splines with non-uniform time warp) that are difficult to obtain, learn, and infer. With the fast emergence of large-scale datasets readily available to animators in the most recent years, deep models afford a powerful alternative to tackle the aforementioned challenges. However, pure data-driven methods still suffer from the remaining challenges such as unseen data in practice and less generative power in model/domain/data transfer, and the measurement of the generative power has always been omitted in these works. In essence, data-driven methods solely rely on the qualities and quantities of training datasets. In this paper, we propose a hybrid approach built upon the seamless integration of data-driven and model-driven methods, called Dynamic Motion Transition (DMT), with the following salient modeling advantages: (1) The data augmentation capability based on the limited human locomotion data capture and the concept of force-derived directly from physical laws; (2) Force learning by which skeleton joints are driven to move, and the Conditional Temporal Transformer (CTT) being trained to learn the force change in the local range, both at the fine level; and (3) At the coarse level, the effective and flexible creation of the subsequent step motion using Dynamic Movement Primitives (DMP) until the target is reached. Our extensive experiments have confirmed that our model can outperform the state-of-the-art methods under the newly devised metric by virtue of the least action loss function. In addition, our novel method and system are of immediate benefit to many other animation tasks such as motion synthesis and control, and motion tracking and prediction in this <bold>bigdata</bold> graphics era. IEEE
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Wang, Z.
AU  - Li, Z.
AU  - Wang, L.
TI  - Sparse Action Tube Detection
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 1740
EP  - 1752
DO  - 10.1109/TIP.2024.3368958
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187199802&doi=10.1109%2fTIP.2024.3368958&partnerID=40&md5=041a1942f450a353ecbe761949866a04
AB  - Action tube detection is a challenging task as it requires not only to locate action instances in each frame, but also link them in time. Existing action tube detection methods often employ multi-stage pipelines with complex designs and time-consuming linking procedure. In this paper, we present a simple end-to-end action tube detection method, termed as Sparse Tube Detector (STDet). Unlike those dense action detectors, our core idea is to use a set of learnable tube queries and directly decode them into action tubes (i.e., a set of tracked boxes with action label) from video content. This sparse detection paradigm shares several advantages. First, the large number of hand-crafted anchor candidates in dense action detectors is greatly reduced to a small number of learnable tubes, which results in a more efficient detection framework. Second, our learnable tube queries directly attend the whole video content, which endows our method with the capacity of capturing long-range information for action detection. Finally, our action detector is an end-to-end tube detection without requiring the linking procedure, which directly and explicitly predicts the action boundary instead of depending on the linking strategy. Extensive experiments shows that our STDet outperforms the previous state-of-the-art methods on two challenging untrimmed video action detection datasets of UCF101-24 and MultiSports. We hope our method will be an simple end-to-end tube detection baseline and can inspire new ideas in this direction.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ren, T.
AU  - Yu, J.
AU  - Guo, S.
AU  - Ma, Y.
AU  - Ouyang, Y.
AU  - Zeng, Z.
AU  - Zhang, Y.
AU  - Qin, Y.
TI  - Diverse Motion In-betweening from Sparse Keyframes with Dual Posture Stitching
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
SP  - 1
EP  - 12
DO  - 10.1109/TVCG.2024.3363457
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808193&doi=10.1109%2fTVCG.2024.3363457&partnerID=40&md5=fd88c92ba0734fc4fa3a2df3a4bba3cf
AB  - In-betweening is a technique for generating transitions given start and target character states. The majority of existing works require multiple (often <inline-formula><tex-math notation="LaTeX">$\geq$</tex-math></inline-formula> 10) frames as input, which are not always available. In addition, they produce results that lack diversity, which may not fulfill artists&#x0027; requirements. Addressing these gaps, our work deals with a focused yet challenging problem: generating diverse and high-quality transitions given exactly two frames (only the start and target frames). To cope with this challenging scenario, we propose a bi-directional motion generation and stitching scheme which generates forward and backward transitions from the start and target frames with two adversarial autoregressive networks, respectively, and stitches them midway between the start and target frames. In contrast to stitching at the start or target frames, where the ground truth cannot be altered, there is no strict midway ground truth. Thus, our method can capitalize on this flexibility and generate high-quality and diverse transitions simultaneously. Specifically, we employ conditional variational autoencoders (CVAEs) to implement our autoregressive networks and propose a novel stitching loss to stitch the bi-directional generated motions around the midway point. IEEE
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Luo, J.
AU  - Zhao, N.
AU  - Li, W.
AU  - Richardt, C.
TI  - CRefNet: Learning Consistent Reflectance Estimation With a Decoder-Sharing Transformer
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 9
SP  - 6407
EP  - 6420
DO  - 10.1109/TVCG.2023.3337870
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184831901&doi=10.1109%2fTVCG.2023.3337870&partnerID=40&md5=a544e2cdeb5ef9d56146eda9d48f039a
AB  - We present CRefNet, a hybrid transformer-convolutional deep neural network for consistent reflectance estimation in intrinsic image decomposition. Estimating consistent reflectance is particularly challenging when the same material appears differently due to changes in illumination. Our method achieves enhanced global reflectance consistency via a novel transformer module that converts image features to reflectance features. At the same time, this module also exploits long-range data interactions. We introduce reflectance reconstruction as a novel auxiliary task that shares a common decoder with the reflectance estimation task, and which substantially improves the quality of reconstructed reflectance maps. Finally, we improve local reflectance consistency via a new rectified gradient filter that effectively suppresses small variations in predictions without any overhead at inference time. Our experiments show that our contributions enable CRefNet to predict highly consistent reflectance maps and to outperform the state of the art by 10% WHDR.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhao, L.
AU  - Wei, Y.
AU  - Li, J.
AU  - Zhou, J.
AU  - Lu, J.
TI  - Structure-Aware Cross-Modal Transformer for Depth Completion
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 1016
EP  - 1031
DO  - 10.1109/TIP.2024.3355807
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184014464&doi=10.1109%2fTIP.2024.3355807&partnerID=40&md5=7bcc2e2177799b347879b85e83327c28
AB  - In this paper, we present a Structure-aware Cross-Modal Transformer (SCMT) to fully capture the 3D structures hidden in sparse depths for depth completion. Most existing methods learn to predict dense depths by taking depths as an additional channel of RGB images or learning 2D affinities to perform depth propagation. However, they fail to exploit 3D structures implied in the depth channel, thereby losing the informative 3D knowledge that provides important priors to distinguish the foreground and background features. Moreover, since these methods rely on the color textures of 2D images, it is challenging for them to handle poor-texture regions without the guidance of explicit 3D cues. To address this, we disentangle the hierarchical 3D scene-level structure from the RGB-D input and construct a pathway to make sharp depth boundaries and object shape outlines accessible to 2D features. Specifically, we extract 2D and 3D features from depth inputs and the back-projected point clouds respectively by building a two-stream network. To leverage 3D structures, we construct several cross-modal transformers to adaptively propagate multi-scale 3D structural features to the 2D stream, energizing 2D features with priors of object shapes and local geometries. Experimental results show that our SCMT achieves state-of-the-art performance on three popular outdoor (KITTI) and indoor (VOID and NYU) benchmarks. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Coscia, A.
AU  - Endert, A.
TI  - KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 9
SP  - 6520
EP  - 6532
DO  - 10.1109/TVCG.2023.3346713
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181568877&doi=10.1109%2fTVCG.2023.3346713&partnerID=40&md5=c42dff64e6646f64573f7f6892750300
AB  - Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yang, Z.
AU  - Li, Y.
AU  - Zhou, G.
TI  - Unsupervised Sensor-Based Continuous Authentication with Low-Rank Transformer Using Learning-to-Rank Algorithms
PY  - 2024
T2  - IEEE Transactions on Mobile Computing
VL  - 23
IS  - 9
SP  - 8839
EP  - 8854
DO  - 10.1109/TMC.2024.3353209
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182946207&doi=10.1109%2fTMC.2024.3353209&partnerID=40&md5=131460ab55ad2c40df64477a9c1d804a
AB  - With the rapid development of the Internet of Things (IoTs) and mobile communications, mobile devices have become indispensable in our daily lives. Given the substantial amount of private information stored on these devices, the security of mobile devices has emerged as a significant concern for users. Different from conventional methods such as PINs, fingerprints, and face IDs, which authenticate users only during the initial login stage, continuous authentication ensures consistent verification while mobile devices are in use. Current continuous authentication methods require extensive data from a series of users for effective training. Nevertheless, it is challenging to collect sufficient amount of data within a limited time. In this paper, we propose CALL, an unsupervised sensor-based Continuous Authentication system with a Low-rank transformer using Learning-to-rank algorithms. The lightweight CALL is capable of providing both spatial and temporal features for end-to-end authentication. Specifically, CALL utilizes time series data from a legitimate user, collected by the accelerometer, gyroscope, and magnetometer sensors on smartphones, to train a pure one-dimensional autoencoder for spatial features and a shuffle low-rank Transformer (SLRT) for temporal features in the training phase. In the authentication phase, the trained pure one-dimensional autoencoder captures spatial features by reconstructing input data to obtain the reconstruction error, and SLRT captures temporal features by predicting a ranking vector that reveals the order of the shuffled feature sequence. The predicted ranking vector is then used to recover the shuffled sequence and the similarity between the frequency spectrum sequences of the recovered sequence and the original time series data is calculated. The reconstruction error and similarity are compared against pre-defined thresholds, and CALL authenticates a user as legitimate only if both values fall below their respective thresholds. Finally, we evaluate the performance of CALL on UCI-HAR, WISDM-HARB, and our dataset, and the extensive experiments illustrate that CALL reaches the best performance with 96.43%, 95.24% and 96.92% accuracy, and 4.28%, 4.76% and 3.86% EERs on the three datasets, outperforming state-of-the-art continuous authentication methods.  © 2002-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Pu, T.
AU  - Chen, T.
AU  - Wu, H.
AU  - Lu, Y.
AU  - Lin, L.
TI  - Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 556
EP  - 568
DO  - 10.1109/TIP.2023.3345652
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181579636&doi=10.1109%2fTIP.2023.3345652&partnerID=40&md5=33e889ffebfddb1817d4a8601f15810e
AB  - Video scene graph generation (VidSGG) aims to identify objects in visual scenes and infer their relationships for a given video. It requires not only a comprehensive understanding of each object scattered on the whole scene but also a deep dive into their temporal motions and interactions. Inherently, object pairs and their relationships enjoy spatial co-occurrence correlations within each image and temporal consistency/transition correlations across different images, which can serve as prior knowledge to facilitate VidSGG model learning and inference. In this work, we propose a spatial-temporal knowledge-embedded transformer (STKET) that incorporates the prior spatial-temporal knowledge into the multi-head cross-attention mechanism to learn more representative relationship representations. Specifically, we first learn spatial co-occurrence and temporal transition correlations in a statistical manner. Then, we design spatial and temporal knowledge-embedded layers that introduce the multi-head cross-attention mechanism to fully explore the interaction between visual representation and the knowledge to generate spatial- and temporal-embedded representations, respectively. Finally, we aggregate these representations for each subject-object pair to predict the final semantic labels and their relationships. Extensive experiments show that STKET outperforms current competing algorithms by a large margin, e.g., improving the mR@50 by 8.1%, 4.7%, and 2.1% on different settings over current algorithms.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Du, H.
AU  - Yuan, H.
AU  - Zhao, P.
AU  - Wang, D.
AU  - Sheng, V.S.
AU  - Liu, Y.
AU  - Liu, G.
AU  - Zhao, L.
TI  - Feature-Aware Contrastive Learning With Bidirectional Transformers for Sequential Recommendation
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 12
SP  - 8192
EP  - 8205
DO  - 10.1109/TKDE.2023.3343345
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181563688&doi=10.1109%2fTKDE.2023.3343345&partnerID=40&md5=a980c0e49c3e5203ccec6f6c536d2ee7
AB  - Contrastive learning with Transformer-based sequence encoder has gained predominance for sequential recommendation due to its ability to mitigate the data noise and the data sparsity issue. However, existing contrastive learning approaches for sequential recommendation still suffer from two limitations. First, they mainly center on left-to-right unidirectional Transformers as base encoders, which are suboptimal for sequential recommendation because user behaviors may not be a rigid left-to-right sequence. Second, they devise contrastive learning objectives only from the sequence level, neglecting the rich self-supervision signals from the feature level. To address these limitations, we propose a novel framework called Feature-aware Contrastive Learning with bidirectional Transformers for sequential Recommendation (FCLRec) to effectively leverage feature information for sequential recommendation. Specifically, we first augment bidirectional Transformers with a novel feature-aware self-attention module that is able to simultaneously model the complex relationships between sequences and features. Next, we propose a novel feature-aware contrastive learning objective that generates a collection of positive samples via three types of augmentations from three different levels. Finally, we adopt feature prediction as an auxiliary task to strengthen the connections between items and features. Our experimental results on four public benchmark datasets show that FCLRec outperforms the state-of-the-art methods for sequential recommendation. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Du2024Feature-Aware
ER  -

TY  - JOUR
AU  - Yang, M.
AU  - Svirsky, Y.
AU  - Cheng, Z.
AU  - Sharf, A.
TI  - Self-Supervised Fragment Alignment With Gaps
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 9
SP  - 6235
EP  - 6246
DO  - 10.1109/TVCG.2023.3330859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177027270&doi=10.1109%2fTVCG.2023.3330859&partnerID=40&md5=ea03a0cc23479de72529301679c76e77
AB  - Image alignment and registration methods typically rely on visual correspondences across common regions and boundaries to guide the alignment process. Without them, the problem becomes significantly more challenging. Nevertheless, in real world, image fragments may be corrupted with no common boundaries and little or no overlap. In this work, we address the problem of learning the alignment of image fragments with gaps (i.e., without common boundaries or overlapping regions). Our setting is unsupervised, having only the fragments at hand with no ground truth to guide the alignment process. This is usually the situation in the restoration of unique archaeological artifacts such as frescoes and mosaics. Hence, we suggest a self-supervised approach utilizing self-examples which we generate from the existing data and then feed into an adversarial neural network. Our idea is that available information inside fragments is often sufficiently rich to guide their alignment with good accuracy. Following this observation, our method splits the initial fragments into sub-fragments yielding a set of aligned pieces. Thus, sub-fragmentation allows exposing new alignment relations and revealing inner structures and feature statistics. In fact, the new sub-fragments construct true and false alignment relations between fragments. We feed this data to a spatial transformer GAN which learns to predict the alignment between fragments gaps. We test our technique on various synthetic datasets as well as large scale frescoes and mosaics. Results demonstrate our method's capability to learn the alignment of deteriorated image fragments in a self-supervised manner, by examining inner image statistics for both synthetic and real data.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yang, J.
AU  - Du, B.
AU  - Wang, D.
AU  - Zhang, L.
TI  - ITER: Image-to-Pixel Representation for Weakly Supervised HSI Classification
PY  - 2024
T2  - IEEE Transactions on Image Processing
VL  - 33
SP  - 257
EP  - 272
DO  - 10.1109/TIP.2023.3326699
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178065057&doi=10.1109%2fTIP.2023.3326699&partnerID=40&md5=f988679d20c501ed51bbff5e38c2f69a
AB  - Recent years have witnessed the superiority of deep learning-based algorithms in the field of HSI classification. However, a prerequisite for the favorable performance of these methods is a large number of refined pixel-level annotations. Due to atmospheric changes, sensor differences, and complex land cover distribution, pixel-level labeling of high-dimensional hyperspectral image (HSI) is extremely difficult, time-consuming, and laborious. To overcome the above hurdle, an Image-To-pixEl Representation (ITER) approach is proposed in this paper. To the best of our knowledge, this is the first time that image-level annotation is introduced to predict pixel-level classification maps for HSI. The proposed model is along the lines of subject modeling to boundary refinement, corresponding to pseudo-label generation and pixel-level prediction. Concretely, in the pseudo-label generation part, the spectral/spatial activation, spectral-spatial alignment loss, and geographic element enhancement are sequentially designed to locate discriminate regions of each category, optimize multi-domain class activation map (CAM) collaborative training, and refine labels, respectively. For the pixel-level prediction portion, a high frequency-aware self-attention in a high-enhanced transformer is put forward to achieve detailed feature representation. With the two-stage pipeline, ITER explores weakly supervised HSI classification with image-level tags, bridging the gap between image-level annotation and dense prediction. Extensive experiments in three benchmark datasets with state-of-the-art (SOTA) works show the performance of the proposed approach.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 9
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhao, S.
AU  - Xu, T.
AU  - Wu, X.-J.
AU  - Kittler, J.
TI  - Pluggable Attack for Visual Object Tracking
PY  - 2024
T2  - IEEE Transactions on Information Forensics and Security
VL  - 19
SP  - 1227
EP  - 1240
DO  - 10.1109/TIFS.2023.3331899
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177047801&doi=10.1109%2fTIFS.2023.3331899&partnerID=40&md5=c6d6a5f58f5218beaaaf50214442d363
AB  - Performing adversarial attacks on a visual tracker aims to drift the apparent target to the background by adding malicious perturbations to the source images. Demonstrating convincingly their ability to decrease accuracy, existing tracking attackers mislead the target predictions at the decision level, but this is tracker design specific, narrowing their applicability to other tracking approaches. In contrast, we advocate that attacks be performed by corrupting the feature-level clues, i.e., the feature representations extracted by deep networks. The proposed approach provides a general attacking framework for backbone-head tracking architectures. Motivated by the knowledge that the quality of intermediate-level features strongly influences the decision making, four intermediate-level attack methods are proposed to maximise the difference between the feature distributions of natural and adversarial samples, thus decoupling the attack strategies from the form of the output of specific victim trackers. Interestingly, our intermediate-level attacks are compatible with existing decision-level attacks, thus a joint optimisation of these two kinds of adversarial objective functions has the potential to achieve better attacking performance. Hence, the proposed adversarial attack methodology can be used in conjunction with several mainstream tracking paradigms (Discriminative correlation filters, Siamese networks, and Transformer trackers), demonstrating its pluggability. The experimental results on four popular benchmarks, e.g., OTB100, UAV123, LaSOT, and TLP, verify that our method can produce impressive and consistent accuracy degeneration. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Gao, L.
AU  - Shao, Z.
AU  - Luo, Z.
AU  - Hu, H.
AU  - Turkay, C.
AU  - Chen, S.
TI  - TransforLearn: Interactive Visual Tutorial for the Transformer Model
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 1
SP  - 891
EP  - 901
DO  - 10.1109/TVCG.2023.3327353
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181176396&doi=10.1109%2fTVCG.2023.3327353&partnerID=40&md5=89dd3543a8897b02cb98706ca5b7e4f7
AB  - The widespread adoption of Transformers in deep learning, serving as the core framework for numerous large-scale language models, has sparked significant interest in understanding their underlying mechanisms. However, beginners face difficulties in comprehending and learning Transformers due to its complex structure and abstract data representation. We present TransforLearn, the first interactive visual tutorial designed for deep learning beginners and non-experts to comprehensively learn about Transformers. TransforLearn supports interactions for architecture-driven exploration and task-driven exploration, providing insight into different levels of model details and their working processes. It accommodates interactive views of each layer's operation and mathematical formula, helping users to understand the data flow of long text sequences. By altering the current decoder-based recursive prediction results and combining the downstream task abstractions, users can deeply explore model processes. Our user study revealed that the interactions of TransforLearn are positively received. We observe that TransforLearn facilitates users' accomplishment of study tasks and a grasp of key concepts in Transformer effectively. © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Xue, H.
AU  - Salim, F.D.
TI  - PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 11
SP  - 6851
EP  - 6864
DO  - 10.1109/TKDE.2023.3342137
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180312993&doi=10.1109%2fTKDE.2023.3342137&partnerID=40&md5=bca373a2d2f15c16341915fc006b2915
AB  - This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes three real-world forecasting scenarios. We evaluate different SOTA numerical-based forecasting methods and language generation models. The benchmark results with various forecasting settings demonstrate the proposed PromptCast with language generation models is a promising research direction. Additionally, in comparison to conventional numerical-based forecasting, PromptCast shows a much better generalization ability under the zero-shot setting.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; FMS:B; 
LB  - Xue2024PromptCast
ER  -

TY  - JOUR
AU  - Hou, J.
AU  - Dong, Z.
AU  - Zhou, J.
AU  - Liu, Z.
TI  - Discovering Predictable Latent Factors for Time Series Forecasting
PY  - 2024
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 36
IS  - 10
SP  - 5106
EP  - 5119
DO  - 10.1109/TKDE.2023.3335240
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178060240&doi=10.1109%2fTKDE.2023.3335240&partnerID=40&md5=10cb4e92cfdab0b280a199732131c27a
AB  - Modern temporal modeling methods, such as Transformer and its variants, have demonstrated remarkable capabilities in handling sequential data from specific domains like language and vision. Though achieving high performance with large-scale data, they often have redundant or unexplainable structures. When encountering some real-world datasets with limited observable variables that can be affected by many unknown factors, these methods may struggle to identify meaningful patterns and dependencies inherent in data, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this article, we develop a novel algorithmic framework for inferring latent factors implied by the observed temporal data. The inferred factors are used to form multiple predictable and independent signal components that enable not only the reconstruction of future time series for accurate prediction but also sparse relation reasoning for long-term efficiency. To achieve this, we introduce three characteristics, i.e., predictability, sufficiency, and identifiability, and model these characteristics of latent factors via powerful deep latent dynamics models to infer the predictable signal components. Empirical results on multiple real datasets show the efficiency of our method for different kinds of time series forecasting tasks. Statistical analyses validate the predictability and interpretability of the learned latent factors.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; FMS:B; 
LB  - Hou2024Discovering
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Kang, D.
AU  - Pei, W.
AU  - Zhe, X.
AU  - Zhang, Y.
AU  - Bao, L.
AU  - He, Z.
TI  - Audio2Gestures: Generating Diverse Gestures from Audio
PY  - 2024
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 8
SP  - 4752
EP  - 4766
DO  - 10.1109/TVCG.2023.3276973
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162927469&doi=10.1109%2fTVCG.2023.3276973&partnerID=40&md5=f6d2e867f44dfcec990dfed1ea221414
AB  - People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (i.e., RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (e.g. STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (e.g. PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Quan, W.
AU  - Deng, P.
AU  - Wang, K.
AU  - Yan, D.-M.
TI  - CGFormer: ViT-Based Network for Identifying Computer-Generated Images With Token Labeling
PY  - 2024
T2  - IEEE Transactions on Information Forensics and Security
VL  - 19
SP  - 235
EP  - 250
DO  - 10.1109/TIFS.2023.3322083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174835083&doi=10.1109%2fTIFS.2023.3322083&partnerID=40&md5=7fe1429366cef304203d229f391395d2
AB  - The advanced graphics rendering techniques and image generation algorithms significantly improve the visual quality of computer-generated (CG) images, and this makes it more challenging to distinguish between CG images and natural images (NIs) for a forensic detector. For the identification of CG images, human beings often need to inspect and evaluate the entire image and its local region as well. In addition, we observe that the distributions of both near and far patch-wise correlation have differences between CG images and NIs. Current mainstream methods adopt the CNN-based architecture with the classical cross entropy loss, however, there are several limitations: 1) the weakness of long-distance relationship modeling of image content due to the local receptive field of CNN; 2) the pixel sensitivity due to the convolutional computation; 3) the insufficient supervision due to the training loss on the whole image. In this paper, we propose a novel vision transformer (ViT)-based network with token labeling for CG image identification. Our network, called CGFormer, consists of patch embedding, feature modeling, and token prediction. We apply patch embedding to sequence the input image and weaken the pixel sensitivity. Stacked multi-head attention-based transformer blocks are utilized to model the patch-wise relationship and introduce a certain level of adaptability. Besides the conventional classification loss on class token of the whole image, we additionally introduce a soft cross entropy loss on patch tokens to comprehensively exploit the supervision information from local patches. Extensive experiments demonstrate that our method achieves the state-of-the-art forensic performance on six publicly available datasets in terms of classification accuracy, generalization, and robustness. Code is available at https://github.com/feipiefei/CGFormer.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wei, Z.
AU  - Zhang, X.
AU  - Ji, Z.
AU  - Li, J.
AU  - Wei, J.
TI  - Revisit and Benchmarking of Automated Quantization Toward Fair Comparison
PY  - 2024
T2  - IEEE Transactions on Computers
VL  - 73
IS  - 1
SP  - 18
EP  - 29
DO  - 10.1109/TC.2023.3315836
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171599273&doi=10.1109%2fTC.2023.3315836&partnerID=40&md5=b12accbda6954c8fdef69fb1e12d5719
AB  - —Automated quantization has emerged as an entirely new design paradigm to automate the optimal configuration of bitwidth for deep neural networks (DNNs), making the DNN more memory-efficient and faster to execute on hardware with limited resources. Reinforcement learning (RL) and differentiable neural architecture search (DNAS) are two main solution paths that have shown their superiority. Yet, there are countless methods with various implementations within each path. It has been hard to comprehend their differences and make a relatively fair comparison due to the lack of a benchmark framework and a clear analysis of which aspects are common, respectively distinct, between different implementations. To this end, we introduce BenQ to pave the way towards fair comparisons in two separate race tracks, i.e., intra-comparison of the RL-based and the DNAS-based methods, respectively. We provide a systematic approach, which helps to reveal relatively vital aspects of different implementations. Finally, we conduct comprehensive experiments on VGG, AlexNet, ResNet, GoogleNet, MobileNet-V2, and Vision Transformer (ViT), and the new observations shed light on potential future directions for automated quantization to move forward. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Liu, L.
AU  - Xu, X.
AU  - Lin, Z.
AU  - Liang, J.
AU  - Yan, S.
TI  - Towards Garment Sewing Pattern Reconstruction from a Single Image
PY  - 2023
T2  - ACM Transactions on Graphics
VL  - 42
IS  - 6
C7  - 200
DO  - 10.1145/3618319
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178591438&doi=10.1145%2f3618319&partnerID=40&md5=28824aa9e5e32f6253618ce757e8b23e
AB  - Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models will be released. © 2023 ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wu, R.
AU  - Su, W.
AU  - Ma, K.
AU  - Liao, J.
TI  - IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers
PY  - 2023
T2  - ACM Transactions on Graphics
VL  - 42
IS  - 6
C7  - 230
DO  - 10.1145/3618364
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179619797&doi=10.1145%2f3618364&partnerID=40&md5=8b97ed0d4b214a8278980cd3ddb4cf44
AB  - Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text → raster image → vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text → vector graphics script) through pretrained large language models. Nevertheless, these methods suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively (using the FID and CLIP scores) and qualitatively (through formal subjective user studies). Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion. © 2023 ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yang, Y.
AU  - Bao, R.
AU  - Guo, W.
AU  - Zhan, D.-C.
AU  - Yin, Y.
AU  - Yang, J.
TI  - Deep visual-linguistic fusion network considering cross-modal inconsistency for rumor detection
PY  - 2023
T2  - Science China Information Sciences
VL  - 66
IS  - 12
C7  - 222102
DO  - 10.1007/s11432-021-3530-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175723658&doi=10.1007%2fs11432-021-3530-7&partnerID=40&md5=5a92eedfd15a6e4196738882ce63cc77
AB  - With the development of the Internet, users can freely publish posts on various social media platforms, which offers great convenience for keeping abreast of the world. However, posts usually carry many rumors, which require plenty of manpower for monitoring. Owing to the success of modern machine learning techniques, especially deep learning models, we tried to detect rumors as a classification problem automatically. Early attempts have always focused on building classifiers relying on image or text information, i.e., single modality in posts. Thereafter, several multimodal detection approaches employ an early or late fusion operator for aggregating multiple source information. Nevertheless, they only take advantage of multimodal embeddings for fusion and ignore another important detection factor, i.e., the intermodal inconsistency between modalities. To solve this problem, we develop a novel deep visual-linguistic fusion network (DVLFN) considering cross-modal inconsistency, which detects rumors by comprehensively considering modal aggregation and contrast information. Specifically, the DVLFN first utilizes visual and textual deep encoders, i.e., Faster R-CNN and bidirectional encoder representations from transformers, to extract global and regional embeddings for image and text modalities. Then, it predicts posts’ authenticity from two aspects: (1) intermodal inconsistency, which employs the Wasserstein distance to efficiently measure the similarity between regional embeddings of different modalities, and (2) modal aggregation, which experimentally employs the early fusion to aggregate two modal embeddings for prediction. Consequently, the DVLFN can compose the final prediction based on the modal fusion and inconsistency measure. Experiments are conducted on three real-world multimedia rumor detection datasets collected from Reddit, GoodNews, and Weibo. The results validate the superior performance of the proposed DVLFN. © 2023, Science China Press.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Xu, X.
AU  - Li, Y.-L.
AU  - Lu, C.
TI  - Dynamic Context Removal: A General Training Strategy for Robust Models on Video Action Predictive Tasks
PY  - 2023
T2  - International Journal of Computer Vision
VL  - 131
IS  - 12
SP  - 3272
EP  - 3288
DO  - 10.1007/s11263-023-01850-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167917308&doi=10.1007%2fs11263-023-01850-6&partnerID=40&md5=8661cbae24126e05763c4158cd211750
AB  - Predicting future actions is an essential feature of intelligent systems and embodied AI. However, compared to the traditional recognition tasks, the uncertainty of the future and the reasoning ability requirement make prediction tasks very challenging and far beyond solved. In this field, previous methods usually care more about the model architecture design but little attention has been put on how to train models with a proper learning policy. To this end, in this work, we propose a simple but effective training strategy, Dynamic Context Removal (DCR), which dynamically schedules the visibility of context in different training stages. It follows the human-like curriculum learning process, i.e., gradually removing the event context to increase the prediction difficulty till satisfying the final prediction target. Besides, we explore how to train robust models that give consistent predictions at different levels of observable context. Our learning scheme is plug-and-play and easy to integrate widely-used reasoning models including Transformer and LSTM, with advantages in both effectiveness and efficiency. We study two action prediction problems, i.e., Video Action Anticipation and Early Action Recognition. In extensive experiments, our method achieves state-of-the-art results on several widely-used benchmarks. © 2023, The Author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Cohen, L.A.
AU  - Samuelson, N.L.
AU  - Wang, T.
AU  - Taniguchi, T.
AU  - Watanabe, K.
AU  - Zaletel, M.P.
AU  - Young, A.F.
TI  - Universal chiral Luttinger liquid behavior in a graphene fractional quantum Hall point contact
PY  - 2023
T2  - Science
VL  - 382
IS  - 6670
SP  - 542
EP  - 547
DO  - 10.1126/science.adf9728
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176200613&doi=10.1126%2fscience.adf9728&partnerID=40&md5=6a22d4a7d0715399e046e493d92a908c
AB  - One-dimensional conductors are described by Luttinger liquid theory, which predicts a power-law suppression of the single-electron tunneling density of states at low voltages. The scaling exponent is predicted to be quantized when tunneling into a single isolated chiral edge state of the fractional quantum Hall effect. We report conductance measurements across a point contact linking integer and fractional quantum Hall edge states (at fillings 1 and31, respectively). At weak coupling, we observe the predicted universal quadratic scaling with temperature and voltage. At strong coupling, we demonstrate perfect Andreev reflection of fractionalized quasiparticles at the point contact. We use the strong coupling physics to realize a nearly dissipationless direct current voltage step-up transformer, whose gain arises directly from topological fractionalization of electrical charge. © 2023 American Association for the Advancement of Science. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 8
C2  - ZUFE:TOP; 
ER  -

TY  - JOUR
AU  - Bai, J.
AU  - He, Z.
AU  - Yang, S.
AU  - Guo, J.
AU  - Chen, Z.
AU  - Zhang, Y.
AU  - Guo, Y.
TI  - Local-to-Global Panorama Inpainting for Locale-Aware Indoor Lighting Prediction
PY  - 2023
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 29
IS  - 11
SP  - 4405
EP  - 4416
DO  - 10.1109/TVCG.2023.3320233
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174812976&doi=10.1109%2fTVCG.2023.3320233&partnerID=40&md5=403d42e813352e74811c6ee63a214b28
AB  - Predicting panoramic indoor lighting from a single perspective image is a fundamental but highly ill-posed problem in computer vision and graphics. To achieve locale-aware and robust prediction, this problem can be decomposed into three sub-tasks: depth-based image warping, panorama inpainting and high-dynamic-range (HDR) reconstruction, among which the success of panorama inpainting plays a key role. Recent methods mostly rely on convolutional neural networks (CNNs) to fill the missing contents in the warped panorama. However, they usually achieve suboptimal performance since the missing contents occupy a very large portion in the panoramic space while CNNs are plagued by limited receptive fields. The spatially-varying distortion in the spherical signals further increases the difficulty for conventional CNNs. To address these issues, we propose a local-to-global strategy for large-scale panorama inpainting. In our method, a depth-guided local inpainting is first applied on the warped panorama to fill small but dense holes. Then, a transformer-based network, dubbed PanoTransformer, is designed to hallucinate reasonable global structures in the large holes. To avoid distortion, we further employ cubemap projection in our design of PanoTransformer. The high-quality panorama recovered at any locale helps us to capture spatially-varying indoor illumination with physically-plausible global structures and fine details.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, P.
AU  - Quan, W.
AU  - Guo, J.
AU  - Yan, D.-M.
TI  - Layout-aware Single-image Document Flattening
PY  - 2023
T2  - ACM Transactions on Graphics
VL  - 43
IS  - 1
C7  - 9
DO  - 10.1145/3627818
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182600735&doi=10.1145%2f3627818&partnerID=40&md5=ee5d784bb3022cf2deb2a1dd54d3c01d
AB  - Single image rectification of document deformation is a challenging task. Although some recent deep learning-based methods have attempted to solve this problem, they cannot achieve satisfactory results when dealing with document images with complex deformations. In this article, we propose a new efficient framework for document flattening. Our main insight is that most layout primitives in a document have rectangular outline shapes, making unwarping local layout primitives essentially homogeneous with unwarping the entire document. The former task is clearly more straightforward to solve than the latter due to the more consistent texture and relatively smooth deformation. On this basis, we propose a layout-aware deep model working in a divide-and-conquer manner. First, we employ a transformer-based segmentation module to obtain the layout information of the input document. Then a new regression module is applied to predict the global and local UV maps. Finally, we design an effective merging algorithm to correct the global prediction with local details. Both quantitative and qualitative experimental results demonstrate that our framework achieves favorable performance against state-of-the-art methods. In addition, the current publicly available document flattening datasets have limited 3D paper shapes without layout annotation and also lack a general geometric correction metric. Therefore, we build a new large-scale synthetic dataset by utilizing a fully automatic rendering method to generate deformed documents with diverse shapes and exact layout segmentation labels. We also propose a new geometric correction metric based on our paired document UV maps. Code and dataset will be released at https://github.com/BunnySoCrazy/LA-DocFlatten.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, G.
AU  - Zhong, S.
AU  - Deng, X.
AU  - Xiang, L.
AU  - Gary Chan, S.-H.
AU  - Li, R.
AU  - Liu, Y.
AU  - Zhang, M.
AU  - Hung, C.-C.
AU  - Peng, W.-C.
TI  - A Lightweight and Accurate Spatial-Temporal Transformer for Traffic Forecasting
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 11
SP  - 10967
EP  - 10980
DO  - 10.1109/TKDE.2022.3233086
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147217804&doi=10.1109%2fTKDE.2022.3233086&partnerID=40&md5=d5cb2579192f9e85577009896e534c7e
AB  - We study the forecasting problem for traffic with dynamic, possibly periodical, and joint spatial-temporal dependency between regions. Given the aggregated inflow and outflow traffic of regions in a city from time slots 0 to t-1, we predict the traffic at time t for any region. Prior arts in the area often considered the spatial and temporal dependencies in a decoupled manner, or were rather computationally intensive in training with a large number of hyper-parameters which needed tuning. We propose ST-TIS, a novel, lightweight and accurate Spatial-Temporal Transformer with information fusion and region sampling for traffic forecasting. ST-TIS extends the canonical Transformer with information fusion and region sampling. The information fusion module captures the complex spatial-temporal dependency between regions. The region sampling module is to improve the efficiency and prediction accuracy, cutting the computation complexity for dependency learning from O(n2) to O(nn), where n is the number of regions. With far fewer parameters than state-of-the-art deep learning models, ST-TIS's offline training is significantly faster in terms of tuning and computation (with a reduction of up to 90% on training time and network parameters). Notwithstanding such training efficiency, extensive experiments show that ST-TIS is substantially more accurate in online prediction than state-of-the-art approaches (with an average improvement of 9.5% on RMSE, and 12.4% on MAPE compared to STDN and DSAN). © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; FMS:B; 
LB  - Li2023Lightweight
ER  -

TY  - JOUR
AU  - Zhu, Z.
AU  - Tong, H.
AU  - Wang, Y.
AU  - Li, Y.
TI  - BL-GAN: Semi-Supervised Bug Localization via Generative Adversarial Network
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 11
SP  - 11112
EP  - 11125
DO  - 10.1109/TKDE.2022.3225329
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144084847&doi=10.1109%2fTKDE.2022.3225329&partnerID=40&md5=1b5b996afcde95f5d1205ef303335788
AB  - Various automated bug localization technologies have recently emerged that require adequate bug-fix records available to train a predictive model. However, many projects in practice might not provide these necessities, especially for new projects in the first release, due to the expensive human effort for constructing a large amount of bug-fix records. Aiming to capture the potential relevance distribution between the bug report and code file from a limited number of available bug-fix records, we present the first semi-supervised bug localization model named BL-GAN in this paper. For this purpose, the promising Generative Adversarial Network is introduced in BL-GAN, in which synthetic bug-fix records close to the real ones are constructed by searching the project directory tree to generate file paths instead of traversing the contents of all code files. For processing bug reports, the proposed BL-GAN adopts an attention-based Transformer architecture to capture semantic and sequence information. In order to capture the proprietary structural information in code files, BL-GAN incorporates a novel multilayer Graph Convolutional Network to process the source code in a graphical view. Extensive experiments on large-scale real-world datasets reveal that our model BL-GAN significantly outperforms the state-of-the-art on all evaluation measures.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Zhu2023BL-GAN
ER  -

TY  - JOUR
AU  - Li, K.
AU  - Guo, D.
AU  - Wang, M.
TI  - ViGT: proposal-free video grounding with a learnable token in the transformer
PY  - 2023
T2  - Science China Information Sciences
VL  - 66
IS  - 10
C7  - 202102
DO  - 10.1007/s11432-022-3783-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173558275&doi=10.1007%2fs11432-022-3783-3&partnerID=40&md5=be57f50be730b9187576565efee6613c
AB  - The video grounding (VG) task aims to locate the queried action or event in an untrimmed video based on rich linguistic descriptions. Existing proposal-free methods are trapped in the complex interaction between video and query, overemphasizing cross-modal feature fusion and feature correlation for VG. In this paper, we propose a novel boundary regression paradigm that performs regression token learning in a transformer. Particularly, we present a simple but effective proposal-free framework, namely video grounding transformer (ViGT), which predicts the temporal boundary using a learnable regression token rather than multi-modal or cross-modal features. In ViGT, the benefits of a learnable token are manifested as follows. (1) The token is unrelated to the video or the query and avoids data bias toward the original video and query. (2) The token simultaneously performs global context aggregation from video and query features. First, we employed a sharing feature encoder to project both video and query into a joint feature space before performing cross-modal co-attention (i.e., video-to-query attention and query-to-video attention) to highlight discriminative features in each modality. Furthermore, we concatenated a learnable regression token [REG] with the video and query features as the input of a vision-language transformer. Finally, we utilized the token [REG] to predict the target moment and visual features to constrain the foreground and background probabilities at each timestamp. The proposed ViGT performed well on three public datasets: ANet-Captions, TACoS, and YouCookII. Extensive ablation studies and qualitative analysis further validated the interpretability of ViGT. © 2023, Science China Press.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 23
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Fu, M.
AU  - Nguyen, V.
AU  - Tantithamthavorn, C.(k).
AU  - Le, T.
AU  - Phung, D.
TI  - VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types
PY  - 2023
T2  - IEEE Transactions on Software Engineering
VL  - 49
IS  - 10
SP  - 4550
EP  - 4565
DO  - 10.1109/TSE.2023.3305244
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168263380&doi=10.1109%2fTSE.2023.3305244&partnerID=40&md5=915c3a0a9c829c9a2c0c80dd0a28f186
AB  - Deep learning-based vulnerability prediction approaches are proposed to help under-resourced security practitioners to detect vulnerable functions. However, security practitioners still do not know what type of vulnerabilities correspond to a given prediction (aka CWE-ID). Thus, a novel approach to explain the type of vulnerabilities for a given prediction is imperative. In this paper, we propose VulExplainer, an approach to explain the type of vulnerabilities. We represent VulExplainer as a vulnerability classification task. However, vulnerabilities have diverse characteristics (i.e., CWE-IDs) and the number of labeled samples in each CWE-ID is highly imbalanced (known as a highly imbalanced multi-class classification problem), which often lead to inaccurate predictions. Thus, we introduce a Transformer-based hierarchical distillation for software vulnerability classification in order to address the highly imbalanced types of software vulnerabilities. Specifically, we split a complex label distribution into sub-distributions based on CWE abstract types (i.e., categorizations that group similar CWE-IDs). Thus, similar CWE-IDs can be grouped and each group will have a more balanced label distribution. We learn TextCNN teachers on each of the simplified distributions respectively, however, they only perform well in their group. Thus, we build a transformer student model to generalize the performance of TextCNN teachers through our hierarchical knowledge distillation framework. Through an extensive evaluation using the real-world 8,636 vulnerabilities, our approach outperforms all of the baselines by 5%-29%. The results also demonstrate that our approach can be applied to Transformer-based architectures such as CodeBERT, GraphCodeBERT, and CodeGPT. Moreover, our method maintains compatibility with any Transformer-based model without requiring any architectural modifications but only adds a special distillation token to the input. These results highlight our significant contributions towards the fundamental and practical problem of explaining software vulnerability.  © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 13
C2  - CCF:A期刊; FMS:B; 
LB  - Fu2023VulExplainer
ER  -

TY  - JOUR
AU  - Jiayi, X.I.E.
AU  - Chen, Z.
TI  - Hierarchical Transformer with Spatio-temporal Context Aggregation for Next Point-of-interest Recommendation
PY  - 2023
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 2
C7  - 37
DO  - 10.1145/3597930
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181535550&doi=10.1145%2f3597930&partnerID=40&md5=30d851067f7fdb7ad81556dc4124616f
AB  - Next point-of-interest (POI) recommendation is a critical task in location-based social networks, yet remains challenging due to a high degree of variation and personalization exhibited in user movements. In this work, we explore the latent hierarchical structure composed of multi-granularity short-term structural patterns in user check-in sequences. We propose a Spatio-Temporal context AggRegated Hierarchical Transformer (STAR-HiT) for next POI recommendation, which employs stacked hierarchical encoders to recursively encode the spatio-temporal context and explicitly locate subsequences of different granularities. More specifically, in each encoder, the global attention layer captures the spatio-temporal context of the sequence, while the local attention layer performed within each subsequence enhances subsequence modeling using the local context. The sequence partition layer infers positions and lengths of subsequences from the global context adaptively, such that semantics in subsequences can be well preserved. Finally, the subsequence aggregation layer fuses representations within each subsequence to form the corresponding subsequence representation, thereby generating a new sequence of higher-level granularity. The stacking of hierarchical encoders captures the latent hierarchical structure of the check-in sequence, which is used to predict the next visiting POI. Extensive experiments on three public datasets demonstrate that the proposed model achieves superior performance while providing explanations for recommendations. © 2023 Association for Computing Machinery. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; FMS:B; 
LB  - Jiayi2023Hierarchical
ER  -

TY  - JOUR
AU  - Xie, P.
AU  - Ma, M.
AU  - Li, T.
AU  - Ji, S.
AU  - Du, S.
AU  - Yu, Z.
AU  - Zhang, J.
TI  - Spatio-Temporal Dynamic Graph Relation Learning for Urban Metro Flow Prediction
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 10
SP  - 9973
EP  - 9984
DO  - 10.1109/TKDE.2023.3269771
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159688094&doi=10.1109%2fTKDE.2023.3269771&partnerID=40&md5=9dbfafd8ee6fa5758e990381e3747389
AB  - Urban metro flow prediction is of great value for metro operation scheduling, passenger flow management and personal travel planning. However, the problem is challenging. First, different metro stations, e.g. transfer stations and non-transfer stations have unique traffic patterns. Second, it is difficult to model complex spatio-temporal dynamic relation of metro stations. To address these challenges, we develop a spatio-temporal dynamic graph relational learning model (STDGRL) to predict urban metro station flow. First, we propose a spatio-temporal node embedding representation module to capture the traffic patterns of different stations. Second, we employ a dynamic graph relationship learning module to learn dynamic spatial relationships between metro stations without a predefined graph adjacency matrix. Finally, we provide a transformer-based long-term relationship prediction module for long-term metro flow prediction. Extensive experiments are conducted based on metro data in four cities, China, with experimental results demonstrating the advantages of our method compared over 14 baselines for urban metro flow prediction. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 15
C2  - CCF:A期刊; FMS:B; 
LB  - Xie2023Spatio-Temporal
ER  -

TY  - JOUR
AU  - Yang, Y.
AU  - Zhang, C.
AU  - Song, X.
AU  - Dong, Z.
AU  - Zhu, H.
AU  - Li, W.
TI  - Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation
PY  - 2023
T2  - ACM Transactions on Information Systems
VL  - 42
IS  - 2
C7  - 33
DO  - 10.1145/3597022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165636537&doi=10.1145%2f3597022&partnerID=40&md5=203a3b2760ce14aae74a718a7b0e1c9a
AB  - Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines. © 2023 Association for Computing Machinery. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 14
C2  - CCF:A期刊; FMS:B; 
LB  - Yang2023Contextualized
ER  -

TY  - JOUR
AU  - Huang, Y.
AU  - Ma, L.
AU  - Li, Y.
TI  - PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing
PY  - 2023
T2  - ACM Transactions on Software Engineering and Methodology
VL  - 32
IS  - 6
C7  - 154
DO  - 10.1145/3591870
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168641672&doi=10.1145%2f3591870&partnerID=40&md5=43621e43fe2a26159e8b5f228c95a828
AB  - In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting abnormal inputs instead of training a robust model and asking it to give reliable results for every input, which may inevitably compromise accuracy. Specifically, each input is tested by voting over multiple inferences with different mutated attention masks, where at least one inference is guaranteed to exclude the abnormal patch. This can be seen as complete-coverage testing, which could provide a statistical guarantee on inference at the test time. Our comprehensive evaluation demonstrates that PatchCensor is able to achieve high certified accuracy (e.g., 67.1% on ImageNet for 2%-pixel adversarial patches), significantly outperforming state-of-the-art techniques while achieving similar clean accuracy (81.8% on ImageNet). The clean accuracy is the same as vanilla ViT models. Meanwhile, our technique also supports flexible configurations to handle different adversarial patch sizes by simply changing the masking strategy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, R.
AU  - Zhang, F.
AU  - Li, T.
AU  - Zhang, N.
AU  - Zhang, T.
TI  - DMGAN: Dynamic Multi-Hop Graph Attention Network for Traffic Forecasting
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 9
SP  - 9088
EP  - 9101
DO  - 10.1109/TKDE.2022.3221316
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141640181&doi=10.1109%2fTKDE.2022.3221316&partnerID=40&md5=c312dc7b8c90d9e7693d09725b9827f2
AB  - In the intelligent transportation system, traffic forecasting, which is generally characterized as a graph spatial-temporal prediction task, plays a crucial role. It is challenging to generate reliable forecast results due to the complexity of traffic topological information and the inherent uncertainty of road traffic circumstances. Existing works generally focus on modeling spatial dependency on static graph structures, but ignore dynamic relations between road segments and cannot extract long-range traffic dependencies in spatial-temporal domains. To bridge the above gaps, we present a novel framework, called Dynamic Multi-Hop Graph Attention Network (DMGAN). Specifically, we leverage dynamic graph modeling to capture time-varying relations across road sections and introduce the multi-hop operation in each message propagation layer to extract long-range spatial dependency. Meanwhile, we develop a fusion-attention module, preserving both local and global hidden layer outputs of the encoder, to capture both long- and short-term temporal dependencies jointly. In this way, our method can fully model complex time-varying traffic topology information and capture the internal patterns of traffic series by integrating dynamic graph structure and temporal attention component. DGMAN achieves state-of-the-art performance in three metrics, as demonstrated by experimental findings on four real-world public traffic datasets, METR-LA, PEMS-BAY, PEMS03, and PEMS07.  © 2022 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 9
C2  - CCF:A期刊; FMS:B; 
LB  - Li2023DMGAN
ER  -

TY  - JOUR
AU  - Xiao, M.
AU  - Qiao, Z.
AU  - Fu, Y.
AU  - Dong, H.
AU  - Du, Y.
AU  - Wang, P.
AU  - Xiong, H.
AU  - Zhou, Y.
TI  - Hierarchical Interdisciplinary Topic Detection Model for Research Proposal Classification
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 9
SP  - 9685
EP  - 9699
DO  - 10.1109/TKDE.2023.3248608
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149419699&doi=10.1109%2fTKDE.2023.3248608&partnerID=40&md5=3ca018483ff00bc96c93b10fa597794c
AB  - The peer merit review of research proposals has been the major mechanism to decide grant awards. However, research proposals have become increasingly interdisciplinary. It has been a longstanding challenge to assign interdisciplinary proposals to appropriate reviewers so proposals are fairly evaluated. One of the critical steps in reviewer assignment is to generate accurate interdisciplinary topic labels for proposal-reviewer matching. Existing systems mainly collect topic labels manually generated by principle investigators. However, such human-reported labels can be non-accurate, incomplete, labor intensive, and time costly. What role can AI play in developing a fair and precise proposal reviewer assignment system? In this study, we collaborate with the National Science Foundation of China to address the task of automated interdisciplinary topic path detection. For this purpose, we develop a deep Hierarchical Interdisciplinary Research Proposal Classification Network (HIRPCN). Specifically, we first propose a hierarchical transformer to extract the textual semantic information of proposals. We then design an interdisciplinary graph and leverage GNNs to learn representations of each discipline in order to extract interdisciplinary knowledge. After extracting the semantic and interdisciplinary knowledge, we design a level-wise prediction component to fuse the two types of knowledge representations and detect interdisciplinary topic paths for each proposal. We conduct extensive experiments and expert evaluations on three real-world datasets to demonstrate the effectiveness of our proposed model.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; FMS:B; 
LB  - Xiao2023Hierarchical
ER  -

TY  - JOUR
AU  - Trevithick, A.
AU  - Chan, M.
AU  - Stengel, M.
AU  - Chan, E.
AU  - Liu, C.
AU  - Yu, Z.
AU  - Khamis, S.
AU  - Chandraker, M.
AU  - Ramamoorthi, R.
AU  - Nagano, K.
TI  - Real-Time Radiance Fields for Single-Image Portrait View Synthesis
PY  - 2023
T2  - ACM Transactions on Graphics
VL  - 42
IS  - 4
C7  - 135
DO  - 10.1145/3592460
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166288923&doi=10.1145%2f3592460&partnerID=40&md5=b81a407ad644189d59000ef79be4fe1b
AB  - We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.  © 2023 Owner/Author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 28
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Lin, Z.
AU  - Zang, S.
AU  - Wang, R.
AU  - Sun, Z.
AU  - Senthilnath, J.
AU  - Xu, C.
AU  - Kwoh, C.K.
TI  - Attention Over Self-Attention: Intention-Aware Re-Ranking With Dynamic Transformer Encoders for Recommendation
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 8
SP  - 7782
EP  - 7795
DO  - 10.1109/TKDE.2022.3208633
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139473253&doi=10.1109%2fTKDE.2022.3208633&partnerID=40&md5=007c2a6ff05ab612006d714f19179004
AB  - Re-ranking models refine item recommendation lists generated by the prior global ranking model, which have demonstrated their effectiveness in improving the recommendation quality. However, most existing re-ranking solutions only learn from implicit feedback with a shared prediction model, which regrettably ignore inter-item relationships under diverse user intentions. In this paper, we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer Encoder (RAISE), aiming to perform user-specific prediction for each individual user based on her intentions. Specifically, we first propose to mine latent user intentions from text reviews with an intention discovering module (IDM). By differentiating the importance of review information with a co-attention network, the latent user intention can be explicitly modeled for each user-item pair. We then introduce a dynamic transformer encoder (DTE) to capture user-specific inter-item relationships among item candidates by seamlessly accommodating the learned latent user intentions via IDM. As such, one can not only achieve more personalized recommendations but also obtain corresponding explanations by constructing RAISE upon existing recommendation engines. Empirical study on four public datasets shows the superiority of our proposed RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by Precision@5, MAP@5, and NDCG@5 respectively.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; FMS:B; 
LB  - Lin2023Attention
ER  -

TY  - JOUR
AU  - Deng, J.
AU  - Chen, X.
AU  - Jiang, R.
AU  - Song, X.
AU  - Tsang, I.W.
TI  - A Multi-View Multi-Task Learning Framework for Multi-Variate Time Series Forecasting
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 8
SP  - 7665
EP  - 7680
DO  - 10.1109/TKDE.2022.3218803
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141598882&doi=10.1109%2fTKDE.2022.3218803&partnerID=40&md5=845356357cf485662a5d8851c974025b
AB  - Multi-variate time series (MTS) data is a ubiquitous class of data abstraction in the real world. Any instance of MTS is generated from a hybrid dynamical system and their specific dynamics are usually unknown. The hybrid nature of such a dynamical system is a result of complex external attributes, such as geographic location and time of day, each of which can be categorized into either spatial attributes or temporal attributes. Therefore, there are two fundamental views which can be used to analyze MTS data, namely the spatial view and the temporal view. Moreover, from each of these two views, we can partition the set of data samples of MTS into disjoint forecasting tasks in accordance with their associated attribute values. Then, samples of the same task will manifest similar forthcoming pattern, which is less sophisticated to be predicted in comparison with the original single-view setting. Considering this insight, we propose a novel multi-view multi-task (MVMT) learning framework for MTS forecasting. Instead of being explicitly presented in most scenarios, MVMT information is deeply concealed in the MTS data, which severely hinders the model from capturing it naturally. To this end, we develop two kinds of basic operations, namely task-wise affine transformation and task-wise normalization, respectively. Applying these two operations with prior knowledge on the spatial and temporal view allows the model to adaptively extract MVMT information while predicting. Extensive experiments on three datasets are conducted to illustrate that canonical architectures can be greatly enhanced by the MVMT learning framework in terms of both effectiveness and efficiency. In addition, we design rich case studies to reveal the properties of representations produced at different phases in the entire prediction procedure.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 14
C2  - CCF:A期刊; FMS:B; 
LB  - Deng2023Multi-View
ER  -

TY  - JOUR
AU  - Fang, Y.
AU  - Zhao, X.
AU  - Chen, Y.
AU  - Xiao, W.
AU  - De Rijke, M.
TI  - PF-HIN:Pre-Training for Heterogeneous Information Networks
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 8
SP  - 8372
EP  - 8385
DO  - 10.1109/TKDE.2022.3206597
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139385999&doi=10.1109%2fTKDE.2022.3206597&partnerID=40&md5=676c38b733409cd7b95323c657ebdbba
AB  - In network representation learning we learn how to represent heterogeneous information networks in a low-dimensional space so as to facilitate effective search, classification, and prediction solutions. Previous network representation learning methods typically require sufficient task-specific labeled data to address domain-specific problems. The trained model usually cannot be transferred to out-of-domain datasets. We propose a self-supervised pre-training and fine-tuning framework, PF-HIN, to capture the features of a heterogeneous information network. Unlike traditional network representation learning models that have to train the entire model all over again for every downstream task and dataset, PF-HIN only needs to fine-tune the model and a small number of extra task-specific parameters, thus improving model efficiency and effectiveness. During pre-training, we first transform the neighborhood of a given node into a sequence. PF-HIN is pre-trained based on two self-supervised tasks, masked node modeling and adjacent node prediction. We adopt deep bi-directional transformer encoders to train the model, and leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In the fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification, and node clustering. PF-HIN outperforms state-of-the-art alternatives on each of these tasks, on four datasets.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; FMS:B; 
LB  - Fang2023PF-HIN:Pre-Training
ER  -

TY  - JOUR
AU  - Shi, C.
AU  - Cai, B.
AU  - Zhao, Y.
AU  - Gao, L.
AU  - Sood, K.
AU  - Xiang, Y.
TI  - CoSS: Leveraging Statement Semantics for Code Summarization
PY  - 2023
T2  - IEEE Transactions on Software Engineering
VL  - 49
IS  - 6
SP  - 3472
EP  - 3486
DO  - 10.1109/TSE.2023.3256362
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151331220&doi=10.1109%2fTSE.2023.3256362&partnerID=40&md5=f688da43a2e74f73561d98236d57f320
AB  - Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.  © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; FMS:B; 
LB  - Shi2023CoSS
ER  -

TY  - JOUR
AU  - Yu, L.
AU  - Sun, L.
AU  - Du, B.
AU  - Liu, C.
AU  - Lv, W.
AU  - Xiong, H.
TI  - Heterogeneous Graph Representation Learning With Relation Awareness
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 6
SP  - 5935
EP  - 5947
DO  - 10.1109/TKDE.2022.3160208
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126648622&doi=10.1109%2fTKDE.2022.3160208&partnerID=40&md5=4a6132ea03543206c07e430305230b02
AB  - Representation learning on heterogeneous graphs aims to obtain meaningful node representations to facilitate various downstream tasks, such as node classification and link prediction. Existing heterogeneous graph learning methods are primarily developed by following the propagation mechanism of node representations. There are few efforts on studying the role of relations for improving the learning of more fine-grained node representations. Indeed, it is important to collaboratively learn the semantic representations of relations and discern node representations with respect to different relation types. To this end, in this paper, we propose a Relation-aware Heterogeneous Graph Neural Network, namely R-HGNN, to learn node representations on heterogeneous graphs at a fine-grained level by considering relation-aware characteristics. Specifically, a dedicated graph convolution component is first designed to learn unique node representations from each relation-specific graph separately. Then, a cross-relation message passing module is developed to improve the interactions of node representations across different relations. Also, the relation representations are learned in a layer-wise manner to capture relation semantics, which are used to guide the node representation learning process. Moreover, a semantic fusing module is presented to aggregate relation-aware node representations into a compact representation with the learned relation representations. Finally, we conduct extensive experiments on a variety of graph learning tasks, and experimental results demonstrate that our approach consistently outperforms existing methods among all the tasks. © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 35
C2  - CCF:A期刊; FMS:B; 
LB  - Yu2023Heterogeneous
ER  -

TY  - JOUR
AU  - Xia, L.
AU  - Huang, C.
AU  - Xu, Y.
AU  - Pei, J.
TI  - Multi-Behavior Sequential Recommendation With Temporal Graph Transformer
PY  - 2023
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 35
IS  - 6
SP  - 6099
EP  - 6112
DO  - 10.1109/TKDE.2022.3175094
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132509784&doi=10.1109%2fTKDE.2022.3175094&partnerID=40&md5=c41be6a89b65aa8708a1b8dc7e33c288
AB  - Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in many online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the historical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential recommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive behaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning from informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately characterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of multi-behavior interactive patterns. Towards this end, we propose a new Temporal Graph Transformer (TGT) recommendation framework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations across different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated knowledge for type-specific behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets indicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation codes are available at https://github.com/akaxlh/TGT.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 20
C2  - CCF:A期刊; FMS:B; 
LB  - Xia2023Multi-Behavior
ER  -

TY  - JOUR
AU  - Tian, H.
AU  - Liu, K.
AU  - Li, Y.
AU  - Kaboré, A.K.
AU  - Koyuncu, A.
AU  - Habib, A.
AU  - Li, L.
AU  - Wen, J.
AU  - Klein, J.
AU  - Bissyandé, T.F.
TI  - The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches
PY  - 2023
T2  - ACM Transactions on Software Engineering and Methodology
VL  - 32
IS  - 4
C7  - 92
DO  - 10.1145/3576039
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237362&doi=10.1145%2f3576039&partnerID=40&md5=c5699fa7328a6c7f276d39f9de22523e
AB  - A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction. © 2023 Copyright held by the owner/author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ma, M.
AU  - Ren, P.
AU  - Chen, Z.
AU  - Ren, Z.
AU  - Liang, H.
AU  - Ma, J.
AU  - De Rijke, M.
TI  - Improving Transformer-based Sequential Recommenders through Preference Editing
PY  - 2023
T2  - ACM Transactions on Information Systems
VL  - 41
IS  - 3
C7  - 71
DO  - 10.1145/3564282
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159360887&doi=10.1145%2f3564282&partnerID=40&md5=00313813964ae8e974cccefd8919f954
AB  - One of the key challenges in sequential recommendation is how to extract and represent user preferences. Traditional methods rely solely on predicting the next item. But user behavior may be driven by complex preferences. Therefore, these methods cannot make accurate recommendations when the available information user behavior is limited. To explore multiple user preferences, we propose a transformer-based sequential recommendation model, named MrTransformer (Multi-preference Transformer). For training MrTransformer, we devise a preference-editing-based self-supervised learning (SSL) mechanism that explores extra supervision signals based on relations with other sequences. The idea is to force the sequential recommendation model to discriminate between common and unique preferences in different sequences of interactions. By doing so, the sequential recommendation model is able to disentangle user preferences into multiple independent preference representations so as to improve user preference extraction and representation.We carry out extensive experiments on five benchmark datasets. MrTransformer with preference editing significantly outperforms state-of-the-art sequential recommendation methods in terms of Recall, MRR, and NDCG. We find that long sequences of interactions from which user preferences are harder to extract and represent benefit most from preference editing.  © 2023 Association for Computing Machinery.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - CCF:A期刊; FMS:B; 
LB  - Ma2023Improving
ER  -

TY  - JOUR
AU  - Zhou, H.
AU  - Li, J.
AU  - Zhang, S.
AU  - Zhang, S.
AU  - Yan, M.
AU  - Xiong, H.
TI  - Expanding the prediction capacity in long sequence time-series forecasting
PY  - 2023
T2  - Artificial Intelligence
VL  - 318
C7  - 103886
DO  - 10.1016/j.artint.2023.103886
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149753708&doi=10.1016%2fj.artint.2023.103886&partnerID=40&md5=b5f7ea3329f84926248876ae5a587175
AB  - Many real-world applications show growing demand for the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) requires a higher prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to accommodate the capacity requirements. However, three real challenges that may have prevented expanding the prediction capacity in LSTF are that the Transformer is limited by quadratic time complexity, high memory usage, and slow inference speed under the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics. (i) a ProbSparse self-attention mechanism, which achieves O(Llog⁡L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling promotes dominating attention by convolutional operators. Besides, the halving of layer width is intended to reduce the expense of building a deeper network on extremely long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on ten large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem. © 2023
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 25
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Cui, J.
AU  - Chen, Z.
AU  - Zhou, A.
AU  - Wang, J.
AU  - Zhang, W.
TI  - Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing
PY  - 2023
T2  - ACM Transactions on Information Systems
VL  - 41
IS  - 4
C7  - 104
DO  - 10.1145/3580595
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172418631&doi=10.1145%2f3580595&partnerID=40&md5=eed805af088566ee93a38905a1111d14
AB  - Knowledge tracing, the goal of which is predicting students' future performance given their past question response sequences to trace their knowledge states, is pivotal for computer-Aided education and intelligent tutoring systems. Although many technical efforts have been devoted to modeling students based on their question-response sequences, fine-grained interaction modeling between question-response pairs within each sequence is underexplored. This causes question-response representations less contextualized and further limits student modeling. To address this issue, we first conduct a data analysis and reveal the existence of complex cross effects between different question-response pairs within a sequence. Consequently, we propose MRT-KT, a multi-relational transformer for knowledge tracing, to enable fine-grained interaction modeling between question-response pairs. It introduces a novel relation encoding scheme based on knowledge concepts and student performance. Comprehensive experimental results show that MRT-KT outperforms state-of-The-Art knowledge tracing methods on four widely-used datasets, validating the effectiveness of considering fine-grained interaction for knowledge tracing.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 20
C2  - CCF:A期刊; FMS:B; 
LB  - Cui2023Fine-Grained
ER  -

TY  - JOUR
AU  - Leonhardt, J.
AU  - Rudra, K.
AU  - Anand, A.
TI  - Extractive Explanations for Interpretable Text Ranking
PY  - 2023
T2  - ACM Transactions on Information Systems
VL  - 41
IS  - 4
C7  - 88
DO  - 10.1145/3576924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172691686&doi=10.1145%2f3576924&partnerID=40&md5=63fc2be6478748a262f4465202234e36
AB  - Neural document ranking models perform impressively well due to superior language understanding gained from pre-Training tasks. However, due to their complexity and large number of parameters these (typically transformer-based) models are often non-interpretable in that ranking decisions can not be clearly attributed to specific parts of the input documents.In this article, we propose ranking models that are inherently interpretable by generating explanations as a by-product of the prediction decision. We introduce the Select-And-Rank paradigm for document ranking, where we first output an explanation as a selected subset of sentences in a document. Thereafter, we solely use the explanation or selection to make the prediction, making explanations first-class citizens in the ranking process. Technically, we treat sentence selection as a latent variable trained jointly with the ranker from the final output. To that end, we propose an end-To-end training technique for Select-And-Rank models utilizing reparameterizable subset sampling using the Gumbel-max trick.We conduct extensive experiments to demonstrate that our approach is competitive to state-of-The-Art methods. Our approach is broadly applicable to numerous ranking tasks and furthers the goal of building models that are interpretable by design. Finally, we present real-world applications that benefit from our sentence selection method. © 2023 Association for Computing Machinery.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 8
C2  - CCF:A期刊; FMS:B; 
LB  - Leonhardt2023Extractive
ER  -

TY  - JOUR
AU  - Lyu, Y.
AU  - Jing, L.
AU  - Wang, J.
AU  - Guo, M.
AU  - Wang, X.
AU  - Yu, J.
TI  - Siamese transformer with hierarchical concept embedding for fine-grained image recognition
PY  - 2023
T2  - Science China Information Sciences
VL  - 66
IS  - 3
C7  - 132107
DO  - 10.1007/s11432-022-3586-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148206621&doi=10.1007%2fs11432-022-3586-y&partnerID=40&md5=db9a9fd620c8c61ff687a99b7fcbf428
AB  - Distinguishing the subtle differences among fine-grained images from subordinate concepts of a concept hierarchy is a challenging task. In this paper, we propose a Siamese transformer with hierarchical concept embedding (STrHCE), which contains two transformer subnetworks sharing all configurations, and each subnetwork is equipped with the hierarchical semantic information at different concept levels for fine-grained image embeddings. In particular, one subnetwork is for coarse-scale patches to learn the discriminative regions with the aid of the innate multi-head self-attention mechanism of the transformer. The other subnetwork is for finer-scale patches, which are adaptively sampled from the discriminative regions, to capture subtle yet discriminative visual cues and eliminate redundant information. STrHCE connects the two subnetworks through a score margin adjustor to enforce the most discriminative regions generating more confident predictions. Extensive experiments conducted on four commonly-used benchmark datasets, including CUB-200-2011, FGVC-Aircraft, Stanford Dogs, and NABirds, empirically demonstrate the superiority of the proposed STrHCE over state-of-the-art baselines. © 2023, Science China Press.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Chi, J.
AU  - Qu, Y.
AU  - Liu, T.
AU  - Zheng, Q.
AU  - Yin, H.
TI  - SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning
PY  - 2023
T2  - IEEE Transactions on Software Engineering
VL  - 49
IS  - 2
SP  - 564
EP  - 585
DO  - 10.1109/TSE.2022.3156637
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126293988&doi=10.1109%2fTSE.2022.3156637&partnerID=40&md5=d33f94d2bdbfca3e7c99a43c3a7b0430
AB  - Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmers' manual efforts. Developers need to deeply understand the vulnerability and affect the system's functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization). © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 25
C2  - CCF:A期刊; FMS:B; 
LB  - Chi2023SeqTrans
ER  -

TY  - JOUR
AU  - He, T.
AU  - Gao, L.
AU  - Song, J.
AU  - Li, Y.-F.
TI  - Toward a Unified Transformer-Based Framework for Scene Graph Generation and Human-Object Interaction Detection
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 6274
EP  - 6288
DO  - 10.1109/TIP.2023.3330304
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177065215&doi=10.1109%2fTIP.2023.3330304&partnerID=40&md5=3e4577c770c71329ad95ea2076630fd3
AB  - Scene graph generation (SGG) and human-object interaction (HOI) detection are two important visual tasks aiming at localising and recognising relationships between objects, and interactions between humans and objects, respectively. Prevailing works treat these tasks as distinct tasks, leading to the development of task-specific models tailored to individual datasets. However, we posit that the presence of visual relationships can furnish crucial contextual and intricate relational cues that significantly augment the inference of human-object interactions. This motivates us to think if there is a natural intrinsic relationship between the two tasks, where scene graphs can serve as a source for inferring human-object interactions. In light of this, we introduce SG2HOI+, a unified one-step model based on the Transformer architecture. Our approach employs two interactive hierarchical Transformers to seamlessly unify the tasks of SGG and HOI detection. Concretely, we initiate a relation Transformer tasked with generating relation triples from a suite of visual features. Subsequently, we employ another transformer-based decoder to predict human-object interactions based on the generated relation triples. A comprehensive series of experiments conducted across established benchmark datasets including Visual Genome, V-COCO, and HICO-DET demonstrates the compelling performance of our SG2HOI+ model in comparison to prevalent one-stage SGG models. Remarkably, our approach achieves competitive performance when compared to state-of-the-art HOI methods. Additionally, we observe that our SG2HOI+ jointly trained on both SGG and HOI tasks in an end-to-end manner yields substantial improvements for both tasks compared to individualized training paradigms.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Durrani, N.
AU  - Dalvi, F.
AU  - Sajjad, H.
TI  - Discovering Salient Neurons in deep NLP models
PY  - 2023
T2  - Journal of Machine Learning Research
VL  - 24
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187782103&partnerID=40&md5=f7b71c88b0f43f5b42d7384f6807f5c8
AB  - While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, work done towards analyzing individual neurons is relatively sparse. We present a technique called Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property, with the goal of understanding how such knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that learn a specific linguistic property? (ii) is a certain linguistic phenomenon in a given model localized (encoded in few individual neurons) or distributed across many neurons? (iii) how redundantly is the information preserved? (iv) how does fine-tuning pre-trained models towards downstream NLP tasks impact the learned linguistic knowledge? (v) how do models vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks; (ii) neurons capturing basic lexical information, such as suffixation, are localized in the lowermost layers; (iii) neurons learning complex concepts, such as syntactic role, are predominantly found in middle and higher layers; (iv) salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserves the higher layers for task-specific information; (v) we found interesting differences across pre-trained models regarding how linguistic information is preserved within them; and (vi) we found that concepts exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit (Dalvi et al., 2023).1. © 2023 Nadir Durrani, Fahim Dalvi and Hassan Sajjad.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, W.
AU  - Feng, H.
AU  - Zhou, W.
AU  - Liao, Z.
AU  - Li, H.
TI  - Model-Aware Pre-Training for Radial Distortion Rectification
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 5764
EP  - 5778
DO  - 10.1109/TIP.2023.3321459
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174827816&doi=10.1109%2fTIP.2023.3321459&partnerID=40&md5=b2b6662820edec6b762f97b91cd24e08
AB  - Camera lenses often suffer from optical aberrations, causing radial distortion in the captured images. In those images, there exists a clear and general physical distortion model. However, in existing solutions, such rich geometric prior is under-utilized, and the formulation of an effective prediction target is under-explored. To this end, we introduce Radial Distortion TRansformer (RDTR), a new framework for radial distortion rectification. Our RDTR includes a model-aware pre-training stage for distortion feature extraction and a deformation estimation stage for distortion rectification. Technically, on the one hand, we formulate the general radial distortion (i.e., barrel distortion and pincushion distortion) in camera-captured images with a shared geometric distortion model and perform a unified model-aware pre-training for its learning. With the pre-training, the network is capable of encoding the specific distortion pattern of a radially distorted image. After that, we transfer the learned representations to the learning of distortion rectification. On the other hand, we introduce a new prediction target called backward warping flow for rectifying images with any resolution while avoiding image defects. Extensive experiments are conducted on our synthetic dataset, and the results demonstrate that our method achieves state-of-the-art performance while operating in real-time. Besides, we also validate the generalization of RDTR on real-world images. Our source code and the proposed dataset are publicly available at https://github.com/wwd-ustc/RDTR.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, J.
AU  - Fan, J.
AU  - Wang, Y.
AU  - Yang, Y.
AU  - Zhang, Z.
TI  - Coarse Mask Guided Interactive Object Segmentation
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 5808
EP  - 5822
DO  - 10.1109/TIP.2023.3322564
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174838536&doi=10.1109%2fTIP.2023.3322564&partnerID=40&md5=8a7fd9d6cac7ef72dbe75914976324b9
AB  - Interactive object segmentation aims to produce object masks with user interactions, such as clicks, bounding boxes, and scribbles. Click point is the most popular interactive cue for its efficiency, and related deep learning methods have attracted lots of interest in recent years. Most works encode click points as gaussian maps and concatenate them with images as the model's input. However, the spatial and semantic information of gaussian maps would be noised through multiple convolution layers and won't be fully exploited by top layers for mask prediction. To pass click information to top layers exactly and efficiently, we propose a coarse mask guided model (CMG) which predicts coarse masks with a coarse module to guide the object mask prediction. Specifically, the coarse module encodes user clicks as query features and enriches their semantic information with backbone features through transformer layers, coarse masks are generated based on the enriched query feature and fed into CMG's decoder. Benefiting from the efficiency of transformer, CMG's coarse module and decoder module are lightweight and computationally efficient, making the interaction process more smooth. Experiments on several segmentation benchmarks demonstrate the effectiveness of our method, and we get new state-of-the-art results compared with previous works.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Chen, D.
AU  - Pan, X.
AU  - Tang, F.
AU  - Dong, W.
AU  - Xu, C.
TI  - SPA2Net: Structure-Preserved Attention Activated Network for Weakly Supervised Object Localization
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 5779
EP  - 5793
DO  - 10.1109/TIP.2023.3323793
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174803254&doi=10.1109%2fTIP.2023.3323793&partnerID=40&md5=384c3ee5e212617ac04f71e6eba83ca6
AB  - By exploring the localizable representations in deep CNN, weakly supervised object localization (WSOL) methods could determine the position of the object in each image just trained by the classification task. However, the partial activation problem caused by the discriminant function makes the network unable to locate objects accurately. To alleviate this problem, we propose Structure-Preserved Attention Activated Network (SPA2Net), a simple and effective one-stage WSOL framework to explore the ability of structure preservation of deep features. Different from traditional WSOL approaches, we decouple the object localization task from the classification branch to reduce their mutual influence by involving a localization branch which is online refined by a self-supervised structural-preserved localization mask. Specifically, we employ the high-order self-correlation as structural prior to enhance the perception of spatial interaction within convolutional features. By succinctly combining the structural prior with spatial attention, activations by SPA2Net will spread from part to the whole object during training. To avoid the structure-missing issue caused by the classification network, we furthermore utilize the restricted activation loss (RAL) to distinguish the difference between foreground and background in the channel dimension. In conjunction with the self-supervised localization branch, SPA2Net can directly predict the class-irrelevant localization map while prompting the network to pay more attention to the target region for accurate localization. Extensive experiments on two publicly available benchmarks, including CUB-200-2011 and ILSVRC, show that our SPA2Net achieves substantial and consistent performance gains compared with baseline approaches. The code and models are available at https://github.com/MsterDC/SPA2Net.  © 2023 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 1
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Fu, Y.
AU  - Gao, D.
AU  - Liu, T.
AU  - Zheng, H.
AU  - Hao, D.
AU  - Pan, Z.
TI  - Evolving into a Transformer: From a Training-Free Retrieval-Based Method for Anomaly Obstacle Segmentation
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 6195
EP  - 6209
DO  - 10.1109/TIP.2023.3312910
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174809244&doi=10.1109%2fTIP.2023.3312910&partnerID=40&md5=596815ae96f08246312cb63d81a5c2dd
AB  - As a key problem of auto-vehicle applications, the goal of Anomaly Obstacle Segmentation (AOS) is to detect some strange and unexpected obstacles (possibly are unseen previously) on the drivable area, thereby equipping the semantic perceptual model to be tolerant of unknown things. Due to its practicality, recently AOS is drawing attentions and a long line of works are proposed to tackle the obstacles with almost infinite diversity. However, these methods usually focus less on the priors of driving scenarios and involve image re-generation or the retraining of perceptual model, which lead to large computational quantity or the degradation of perceptual performance. In this paper, we propose to pay more attention to the characteristics of driving scenarios, lowering the difficulty of this tricky task. A training-free retrieval based method is thereby proposed to distinguish road obstacles from the surrounding road texture by computing the cosine similarity based on their appearance features, and significantly outperforms methods of the same category by around 20 percentage points. Besides, we find that there is a deep relation between our method and self-attention mechanism, and as a result a novel Transformer evolves from our retrieval based method, further boosting the performance.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhou, S.
AU  - Guo, D.
AU  - Li, J.
AU  - Yang, X.
AU  - Wang, M.
TI  - Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 5060
EP  - 5074
DO  - 10.1109/TIP.2023.3310332
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170717522&doi=10.1109%2fTIP.2023.3310332&partnerID=40&md5=cb173e56732bc1bb3134d1af5c685eff
AB  - Text-based visual question answering (TextVQA) faces the significant challenge of avoiding redundant relational inference. To be specific, a large number of detected objects and optical character recognition (OCR) tokens result in rich visual relationships. Existing works take all visual relationships into account for answer prediction. However, there are three observations: (1) a single subject in the images can be easily detected as multiple objects with distinct bounding boxes (considered repetitive objects). The associations between these repetitive objects are superfluous for answer reasoning; (2) two spatially distant OCR tokens detected in the image frequently have weak semantic dependencies for answer reasoning; and (3) the co-existence of nearby objects and tokens may be indicative of important visual cues for predicting answers. Rather than utilizing all of them for answer prediction, we make an effort to identify the most important connections or eliminate redundant ones. We propose a sparse spatial graph network (SSGN) that introduces a spatially aware relation pruning technique to this task. As spatial factors for relation measurement, we employ spatial distance, geometric dimension, overlap area, and DIoU for spatially aware pruning. We consider three visual relationships for graph learning: object-object, OCR-OCR tokens, and object-OCR token relationships. SSGN is a progressive graph learning architecture that verifies the pivotal relations in the correlated object-token sparse graph, and then in the respective object-based sparse graph and token-based sparse graph. Experiment results on TextVQA and ST-VQA datasets demonstrate that SSGN achieves promising performances. And some visualization results further demonstrate the interpretability of our method.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 13
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Shamsolmoali, P.
AU  - Zareapoor, M.
AU  - Zhou, H.
AU  - Tao, D.
AU  - Li, X.
TI  - VTAE: Variational Transformer Autoencoder With Manifolds Learning
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 4486
EP  - 4500
DO  - 10.1109/TIP.2023.3299495
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166758582&doi=10.1109%2fTIP.2023.3299495&partnerID=40&md5=0579f0c7e2a496512f9030e1de02915f
AB  - Deep generative models have demonstrated successful applications in learning non-linear data distributions through a number of latent variables and these models use a non-linear function (generator) to map latent samples into the data space. On the other hand, the non-linearity of the generator implies that the latent space shows an unsatisfactory projection of the data space, which results in poor representation learning. This weak projection, however, can be addressed by a Riemannian metric, and we show that geodesics computation and accurate interpolations between data samples on the Riemannian manifold can substantially improve the performance of deep generative models. In this paper, a Variational spatial-Transformer AutoEncoder (VTAE) is proposed to minimize geodesics on a Riemannian manifold and improve representation learning. In particular, we carefully design the variational autoencoder with an encoded spatial-Transformer to explicitly expand the latent variable model to data on a Riemannian manifold, and obtain global context modelling. Moreover, to have smooth and plausible interpolations while traversing between two different objects' latent representations, we propose a geodesic interpolation network different from the existing models that use linear interpolation with inferior performance. Experiments on benchmarks show that our proposed model can improve predictive accuracy and versatility over a range of computer vision tasks, including image interpolations, and reconstructions. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Jiao, S.
AU  - Goel, V.
AU  - Navasardyan, S.
AU  - Yang, Z.
AU  - Khachatryan, L.
AU  - Yang, Y.
AU  - Wei, Y.
AU  - Zhao, Y.
AU  - Shi, H.
TI  - Collaborative Content-Dependent Modeling: A Return to the Roots of Salient Object Detection
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 4237
EP  - 4246
DO  - 10.1109/TIP.2023.3293759
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164772846&doi=10.1109%2fTIP.2023.3293759&partnerID=40&md5=ff28324d638271961e1e7f5ecf243d2b
AB  - Salient object detection (SOD) aims to identify the most visually distinctive object(s) from each given image. Most recent progresses focus on either adding elaborative connections among different convolution blocks or introducing boundary-aware supervision to help achieve better segmentation, which is actually moving away from the essence of SOD, i.e., distinctiveness/salience. This paper goes back to the roots of SOD and investigates the principles of how to identify distinctive object(s) in a more effective and efficient way. Intuitively, the salience of one object should largely depend on its global context within the input image. Based on this, we devise a clean yet effective architecture for SOD, named Collaborative Content-Dependent Networks (CCD-Net). In detail, we propose a collaborative content-dependent head whose parameters are conditioned on the input image's global context information. Within the content-dependent head, a hand-crafted multi-scale (HMS) module and a self-induced (SI) module are carefully designed to collaboratively generate content-aware convolution kernels for prediction. Benefited from the content-dependent head, CCD-Net is capable of leveraging global context to detect distinctive object(s) while keeping a simple encoder-decoder design. Extensive experimental results demonstrate that our CCD-Net achieves state-of-the-art results on various benchmarks. Our architecture is simple and intuitive compared to previous solutions, resulting in competitive characteristics with respect to model complexity, operating efficiency, and segmentation accuracy. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, G.
AU  - Bai, Z.
AU  - Liu, Z.
AU  - Zhang, X.
AU  - Ling, H.
TI  - Salient Object Detection in Optical Remote Sensing Images Driven by Transformer
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 5257
EP  - 5269
DO  - 10.1109/TIP.2023.3314285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172034124&doi=10.1109%2fTIP.2023.3314285&partnerID=40&md5=ef39d5bc2a239a4669f8e54824caca22
AB  - Existing methods for Salient Object Detection in Optical Remote Sensing Images (ORSI-SOD) mainly adopt Convolutional Neural Networks (CNNs) as the backbone, such as VGG and ResNet. Since CNNs can only extract features within certain receptive fields, most ORSI-SOD methods generally follow the local-to-contextual paradigm. In this paper, we propose a novel Global Extraction Local Exploration Network (GeleNet) for ORSI-SOD following the global-to-local paradigm. Specifically, GeleNet first adopts a transformer backbone to generate four-level feature embeddings with global long-range dependencies. Then, GeleNet employs a Direction-aware Shuffle Weighted Spatial Attention Module (D-SWSAM) and its simplified version (SWSAM) to enhance local interactions, and a Knowledge Transfer Module (KTM) to further enhance cross-level contextual interactions. D-SWSAM comprehensively perceives the orientation information in the lowest-level features through directional convolutions to adapt to various orientations of salient objects in ORSIs, and effectively enhances the details of salient objects with an improved attention mechanism. SWSAM discards the direction-aware part of D-SWSAM to focus on localizing salient objects in the highest-level features. KTM models the contextual correlation knowledge of two middle-level features of different scales based on the self-attention mechanism, and transfers the knowledge to the raw features to generate more discriminative features. Finally, a saliency predictor is used to generate the saliency map based on the outputs of the above three modules. Extensive experiments on three public datasets demonstrate that the proposed GeleNet outperforms relevant state-of-the-art methods. The code and results of our method are available at https://github.com/MathLee/GeleNet.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 23
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Quan, Y.
AU  - Zhang, D.
AU  - Zhang, L.
AU  - Tang, J.
TI  - Centralized Feature Pyramid for Object Detection
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 4341
EP  - 4354
DO  - 10.1109/TIP.2023.3297408
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165913759&doi=10.1109%2fTIP.2023.3297408&partnerID=40&md5=b545ebd33cd799c92a001865bd8a22e2
AB  - The visual feature pyramid has shown its superiority in both effectiveness and efficiency in a variety of applications. However, current methods overly focus on inter-layer feature interactions while disregarding the importance of intra-layer feature regulation. Despite some attempts to learn a compact intra-layer feature representation with the use of attention mechanisms or vision transformers, they overlook the crucial corner regions that are essential for dense prediction tasks. To address this problem, we propose a Centralized Feature Pyramid (CFP) network for object detection, which is based on a globally explicit centralized feature regulation. Specifically, we first propose a spatial explicit visual center scheme, where a lightweight MLP is used to capture the globally long-range dependencies, and a parallel learnable visual center mechanism is used to capture the local corner regions of the input images. Based on this, we then propose a globally centralized regulation for the commonly-used feature pyramid in a top-down fashion, where the explicit visual center information obtained from the deepest intra-layer feature is used to regulate frontal shallow features. Compared to the existing feature pyramids, CFP not only has the ability to capture the global long-range dependencies but also efficiently obtain an all-round yet discriminative feature representation. Experimental results on the challenging MS-COCO validate that our proposed CFP can achieve consistent performance gains on the state-of-the-art YOLOv5 and YOLOX object detection baselines. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 112
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Lu, Z.
AU  - He, S.
AU  - Li, D.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Prediction Calibration for Generalized Few-Shot Semantic Segmentation
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 3311
EP  - 3323
DO  - 10.1109/TIP.2023.3282070
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161473126&doi=10.1109%2fTIP.2023.3282070&partnerID=40&md5=2f463685b04a61058681ed6442b2c67b
AB  - Generalized Few-shot Semantic Segmentation (GFSS) aims to segment each image pixel into either base classes with abundant training examples or novel classes with only a handful of (e. g., 1-5) training images per class. Compared to the widely studied Few-shot Semantic Segmentation (FSS), which is limited to segmenting novel classes only, GFSS is much under-studied despite being more practical. Existing approach to GFSS is based on classifier parameter fusion whereby a newly trained novel class classifier and a pre-trained base class classifier are combined to form a new classifier. As the training data is dominated by base classes, this approach is inevitably biased towards the base classes. In this work, we propose a novel Prediction Calibration Network (PCN) to address this problem. Instead of fusing the classifier parameters, we fuse the scores produced separately by the base and novel classifiers. To ensure that the fused scores are not biased to either the base or novel classes, a new Transformer-based calibration module is introduced. It is known that the lower-level features are useful of detecting edge information in an input image than higher-level features. Thus, we build a cross-attention module that guides the classifier's final prediction using the fused multi-level features. However, transformers are computationally demanding. Crucially, to make the proposed cross-attention module training tractable at the pixel level, this module is designed based on feature-score cross-covariance and episodically trained to be generalizable at inference time. Extensive experiments on PASCAL- 5^i and COCO- 20^i show that our PCN outperforms the state-the-the-art alternatives by large margins.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, H.
AU  - Huang, J.
AU  - Jin, P.
AU  - Song, G.
AU  - Wu, Q.
AU  - Chen, J.
TI  - Weakly-Supervised 3D Spatial Reasoning for Text-Based Visual Question Answering
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 3367
EP  - 3382
DO  - 10.1109/TIP.2023.3276570
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161046081&doi=10.1109%2fTIP.2023.3276570&partnerID=40&md5=8e8f1e9e777d63adf63fa52381d01dec
AB  - Text-based Visual Question Answering (TextVQA) aims to produce correct answers for given questions about the images with multiple scene texts. In most cases, the texts naturally attach to the surface of the objects. Therefore, spatial reasoning between texts and objects is crucial in TextVQA. However, existing approaches are constrained within 2D spatial information learned from the input images and rely on transformer-based architectures to reason implicitly during the fusion process. Under this setting, these 2D spatial reasoning approaches cannot distinguish the fine-grained spatial relations between visual objects and scene texts on the same image plane, thereby impairing the interpretability and performance of TextVQA models. In this paper, we introduce 3D geometric information into the spatial reasoning process to capture the contextual knowledge of key objects step-by-step. Specifically, (i) we propose a relation prediction module for accurately locating the region of interest of critical objects; (ii) we design a depth-aware attention calibration module for calibrating the OCR tokens' attention according to critical objects. Extensive experiments show that our method achieves state-of-the-art performance on TextVQA and ST-VQA datasets. More encouragingly, our model surpasses others by clear margins of 5.7% and 12.1% on questions that involve spatial reasoning in TextVQA and ST-VQA valid split. Besides, we also verify the generalizability of our model on the text-based image captioning task. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 13
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zeng, Z.
AU  - Dai, P.
AU  - Zhang, X.
AU  - Zhang, L.
AU  - Cao, X.
TI  - Cognition Guided Human-Object Relationship Detection
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 2468
EP  - 2480
DO  - 10.1109/TIP.2023.3270040
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159554676&doi=10.1109%2fTIP.2023.3270040&partnerID=40&md5=28b3f6326b05a8bca203296239cbed5d
AB  - Human-object relationship detection reveals the fine-grained relationship between humans and objects, helping the comprehensive understanding of videos. Previous human-object relationship detection approaches are mainly developed with object features and relation features without exploring the specific information of humans. In this paper, we propose a novel Relation-Pose Transformer (RPT) for human-object relationship detection. Inspired by the coordination of eye-head-body movements in cognitive science, we employ the head pose to find those crucial objects that humans focus on and use the body pose with skeleton information to represent multiple actions. Then, we utilize the spatial encoder to capture spatial contextualized information of the relation pair, which integrates the relation features and pose features. Next, the temporal decoder aims to model the temporal dependency of the relationship. Finally, we adopt multiple classifiers to predict different types of relationships. Extensive experiments on the benchmark Action Genome validate the effectiveness of our proposed method and show the state-of-the-art performance compared with related methods.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, D.
AU  - Zhang, J.
AU  - Du, B.
AU  - Zhang, L.
AU  - Tao, D.
TI  - DCN-T: Dual Context Network With Transformer for Hyperspectral Image Classification
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 2536
EP  - 2551
DO  - 10.1109/TIP.2023.3270104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159666931&doi=10.1109%2fTIP.2023.3270104&partnerID=40&md5=96fe1c69f66f759e8fe3f6e3be4bd7db
AB  - Hyperspectral image (HSI) classification is challenging due to spatial variability caused by complex imaging conditions. Prior methods suffer from limited representation ability, as they train specially designed networks from scratch on limited annotated data. We propose a tri-spectral image generation pipeline that transforms HSI into high-quality tri-spectral images, enabling the use of off-the-shelf ImageNet pretrained backbone networks for feature extraction. Motivated by the observation that there are many homogeneous areas with distinguished semantic and geometric properties in HSIs, which can be used to extract useful contexts, we propose an end-to-end segmentation network named DCN-T. It adopts transformers to effectively encode regional adaptation and global aggregation spatial contexts within and between the homogeneous areas discovered by similarity-based clustering. To fully exploit the rich spectrums of the HSI, we adopt an ensemble approach where all segmentation results of the tri-spectral images are integrated into the final prediction through a voting scheme. Extensive experiments on three public benchmarks show that our proposed method outperforms state-of-the-art methods for HSI classification. The code will be released at https://github.com/DotWang/DCN-T.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 41
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Lin, C.-S.
AU  - Wang, Y.-C.F.
TI  - Describe, Spot and Explain: Interpretable Representation Learning for Discriminative Visual Reasoning
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 2481
EP  - 2492
DO  - 10.1109/TIP.2023.3268001
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153801831&doi=10.1109%2fTIP.2023.3268001&partnerID=40&md5=47cbc9da95ae750297ef47aac8b56496
AB  - Despite the recent success achieved by deep neural networks (DNNs), it remains challenging to disclose/explain the decision-making process from the numerous parameters and complex non-linear functions. To address the problem, explainable AI (XAI) aims to provide explanations corresponding to the learning and prediction processes for deep learning models. In this paper, we propose a novel representation learning framework of Describe, Spot and eXplain (DSX). Based on the architecture of Transformer, our proposed DSX framework is composed of two learning stages, descriptive prototype learning and discriminative prototype discovery. Given an input image, the former stage is designed to derive a set of descriptive representations, while the latter stage further identifies a discriminative subset, offering semantic interpretability for the corresponding classification tasks. While our DSX does not require any ground truth attribute supervision during training, the derived visual representations can be practically associated with physical attributes provided by domain experts. Extensive experiments on fine-grained classification and person re-identification tasks qualitatively and quantitatively verify the use our DSX model for offering semantically practical interpretability with satisfactory recognition performances.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wan, J.
AU  - Liu, J.
AU  - Zhou, J.
AU  - Lai, Z.
AU  - Shen, L.
AU  - Sun, H.
AU  - Xiong, P.
AU  - Min, W.
TI  - Precise Facial Landmark Detection by Reference Heatmap Transformer
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 1966
EP  - 1977
DO  - 10.1109/TIP.2023.3261749
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151564869&doi=10.1109%2fTIP.2023.3261749&partnerID=40&md5=7e9efcc750a5ec1f29128123c42205ab
AB  - Most facial landmark detection methods predict landmarks by mapping the input facial appearance features to landmark heatmaps and have achieved promising results. However, when the face image is suffering from large poses, heavy occlusions and complicated illuminations, they cannot learn discriminative feature representations and effective facial shape constraints, nor can they accurately predict the value of each element in the landmark heatmap, limiting their detection accuracy. To address this problem, we propose a novel Reference Heatmap Transformer (RHT) by introducing reference heatmap information for more precise facial landmark detection. The proposed RHT consists of a Soft Transformation Module (STM) and a Hard Transformation Module (HTM), which can cooperate with each other to encourage the accurate transformation of the reference heatmap information and facial shape constraints. Then, a Multi-Scale Feature Fusion Module (MSFFM) is proposed to fuse the transformed heatmap features and the semantic features learned from the original face images to enhance feature representations for producing more accurate target heatmaps. To the best of our knowledge, this is the first study to explore how to enhance facial landmark detection by transforming the reference heatmap information. The experimental results from challenging benchmark datasets demonstrate that our proposed method outperforms the state-of-the-art methods in the literature.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 40
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Shi, H.
AU  - Zhang, X.-Y.
AU  - Li, C.
TI  - StochasticFormer: Stochastic Modeling for Weakly Supervised Temporal Action Localization
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 1379
EP  - 1389
DO  - 10.1109/TIP.2023.3244411
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149282069&doi=10.1109%2fTIP.2023.3244411&partnerID=40&md5=acba4bb525e56bc67153089413933e28
AB  - Weakly supervised temporal action localization (WS-TAL) aims to identify the time intervals corresponding to actions of interest in untrimmed videos with video-level weak supervision. For most existing WS-TAL methods, two commonly encountered challenges are under-localization and over-localization, which inevitably bring about severe performance deterioration. To address the issues, this paper proposes a transformer-structured stochastic process modeling framework, namely StochasticFormer, to fully investigate finer-grained interactions among the intermediate predictions to achieve further refined localization. StochasticFormer is built on a standard attention-based pipeline to derive preliminary frame/snippet-level predictions. Then, the pseudo localization module generates variable-length pseudo action instances with the corresponding pseudo labels. Using the pseudo 'action instance - action category' pairs as fine-grained pseudo supervision, the stochastic modeler aims to learn the underlying interaction among the intermediate predictions with an encoder-decoder network. The encoder consists of the deterministic and latent path to capture the local and global information, which are subsequently integrated by the decoder to obtain reliable predictions. The framework is optimized with three carefully designed losses, i.e. the video-level classification loss, the frame-level semantic coherence loss, and the ELBO loss. Extensive experiments on two benchmarks, i.e., THUMOS14 and ActivityNet1.2, have shown the efficacy of StochasticFormer compared with the state-of-the-art methods.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Lim, J.
AU  - Baskaran, V.M.
AU  - Lim, J.M.-Y.
AU  - Wong, K.
AU  - See, J.
AU  - Tistarelli, M.
TI  - ERNet: An Efficient and Reliable Human-Object Interaction Detection Network
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 964
EP  - 979
DO  - 10.1109/TIP.2022.3231528
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148230707&doi=10.1109%2fTIP.2022.3231528&partnerID=40&md5=23931b6e3f9607245ee36b17e2298653
AB  - Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at https://github.com/Monash-CyPhi-AI-Research-Lab/ernet.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 18
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yu, W.
AU  - Zhu, M.
AU  - Wang, N.
AU  - Wang, X.
AU  - Gao, X.
TI  - An Efficient Transformer Based on Global and Local Self-Attention for Face Photo-Sketch Synthesis
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 483
EP  - 495
DO  - 10.1109/TIP.2022.3229614
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146228354&doi=10.1109%2fTIP.2022.3229614&partnerID=40&md5=9292c32d1b0eaabdf299233ef8cd03f7
AB  - Face photo-sketch synthesis tasks have been dominated by convolutional neural networks (CNNs), especially CNN-based generative adversarial networks (GANs), because of their strong texture modeling capabilities and thus their ability to generate more realistic face photos/sketches beyond traditional methods. However, due to CNNs' locality and spatial invariance properties, there have weaknesses in capturing the global and structural information which are extremely important for face images. Inspired by the recent phenomenal success of the Transformer in vision tasks, we propose replacing CNNs with Transformers that are able to model long-range dependencies to synthesize more structured and realistic face images. However, the existing vision Transformers are mainly designed for high-level vision tasks and lack the dense prediction ability to generate high resolution images due to the quadratic computational complexity of their self-attention mechanism. In addition, the original Transformer is not capable of modeling local correlations which is an important skill for image generation. To address these challenges, we propose two types of memory-friendly Transformer encoders, one for processing local correlations via local self-attention and another for modeling global information via global self-attention. By integrating the two proposed Transformer encoders, we present an efficient GL-Transformer for face photo-sketch synthesis, which can synthesize realistic face photo/sketch images from coarse to fine. Extensive experiments demonstrate that our model achieves a comparable or better performance beyond the state-of-the-art CNN-based methods both qualitatively and quantitatively.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 18
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, H.
AU  - Du, Y.
AU  - Zhang, Y.
AU  - Li, S.
AU  - Zhang, L.
TI  - One-Stage Visual Relationship Referring With Transformers and Adaptive Message Passing
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 190
EP  - 202
DO  - 10.1109/TIP.2022.3226624
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144789171&doi=10.1109%2fTIP.2022.3226624&partnerID=40&md5=a0f6c4e5e98d834a99a24b1bb2150179
AB  - There exist a variety of visual relationships among entities in an image. Given a relationship query langle subject, predicate, object rangle , the task of visual relationship referring (VRR) aims to disambiguate instances of the same entity category and simultaneously localize the subject and object entities in an image. Previous works of VRR can be generally categorized into one-stage and multi-stage methods. The former ones directly localize a pair of entities from the image but they suffer from low prediction accuracy, while the latter ones perform better but they are indirect to localize only a couple of entities by pre-generating a rich amount of candidate proposals. In this paper, we formulate the task of VRR as an end-to-end bounding box regression problem and propose a novel one-stage approach, called VRR-TAMP, by effectively integrating Transformers and an adaptive message passing mechanism. First, visual relationship queries and images are respectively encoded to generate the basic modality-specific embeddings, which are then fed into a cross-modal Transformer encoder to produce the joint representation. Second, to obtain the specific representation of each entity, we introduce an adaptive message passing mechanism and design an entity-specific information distiller SR-GMP, which refers to a gated message passing (GMP) module that works on the joint representation learned from a single learnable token. The GMP module adaptively distills the final representation of an entity by incorporating the contextual cues regarding the predicate and the other entity. Experiments on VRD and Visual Genome datasets demonstrate that our approach significantly outperforms its one-stage competitors and achieves competitive results with the state-of-the-art multi-stage methods. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 0
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, L.
AU  - Liu, H.
AU  - Zhou, S.
AU  - Tang, W.
AU  - Hua, G.
TI  - Instance Motion Tendency Learning for Video Panoptic Segmentation
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 764
EP  - 778
DO  - 10.1109/TIP.2022.3226414
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144811270&doi=10.1109%2fTIP.2022.3226414&partnerID=40&md5=6fc2e498df71c96ad3dffb799ea2b9e6
AB  - Video panoptic segmentation is an important but challenging task in computer vision. It not only performs panoptic segmentation of each frame, but also associates the same instance across adjacent frames. Due to the lack of temporal coherence modeling, most existing approaches often generate identity switches during instance association, and they cannot handle ambiguous segmentation boundaries caused by motion blur. To address these difficult issues, we introduce a simple yet effective Instance Motion Tendency Network (IMTNet) for video panoptic segmentation. It learns a global motion tendency map for instance association, and a hierarchical classifier for motion boundary refinement. Specifically, a Global Motion Tendency Module (GMTM) is designed to learn robust motion features from optical flows, which can directly associate each instance in the previous frame to the corresponding instance in the current frame. In addition, we propose a Motion Boundary Refinement Module (MBRM) to learn a hierarchical classifier to handle the boundary pixels of moving targets, which can effectively revise the inaccurate segmentation predictions. Experimental results on both Cityscapes and Cityscapes-VPS datasets show that our IMTNet outperforms most state-of-the-art approaches.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Dou, S.
AU  - Zhao, C.
AU  - Jiang, X.
AU  - Zhang, S.
AU  - Zheng, W.-S.
AU  - Zuo, W.
TI  - Human Co-Parsing Guided Alignment for Occluded Person Re-Identification
PY  - 2023
T2  - IEEE Transactions on Image Processing
VL  - 32
SP  - 458
EP  - 470
DO  - 10.1109/TIP.2022.3229639
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146248032&doi=10.1109%2fTIP.2022.3229639&partnerID=40&md5=e5ddabaef5c82b97c903a54236840e4a
AB  - Occluded person re-identification (ReID) is a challenging task due to more background noises and incomplete foreground information. Although existing human parsing-based ReID methods can tackle this problem with semantic alignment at the finest pixel level, their performance is heavily affected by the human parsing model. Most supervised methods propose to train an extra human parsing model aside from the ReID model with cross-domain human parts annotation, suffering from expensive annotation cost and domain gap; Unsupervised methods integrate a feature clustering-based human parsing process into the ReID model, but lacking supervision signals brings less satisfactory segmentation results. In this paper, we argue that the pre-existing information in the ReID training dataset can be directly used as supervision signals to train the human parsing model without any extra annotation. By integrating a weakly supervised human co-parsing network into the ReID network, we propose a novel framework that exploits shared information across different images of the same pedestrian, called the Human Co-parsing Guided Alignment (HCGA) framework. Specifically, the human co-parsing network is weakly supervised by three consistency criteria, namely global semantics, local space, and background. By feeding the semantic information and deep features from the person ReID network into the guided alignment module, features of the foreground and human parts can then be obtained for effective occluded person ReID. Experiment results on two occluded and two holistic datasets demonstrate the superiority of our method. Especially on Occluded-DukeMTMC, it achieves 70.2% Rank-1 accuracy and 57.5% mAP.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 11
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Cheng, L.
AU  - Jia, W.
AU  - Yang, W.
TI  - Capture Salient Historical Information: A Fast and Accurate Non-autoregressive Model for Multi-turn Spoken Language Understanding
PY  - 2022
T2  - ACM Transactions on Information Systems
VL  - 41
IS  - 2
C7  - 41
DO  - 10.1145/3545800
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143432685&doi=10.1145%2f3545800&partnerID=40&md5=bf6c108820a37bfee3d2f1f662c20642
AB  - Spoken Language Understanding (SLU), a core component of the task-oriented dialogue system, expects a shorter inference facing the impatience of human users. Existing work increases inference speed by designing non-autoregressive models for single-turn SLU tasks but fails to apply to multi-turn SLU in confronting the dialogue history. The intuitive idea is to concatenate all historical utterances and utilize the non-autoregressive models directly. However, this approach seriously misses the salient historical information and suffers from the uncoordinated-slot problems. To overcome those shortcomings, we propose a novel model for multi-turn SLU named Salient History Attention with Layer-Refined Transformer (SHA-LRT), which comprises a SHA module, a Layer-Refined Mechanism (LRM), and a Slot Label Generation (SLG) task. SHA captures salient historical information for the current dialogue from both historical utterances and results via a well-designed history-attention mechanism. LRM predicts preliminary SLU results from Transformer's middle states and utilizes them to guide the final prediction, and SLG obtains the sequential dependency information for the non-autoregressive encoder. Experiments on public datasets indicate that our model significantly improves multi-turn SLU performance (17.5% on Overall) with accelerating (nearly 15 times) the inference process over the state-of-the-art baseline as well as effective on the single-turn SLU tasks.  © 2022 Association for Computing Machinery.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 4
C2  - CCF:A期刊; FMS:B; 
LB  - Cheng2022Capture
ER  -

TY  - JOUR
AU  - Sevastjanova, R.
AU  - Cakmak, E.
AU  - Ravfogel, S.
AU  - Cotterell, R.
AU  - El-Assady, M.
TI  - Visual Comparison of Language Model Adaptation
PY  - 2023
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 29
IS  - 1
SP  - 1178
EP  - 1188
DO  - 10.1109/TVCG.2022.3209458
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139525986&doi=10.1109%2fTVCG.2022.3209458&partnerID=40&md5=abb3c0ac45b4cd458748982eb934d80e
AB  - Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.  © 2022 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 11
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Liu, T.
AU  - Zhang, C.
AU  - Lam, K.-M.
AU  - Kong, J.
TI  - Decouple and Resolve: Transformer-Based Models for Online Anomaly Detection From Weakly Labeled Videos
PY  - 2023
T2  - IEEE Transactions on Information Forensics and Security
VL  - 18
SP  - 15
EP  - 28
DO  - 10.1109/TIFS.2022.3216479
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140722808&doi=10.1109%2fTIFS.2022.3216479&partnerID=40&md5=a752bcb94dda28f4b65cedc3316a0903
AB  - As one of the vital topics in intelligent surveillance, weakly supervised online video anomaly detection (WS-OVAD) aims to identify the ongoing anomalous events moment-to-moment in streaming videos, trained with only video-level annotations. Previous studies tended to utilize a unified single-stage framework, which struggled to simultaneously address the issues of online constraints and weakly supervised settings. To solve this dilemma, in this paper, we propose a two-stage-based framework, namely 'decouple and resolve' (DAR), which consists of two modules, i.e., temporal proposal producer (TPP) and online anomaly localizer (OAL). With the supervision of video-level binary labels, the TPP module targets fully exploiting hierarchical temporal relations among snippets for generating precise snippet-level pseudo-labels. Then, given fine-grained supervisory signals produced by TPP, the Transformer-based OAL module is trained to aggregate both the useful cues retrieved from historical observations and anticipated future semantics, for making predictions at the current time step. Both the TPP and OAL modules are jointly trained to share the beneficial knowledge in a multi-task learning paradigm. Extensive experimental results on three public data sets validate the superior performance of the proposed DAR framework over the competing methods.  © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 20
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ciniselli, M.
AU  - Cooper, N.
AU  - Pascarella, L.
AU  - Mastropaolo, A.
AU  - Aghajani, E.
AU  - Poshyvanyk, D.
AU  - Di Penta, M.
AU  - Bavota, G.
TI  - An Empirical Study on the Usage of Transformer Models for Code Completion
PY  - 2022
T2  - IEEE Transactions on Software Engineering
VL  - 48
IS  - 12
SP  - 4818
EP  - 4837
DO  - 10.1109/TSE.2021.3128234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145349356&doi=10.1109%2fTSE.2021.3128234&partnerID=40&md5=07328f166363007a0e8b14a1e3bcec1a
AB  - Code completion aims at speeding up code writing by predicting the next code token(s) the developer is likely to write. Works in this field focused on improving the accuracy of the generated predictions, with substantial leaps forward made possible by deep learning (DL) models. However, code completion techniques are mostly evaluated in the scenario of predicting the next token to type, with few exceptions pushing the boundaries to the prediction of an entire code statement. Thus, little is known about the performance of state-of-the-art code completion approaches in more challenging scenarios in which, for example, an entire code block must be generated. We present a large-scale study exploring the capabilities of state-of-the-art Transformer-based models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). We experimented with several variants of two recently proposed Transformer-based models, namely RoBERTa and the Text-To-Text Transfer Transformer (T5), for the task of code completion. The achieved results show that Transformer-based models, and in particular the T5, represent a viable solution for code completion, with perfect predictions ranging from ∼29%, obtained when asking the model to guess entire blocks, up to ∼69%, reached in the simpler scenario of few tokens masked from the same code statement.  © 1976-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 41
C2  - CCF:A期刊; FMS:B; 
LB  - Ciniselli2022Empirical
ER  -

TY  - JOUR
AU  - Li, T.
AU  - Jiang, Y.
AU  - Lin, C.
AU  - Obaidat, M.S.
AU  - Shen, Y.
AU  - Ma, J.
TI  - DeepAG: Attack Graph Construction and Threats Prediction With Bi-Directional Deep Learning
PY  - 2023
T2  - IEEE Transactions on Dependable and Secure Computing
VL  - 20
IS  - 1
SP  - 740
EP  - 757
DO  - 10.1109/TDSC.2022.3143551
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123382299&doi=10.1109%2fTDSC.2022.3143551&partnerID=40&md5=78db39f978d6a7366f3c8e511f1eb2f9
AB  - The complicated multi-step attacks, such as Advanced Persistent Threats (APTs), have brought considerable threats to cybersecurity because they are naturally varied and complex. Therefore, studying the strategies of adversaries and making predictions are still significant challenges for attack prevention. To address these problems, we propose DeepAG, a framework utilizing system logs to detect threats and predict the attack paths. DeepAG leverages transformer models to novelly detect APT attack sequences by modeling semantic information of system logs. On the other hand, DeepAG utilizes Long Short-Term Memory (LSTM) network to propose bi-directional prediction for attack paths, which achieves higher performance than traditional BiLSTM. In addition, with previously detected attack sequences and predicted paths, DeepAG constructs the attack graphs that attackers may follow to compromise the network. Furthermore, DeepAG offers the mechanisms of Out-Of-Vocabulary (OOV) word processor and online update respectively to adapt new attack patterns that show up during detection and prediction stages. The experiments on open-source data sets show that more than 99% of over 15000 sequences can be detected accurately by DeepAG. Moreover, DeepAG can improve the baseline by 11.166% of accuracy in terms of prediction. © 2004-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 30
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Gurevych, I.
AU  - Kohler, M.
AU  - Sahin, G.G.
TI  - On the Rate of Convergence of a Classifier Based on a Transformer Encoder
PY  - 2022
T2  - IEEE Transactions on Information Theory
VL  - 68
IS  - 12
SP  - 8139
EP  - 8155
DO  - 10.1109/TIT.2022.3191747
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135219411&doi=10.1109%2fTIT.2022.3191747&partnerID=40&md5=1d248b503ca1f55c088d958694afdbb3
AB  - Pattern recognition based on a high-dimensional predictor is considered. A classifier is defined which is based on a Transformer encoder. The rate of convergence of the misclassification probability of the classifier towards the optimal misclassification probability is analyzed. It is shown that this classifier is able to circumvent the curse of dimensionality provided the a posteriori probability satisfies a suitable hierarchical composition model. Furthermore, the difference between the Transformer classifiers theoretically analyzed in this paper and the ones used in practice today is illustrated by means of classification problems in natural language processing.  © 1963-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 3
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Tian, Y.
AU  - Yan, Y.
AU  - Zhai, G.
AU  - Guo, G.
AU  - Gao, Z.
TI  - EAN: Event Adaptive Network for Enhanced Action Recognition
PY  - 2022
T2  - International Journal of Computer Vision
VL  - 130
IS  - 10
SP  - 2453
EP  - 2471
DO  - 10.1007/s11263-022-01661-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135590036&doi=10.1007%2fs11263-022-01661-1&partnerID=40&md5=084d48e2994bc438fd524f56d7e18806
AB  - Efficiently modeling spatial–temporal information in videos is crucial for action recognition. To achieve this goal, state-of-the-art methods typically employ the convolution operator and the dense interaction modules such as non-local blocks. However, these methods cannot accurately fit the diverse events in videos. On the one hand, the adopted convolutions are with fixed scales, thus struggling with events of various scales. On the other hand, the dense interaction modeling paradigm only achieves sub-optimal performance as action-irrelevant parts bring additional noises for the final prediction. In this paper, we propose a unified action recognition framework to investigate the dynamic nature of video content by introducing the following designs. First, when extracting local cues, we generate the spatial–temporal kernels of dynamic-scale to adaptively fit the diverse events. Second, to accurately aggregate these cues into a global video representation, we propose to mine the interactions only among a few selected foreground objects by a Transformer, which yields a sparse paradigm. We call the proposed framework as Event Adaptive Network because both key designs are adaptive to the input video content. To exploit the short-term motions within local segments, we propose a novel and efficient Latent Motion Code module, further improving the performance of the framework. Extensive experiments on several large-scale video datasets, e.g., Something-to-Something V1 &V2, Kinetics, and Diving48, verify that our models achieve state-of-the-art or competitive performances at low FLOPs. Codes are available at: https://github.com/tianyuan168326/EAN-Pytorch. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 32
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Guo, H.
AU  - Liu, S.
AU  - Pan, H.
AU  - Liu, Y.
AU  - Tong, X.
AU  - Guo, B.
TI  - ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation
PY  - 2022
T2  - ACM Transactions on Graphics
VL  - 41
IS  - 4
C7  - 3530078
DO  - 10.1145/3528223.3530078
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135139708&doi=10.1145%2f3528223.3530078&partnerID=40&md5=71af7d15c78c1d1f0bb8b703d40332cb
AB  - We view the reconstruction of CAD models in the boundary representation (B-Rep) as the detection of geometric primitives of different orders, i.e., vertices, edges and surface patches, and the correspondence of primitives, which are holistically modeled as a chain complex, and show that by modeling such comprehensive structures more complete and regularized reconstructions can be achieved. We solve the complex generation problem in two steps. First, we propose a novel neural framework that consists of a sparse CNN encoder for input point cloud processing and a tri-path transformer decoder for generating geometric primitives and their mutual relationships with estimated probabilities. Second, given the probabilistic structure predicted by the neural network, we recover a definite B-Rep chain complex by solving a global optimization maximizing the likelihood under structural validness constraints and applying geometric refinements. Extensive tests on large scale CAD datasets demonstrate that the modeling of B-Rep chain complex structure enables more accurate detection for learning and more constrained reconstruction for optimization, leading to structurally more faithful and complete CAD B-Rep models than previous results. © 2022 ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 38
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, X.
AU  - Xiong, H.
AU  - Li, X.
AU  - Wu, X.
AU  - Chen, Z.
AU  - Dou, D.
TI  - InterpretDL: Explaining Deep Models in PaddlePaddle
PY  - 2022
T2  - Journal of Machine Learning Research
VL  - 23
C7  - 197
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144854706&partnerID=40&md5=ab256db5e659e201bdb94bcd847edd6c
AB  - Techniques to explain the predictions of deep neural networks (DNNs) have been largely required for gaining insights into the black boxes. We introduce InterpretDL, a toolkit of explanation algorithms based on PaddlePaddle, with uniformed programming interfaces and “plug-and-play” designs. A few lines of codes are needed to obtain the explanation results without modifying the structure of the model. InterpretDL currently contains 16 algorithms, explaining training phases, datasets, global and local behaviors of post-trained deep models. InterpretDL also provides a number of tutorial examples and showcases to demonstrate the capability of InterpretDL working on a wide range of deep learning models, e.g., Convolutional Neural Networks (CNNs), Multi-Layer Preceptors (MLPs), Transformers, etc., for various tasks in both Computer Vision (CV) and Natural Language Processing (NLP). Furthermore, InterpretDL modularizes the implementations, making efforts to support the compatibility across frameworks. The project is available at https://github.com/PaddlePaddle/InterpretDL. ©2022 Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Zeyu Chen, and Dejing Dou.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Jiang, Y.
AU  - Yang, S.
AU  - Qju, H.
AU  - Wu, W.
AU  - Loy, C.C.
AU  - Liu, Z.
TI  - Text2Human: Text-Driven Controllable Human Image Generation
PY  - 2022
T2  - ACM Transactions on Graphics
VL  - 41
IS  - 4
C7  - 3530104
DO  - 10.1145/3528223.3530104
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135161067&doi=10.1145%2f3528223.3530104&partnerID=40&md5=95ed064cf027fa781f8fc5dcf5b4342f
AB  - Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural representations for each type of texture. The codebook at the coarse level includes the structural representations of textures, while the codebook at the fine level focuses on the details of textures. To make use of the learned hierarchical codebook to synthesize desired images, a diffusion-based transformer sampler with mixture of experts is firstly employed to sample indices from the coarsest level of the codebook, which then is used to predict the indices of the codebook at finer levels. The predicted indices at different levels are translated to human images by the decoder learned accompanied with hierarchical codebooks. The use of mixture-of-experts allows for the generated image conditioned on the fine-grained text input. The prediction for finer level indices refines the quality of clothing textures. Extensive quantitative and qualitative evaluations demonstrate that our proposed Text2Human framework can generate more diverse and realistic human images compared to state-of-the-art methods. Our project page is https://yumingj.github.io/projects/Text2Human.html. Code and pretrained models are available at https://github.com/yumingj/Text2Human. © 2022 ACM.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 70
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, M.
AU  - Wu, S.
AU  - Gao, M.
AU  - Jiang, X.
AU  - Xu, K.
AU  - Wang, L.
TI  - Personalized Graph Neural Networks With Attention Mechanism for Session-Aware Recommendation
PY  - 2022
T2  - IEEE Transactions on Knowledge and Data Engineering
VL  - 34
IS  - 8
SP  - 3946
EP  - 3957
DO  - 10.1109/TKDE.2020.3031329
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095986918&doi=10.1109%2fTKDE.2020.3031329&partnerID=40&md5=29c8c0fe9807286d6724e7a1801c07ee
AB  - The problem of session-aware recommendation aims to predict users' next click based on their current session and historical sessions. Existing session-aware recommendation methods have defects in capturing complex item transition relationships. Other than that, most of them fail to explicitly distinguish the effects of different historical sessions on the current session. To this end, we propose a novel method, named Personalized Graph Neural Networks with Attention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two components: one is Personalized Graph Neural Network (PGNN), which is used to extract the personalized structural information in each user behavior graph, compared with the traditional Graph Neural Network (GNN) model, which considers the role of the user when the node embedding is updated. The other is Dot-Product Attention mechanism, which draws on the Transformer net to explicitly model the effect of historical sessions on the current session. Extensive experiments conducted on two real-world data sets show that A-PGNN evidently outperforms the state-of-the-art personalized session-aware recommendation methods.  © 1989-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 77
C2  - CCF:A期刊; FMS:B; 
LB  - Zhang2022Personalized
ER  -

TY  - JOUR
AU  - Liang, D.
AU  - Chen, X.
AU  - Xu, W.
AU  - Zhou, Y.
AU  - Bai, X.
TI  - TransCrowd: weakly-supervised crowd counting with transformers
PY  - 2022
T2  - Science China Information Sciences
VL  - 65
IS  - 6
C7  - 160104
DO  - 10.1007/s11432-021-3445-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129643241&doi=10.1007%2fs11432-021-3445-y&partnerID=40&md5=fdb4ab43bc95eb8adfcf9bed5603014e
AB  - The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods. © 2022, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 145
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhang, W.
AU  - Chen, Z.
AU  - Zha, H.
AU  - Wang, J.
TI  - Learning from Substitutable and Complementary Relations for Graph-based Sequential Product Recommendation
PY  - 2022
T2  - ACM Transactions on Information Systems
VL  - 40
IS  - 2
C7  - 3464302
DO  - 10.1145/3464302
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124071771&doi=10.1145%2f3464302&partnerID=40&md5=4ac6dfb860ed58541c73c305993c5fba
AB  - Sequential product recommendation, aiming at predicting the products that a target user will interact with soon, has become a hotspot topic. Most of the sequential recommendation models focus on learning from users' interacted product sequences in a purely data-driven manner. However, they largely overlook the knowledgeable substitutable and complementary relations between products. To address this issue, we propose a novel Substitutable and Complementary Graph-based Sequential Product Recommendation model, namely, SCG-SPRe. The innovations of SCG-SPRe lie in its two main modules: (1) The module of interactive graph neural networks jointly encodes the high-order product correlations in the substitutable graph and the complementary graph into two types of relation-specific product representations. (2) The module of kernel-enhanced transformer networks adaptively fuses multiple temporal kernels to characterize the unique temporal patterns between a candidate product to be recommended and any interacted product in a target behavior sequence. Thanks to the seamless integration of the two modules, SCG-SPRe obtains candidate-dependent user representations for different candidate products to compute the corresponding ranking scores. We conduct extensive experiments on three public datasets, demonstrating SCG-SPRe is superior to competitive sequential recommendation baselines and validating the benefits of explicitly modeling the product-product relations.  © 2021 Association for Computing Machinery.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 19
C2  - CCF:A期刊; FMS:B; 
LB  - Zhang2022Learning
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Qi, G.
AU  - Li, S.
AU  - Chai, Y.
AU  - Li, H.
TI  - Body Part-Level Domain Alignment for Domain-Adaptive Person Re-Identification With Transformer Framework
PY  - 2022
T2  - IEEE Transactions on Information Forensics and Security
VL  - 17
SP  - 3321
EP  - 3334
DO  - 10.1109/TIFS.2022.3207893
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139418888&doi=10.1109%2fTIFS.2022.3207893&partnerID=40&md5=5a63efc9de7d39907135947bbe251ad4
AB  - Although existing domain-adaptive person re-identification (re-ID) methods have achieved competitive per- formance, most of them highly rely on the reliability of pseudo-label prediction, which seriously limits their applicability as noisy labels cannot be avoided. This paper designs a Transformer framework based on body part-level domain alignment to solve the above-mentioned issues in domain-adaptive person re-ID. Different parts of the human body (such as head, torso, and legs) have different structures and shapes. Therefore, they usually exhibit different characteristics. The proposed method makes full use of the dissimilarity between different human body parts. Specifically, the local features from the same body part are aggregated by the Transformer to obtain the corresponding class token, which is used as the global representation of this body part. Additionally, a Transformer layer-embedded adversarial learning strategy is designed. This strategy can simultaneously achieve domain alignment and classification of the class token for each human body part in both target and source domains by an integrated discriminator, thereby realizing domain alignment at human body part level. Compared with existing domain-level and identity-level alignment methods, the proposed method has a stronger fine-grained domain alignment capability. Therefore, the information loss or distortion that may occur in the feature alignment process can be effectively alleviated. The proposed method does not need to predict pseudo labels of any target sample, so the negative impact caused by unreliable pseudo labels on re-ID performance can be effectively avoided. Compared with state-of-the-art methods, the proposed method achieves better performance on the datasets that are in line with real-world scene settings.  © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 16
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Zhao, X.
AU  - Pang, Y.
AU  - Zhang, L.
AU  - Lu, H.
TI  - Joint Learning of Salient Object Detection, Depth Estimation and Contour Extraction
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 7350
EP  - 7362
DO  - 10.1109/TIP.2022.3222641
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143180166&doi=10.1109%2fTIP.2022.3222641&partnerID=40&md5=04300111227f763a030255a406819ec3
AB  - Benefiting from color independence, illumination invariance and location discrimination attributed by the depth map, it can provide important supplemental information for extracting salient objects in complex environments. However, high-quality depth sensors are expensive and can not be widely applied. While general depth sensors produce the noisy and sparse depth information, which brings the depth-based networks with irreversible interference. In this paper, we propose a novel multi-task and multi-modal filtered transformer (MMFT) network for RGB-D salient object detection (SOD). Specifically, we unify three complementary tasks: depth estimation, salient object detection and contour estimation. The multi-task mechanism promotes the model to learn the task-aware features from the auxiliary tasks. In this way, the depth information can be completed and purified. Moreover, we introduce a multi-modal filtered transformer (MFT) module, which equips with three modality-specific filters to generate the transformer-enhanced feature for each modality. The proposed model works in a depth-free style during the testing phase. Experiments show that it not only significantly surpasses the depth-based RGB-D SOD methods on multiple datasets, but also precisely predicts a high-quality depth map and salient contour at the same time. And, the resulted depth map can help existing RGB-D SOD methods obtain significant performance gain.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 15
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Li, Y.
AU  - Tu, Y.
AU  - Chen, X.
AU  - Zhao, H.
AU  - Zhou, G.
TI  - Distance-Aware Occlusion Detection with Focused Attention
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 5661
EP  - 5676
DO  - 10.1109/TIP.2022.3197984
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137607493&doi=10.1109%2fTIP.2022.3197984&partnerID=40&md5=55f242cdbe15a4a265368306d183d7ee
AB  - For humans, understanding the relationships between objects using visual signals is intuitive. For artificial intelligence, however, this task remains challenging. Researchers have made significant progress studying semantic relationship detection, such as human-object interaction detection and visual relationship detection. We take the study of visual relationships a step further from semantic to geometric. In specific, we predict relative occlusion and relative distance relationships. However, detecting these relationships from a single image is challenging. Enforcing focused attention to task-specific regions plays a critical role in successfully detecting these relationships. In this work, (1) we propose a novel three-decoder architecture as the infrastructure for focused attention; 2) we use the generalized intersection box prediction task to effectively guide our model to focus on occlusion-specific regions; 3) our model achieves a new state-of-the-art performance on distance-aware relationship detection. Specifically, our model increases the distance F1-score from 33.8% to 38.6% and boosts the occlusion F1-score from 34.4% to 41.2%. Our code is publicly available. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 5
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Ye, S.
AU  - Chen, D.
AU  - Han, S.
AU  - Liao, J.
TI  - 3D Question Answering
PY  - 2022
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 30
IS  - 3
SP  - 1772
EP  - 1786
DO  - 10.1109/TVCG.2022.3225327
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144017012&doi=10.1109%2fTVCG.2022.3225327&partnerID=40&md5=a4f916d07b36e8c929a220eaa20e8fa5
AB  - Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this article, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine’s perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework “3DQA-TR”, which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset “ScanQA”, which builds on the ScanNet dataset and contains over 10 K question-answer pairs for 806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated. We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. © 2022 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 6
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wang, Y.
AU  - Xie, H.
AU  - Fang, S.
AU  - Xing, M.
AU  - Wang, J.
AU  - Zhu, S.
AU  - Zhang, Y.
TI  - PETR: Rethinking the Capability of Transformer-Based Language Model in Scene Text Recognition
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 5585
EP  - 5598
DO  - 10.1109/TIP.2022.3197981
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137126269&doi=10.1109%2fTIP.2022.3197981&partnerID=40&md5=93e88cd3715e0f20ea12fbfe63f6f0ee
AB  - The exploration of linguistic information promotes the development of scene text recognition task. Benefiting from the significance in parallel reasoning and global relationship capture, transformer-based language model (TLM) has achieved dominant performance recently. As a decoupled structure from the recognition process, we argue that TLM's capability is limited by the input low-quality visual prediction. To be specific: 1) The visual prediction with low character-wise accuracy increases the correction burden of TLM. 2) The inconsistent word length between visual prediction and original image provides a wrong language modeling guidance in TLM. In this paper, we propose a Progressive scEne Text Recognizer (PETR) to improve the capability of transformer-based language model by handling above two problems. Firstly, a Destruction Learning Module (DLM) is proposed to consider the linguistic information in the visual context. DLM introduces the recognition of destructed images with disordered patches in the training stage. Through guiding the vision model to restore patch orders and make word-level prediction on the destructed images, visual prediction with high character-wise accuracy is obtained by exploring inner relationship between the local visual patches. Secondly, a new Language Rectification Module (LRM) is proposed to optimize the word length for language guidance rectification. Through progressively implementing LRM in different language modeling steps, a novel progressive rectification network is constructed to handle some extremely challenging cases (e.g. distortion, occlusion, etc.). By utilizing DLM and LRM, PETR enhances the capability of transformer-based language model from a more general aspect, that is, focusing on the reduction of correction burden and rectification of language modeling guidance. Compared with parallel transformer-based methods, PETR obtains 1.0% and 0.8% improvement on regular and irregular datasets respectively while introducing only 1.7M additional parameters. The extensive experiments on both English and Chinese benchmarks demonstrate that PETR achieves the state-of-the-art results.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 24
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Maiya, A.S.
TI  - ktrain: A Low-Code Library for Augmented Machine Learning
PY  - 2022
T2  - Journal of Machine Learning Research
VL  - 23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131438217&partnerID=40&md5=5459a02a11bbe4bc5a163d2585326c95
AB  - We present ktrain, a low-code Python library that makes machine learning more ac- cessible and easier to apply. As a wrapper to TensorFlow and many other libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to make sophis- ticated, state-of-the-art machine learning models simple to build, train, inspect, and apply by both beginners and experienced practitioners. Featuring modules that support text data (e.g., text classification, sequence tagging, open-domain question-answering), vision data (e.g., image classification), graph data (e.g., node classification, link prediction), and tabular data, ktrain presents a simple unified interface enabling one to quickly solve a wide range of tasks in as little as three or four "commands" or lines of code. © 2022 Microtome Publishing. All rights reserved.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 36
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Liu, X.
AU  - Wang, Q.
AU  - Hu, Y.
AU  - Tang, X.
AU  - Zhang, S.
AU  - Bai, S.
AU  - Bai, X.
TI  - End-to-End Temporal Action Detection With Transformer
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 5427
EP  - 5441
DO  - 10.1109/TIP.2022.3195321
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136107120&doi=10.1109%2fTIP.2022.3195321&partnerID=40&md5=f96b5e698d95c9460abdc2bf36f293ab
AB  - Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR. © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 140
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yu, H.-F.
AU  - Zhong, K.
AU  - Zhang, J.
AU  - Chang, W.-C.
AU  - Dhillon, I.S.
TI  - PECOS: Prediction for Enormous and Correlated Output Spaces
PY  - 2022
T2  - Journal of Machine Learning Research
VL  - 23
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128313329&partnerID=40&md5=99e326d333459d8f57dc8163ac4e3837
AB  - Many large-scale applications amount to finding relevant results from an enormous output space of potential candidates. For example, finding the best matching product from a large catalog or suggesting related search phrases on a search engine. The size of the output space for these problems can range from millions to billions, and can even be infinite in some applications. More- over, training data is often limited for the \long-tail"items in the output space. Fortunately, items in the output space are often correlated thereby presenting an opportunity to alleviate the data sparsity issue. In this paper, we propose the Prediction for Enormous and Correlated Out- put Spaces (PECOS) framework, a versatile and modular machine learning framework for solving prediction problems for very large output spaces, and apply it to the eXtreme Multilabel Rank- ing (XMR) problem: given an input instance, find and rank the most relevant items from an enormous but fixed and finite output space. We propose a three phase framework for PECOS: (i) in the first phase, PECOS organizes the output space using a semantic indexing scheme, (ii) in the second phase, PECOS uses the indexing to narrow down the output space by orders of mag- nitude using a machine learned matching scheme, and (iii) in the third phase, PECOS ranks the matched items using a final ranking scheme. The versatility and modularity of PECOS allows for easy plug-and-play of various choices for the indexing, matching, and ranking phases. The indexing and matching phases alleviate the data sparsity issue by leveraging correlations across different items in the output space. For the critical matching phase, we develop a recursive machine learned matching strategy with both linear and neural matchers. When applied to eXtreme Multilabel Ranking where the input instances are in textual form, we find that the recursive Transformer matcher gives state-of-the-art accuracy results, at the cost of two orders of magnitude increased training time compared to the recursive linear matcher. For example, on a dataset where the output space is of size 2.8 million, the recursive Transformer matcher results in a 6% increase in precision@1 (from 48.6% to 54.2%) over the recursive linear matcher but takes 100x more time to train. Thus it is up to the practitioner to evaluate the trade-offs and decide whether the increased training time and infrastructure cost is warranted for their application; indeed, the exibility of the PECOS framework seamlessly allows different strategies to be used. We also develop very fast inference procedures which allow us to perform XMR predictions in real time; for example, infer- ence takes less than 1 millisecond per input on the dataset with 2.8 million labels. The PECOS software is available at https://libpecos.org.  © 2022 Hsiang-Fu Yu, Kai Zhong, Jiong Zhang, Wei-Cheng Chang, Inderjit S. Dhillon.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 33
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Yang, K.
AU  - Gu, R.
AU  - Wang, M.
AU  - Toyoura, M.
AU  - Xu, G.
TI  - LASOR: Learning Accurate 3D Human Pose and Shape via Synthetic Occlusion-Aware Data and Neural Mesh Rendering
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 1938
EP  - 1948
DO  - 10.1109/TIP.2022.3149229
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124739989&doi=10.1109%2fTIP.2022.3149229&partnerID=40&md5=507f4b35cfebf29732896abca1d7cee6
AB  - A key challenge in the task of human pose and shape estimation is occlusion, including self-occlusions, object-human occlusions, and inter-person occlusions. The lack of diverse and accurate pose and shape training data becomes a major bottleneck, especially for scenes with occlusions in the wild. In this paper, we focus on the estimation of human pose and shape in the case of inter-person occlusions, while also handling object-human occlusions and self-occlusion. We propose a novel framework that synthesizes occlusion-aware silhouette and 2D keypoints data and directly regress to the SMPL pose and shape parameters. A neural 3D mesh renderer is exploited to enable silhouette supervision on the fly, which contributes to great improvements in shape estimation. In addition, keypoints-and-silhouette-driven training data in panoramic viewpoints are synthesized to compensate for the lack of viewpoint diversity in any existing dataset. Experimental results show that we are among the state-of-the-art on the 3DPW and 3DPW-Crowd datasets in terms of pose estimation accuracy. The proposed method evidently outperforms Mesh Transformer, 3DCrowdNet and ROMP in terms of shape estimation. Top performance is also achieved on SSP-3D in terms of shape prediction accuracy. Demo and code will be available at https://igame-lab.github.io/LASOR/.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Wu, L.
AU  - Yu, Z.
AU  - Liu, Y.
AU  - Liu, Q.
TI  - Limb Pose Aware Networks for Monocular 3D Pose Estimation
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 906
EP  - 917
DO  - 10.1109/TIP.2021.3136613
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122078796&doi=10.1109%2fTIP.2021.3136613&partnerID=40&md5=e79e972a0050633f6a8f9f828b953a29
AB  - In the task of monocular 3D pose estimation, the estimation errors of limb joints (i.e., wrist, ankle, etc) with a higher degree of freedom(DOF) are larger than that of others (i.e., hip, thorax, etc). Specifically, errors may accumulate along the physiological structure of human body parts, and trajectories of joints with higher DOF bring in higher complexity. To address this problem, we propose a limb pose aware framework, involving a kinematic constraint aware network as well as a trajectory aware temporal module, to improve the 3D prediction accuracy of limb joint positions. Two kinematic constraints named relative bone angles and absolute bone angles are introduced in this paper, the former being used for building the angular relation between adjacent bones and the latter for building the angular relation between bones and the camera plane. As a joint result of two constraints, our work suppresses errors accumulated along limbs. Furthermore, we propose a trajectory-aware network, named as Hierarchical Transformer, which takes temporal trajectories of joints as input and generates fused trajectory estimation as a result. The Hierarchical Transformer consists of Transformer Encoder blocks and aims at improving the performance of fusing temporal features. Under the effect of kinematic constraints and trajectory network, we alleviate the problem of errors accumulated along limbs and achieve promising results. Most of the off-the-shelf 2D pose estimators can be easily integrated into our framework. We perform extensive experiments on public datasets and validate the effectiveness of the framework. The ablation studies show the strength of each individual sub-module.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Mustar, A.
AU  - Lamprier, S.
AU  - Piwowarski, B.
TI  - On the Study of Transformers for Query Suggestion
PY  - 2022
T2  - ACM Transactions on Information Systems
VL  - 40
IS  - 1
C7  - 18
DO  - 10.1145/3470562
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123915257&doi=10.1145%2f3470562&partnerID=40&md5=fbd7f4e9a65c6fca3a969596f8af4918
AB  - When conducting a search task, users may find it difficult to articulate their need, even more so when the task is complex. To help them complete their search, search engine usually provide query suggestions. A good query suggestion system requires to model user behavior during the search session. In this article, we study multiple Transformer architectures applied to the query suggestion task and compare them with recurrent neural network (RNN)-based models. We experiment Transformer models with different tokenizers, with different Encoders (large pretrained models or fully trained ones), and with two kinds of architectures (flat or hierarchic). We study the performance and the behaviors of these various models, and observe that Transformer-based models outperform RNN-based ones. We show that while the hierarchical architectures exhibit very good performances for query suggestion, the flat models are more suitable for complex and long search tasks. Finally, we investigate the flat models behavior and demonstrate that they indeed learn to recover the hierarchy of a search session. © 2021 Association for Computing Machinery.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 19
C2  - CCF:A期刊; FMS:B; 
LB  - Mustar2022On
ER  -

TY  - JOUR
AU  - Liu, M.
AU  - Zhang, C.
AU  - Bai, H.
AU  - Zhang, R.
AU  - Zhao, Y.
TI  - Cross-Part Learning for Fine-Grained Image Classification
PY  - 2022
T2  - IEEE Transactions on Image Processing
VL  - 31
SP  - 748
EP  - 758
DO  - 10.1109/TIP.2021.3135477
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122105328&doi=10.1109%2fTIP.2021.3135477&partnerID=40&md5=af6aeeafb7f920fc5d4498b91b25abbb
AB  - Recent techniques have achieved remarkable improvements depended on mining subtle yet distinctive features for fine-grained visual classification (FGVC). While prior works directly combine discriminative features extracted from different parts, we argue that the potential interactions between different parts and their abilities to category predictions should be taken into consideration, which enables significant parts to contribute more to the decision of the sub-category. To this end, we present a Cross-Part Convolutional Neural Network (CP-CNN) in a weakly supervised manner to explore cross-learning among multi-regional features. Specifically, the context transformer is implemented to encourage joint feature learning across different parts under the guidance of a navigator. The part with the highest confidence is regarded as a navigator to deliver distinguishing characteristics to the others with lower confidence while the complementary information is retained. To locate discriminative but subtle parts precisely, a part proposal generator (PPG) is designed with the feature enhancement blocks, through which complex scale variations caused by the viewpoint diversity can be effectively alleviated. Extensive experiments on three benchmark datasets demonstrate that our proposed method consistently outperforms existing state-of-the-art methods.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 36
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Jaunet, T.
AU  - Kervadec, C.
AU  - Vuillemot, R.
AU  - Antipov, G.
AU  - Baccouche, M.
AU  - Wolf, C.
TI  - VisQA: X-raying Vision and Language Reasoning in Transformers
PY  - 2022
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 28
IS  - 1
SP  - 976
EP  - 986
DO  - 10.1109/TVCG.2021.3114683
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118623500&doi=10.1109%2fTVCG.2021.3114683&partnerID=40&md5=9d34e9277dc0cb35505fa7e87df90609
AB  - Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-The-Art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-The-Art neural models-attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users.  © 1995-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 15
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Kandukuri, R.K.
AU  - Achterhold, J.
AU  - Moeller, M.
AU  - Stueckler, J.
TI  - Physical Representation Learning and Parameter Identification from Video Using Differentiable Physics
PY  - 2022
T2  - International Journal of Computer Vision
VL  - 130
IS  - 1
SP  - 3
EP  - 16
DO  - 10.1007/s11263-021-01493-5
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117584714&doi=10.1007%2fs11263-021-01493-5&partnerID=40&md5=4195cbef82b6231525f3b0bbe652c466
AB  - Representation learning for video is increasingly gaining attention in the field of computer vision. For instance, video prediction models enable activity and scene forecasting or vision-based planning and control. In this article, we investigate the combination of differentiable physics and spatial transformers in a deep action conditional video representation network. By this combination our model learns a physically interpretable latent representation and can identify physical parameters. We propose supervised and self-supervised learning methods for our architecture. In experiments, we consider simulated scenarios with pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. We demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences. We evaluate the accuracy of our training methods, and demonstrate the ability of our method to predict future video frames from input images and actions. © 2021, The Author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 7
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Chen, Y.
AU  - Lin, L.
AU  - Li, B.
AU  - Wang, Q.
AU  - Zhang, Q.
TI  - Silhouette: Efficient Cloud Configuration Exploration for Large-Scale Analytics
PY  - 2021
T2  - IEEE Transactions on Parallel and Distributed Systems
VL  - 32
IS  - 8
C7  - 9351648
SP  - 2049
EP  - 2061
DO  - 10.1109/TPDS.2021.3058165
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101455050&doi=10.1109%2fTPDS.2021.3058165&partnerID=40&md5=db2f22d879756c1b9f04c894b4f9deaf
AB  - Choosing the best cloud configuration for large-scale data analytics jobs deployed in the cloud can substantially improve their performance and reduce costs. However, current cloud providers offer a wide variety of instance types and customized cluster sizes, making it both time-consuming and costly to pinpoint the optimal cloud configuration. This article presents the design, implementation, and evaluation of Silhouette, a cloud configuration selection framework based on performance models for various large-scale analytics jobs with minimal training overhead. The essence of Silhouette is to build performance prediction models with carefully selected small-scale experiments on small subsets of input data to estimate the performance with entire input data on larger cluster sizes. To reduce the training time and cost, Silhouette incorporates new statistical techniques to select those experiments that yield the best possible information for performance prediction. Moreover, we develop a novel model transformer to convert a prediction model built on one instance type to a different instance type with only one extra experiment, which significantly reduces the training overhead. We evaluate Silhouette with an extensive array of large-scale data analytics jobs on Amazon EC2. Our experimental results have shown convincing evidence that Silhouette is effective in optimizing cloud configuration while saving both training time and costs compared with existing solutions.  © 1990-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 10
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Saunders, B.
AU  - Camgoz, N.C.
AU  - Bowden, R.
TI  - Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks
PY  - 2021
T2  - International Journal of Computer Vision
VL  - 129
IS  - 7
SP  - 2113
EP  - 2135
DO  - 10.1007/s11263-021-01457-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105522926&doi=10.1007%2fs11263-021-01457-9&partnerID=40&md5=fe54ea63ba5607ce90f0ba7fa836c573
AB  - Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production. In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to produce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions. © 2021, The Author(s).
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 51
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Roy, D.
AU  - Fernando, B.
TI  - Action Anticipation Using Pairwise Human-Object Interactions and Transformers
PY  - 2021
T2  - IEEE Transactions on Image Processing
VL  - 30
SP  - 8116
EP  - 8129
DO  - 10.1109/TIP.2021.3113114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115670782&doi=10.1109%2fTIP.2021.3113114&partnerID=40&md5=416fdfc2a8a6456b8e64f51a0a4e9460
AB  - The ability to anticipate future actions of humans is useful in application areas such as automated driving, robot-assisted manufacturing, and smart homes. These applications require representing and anticipating human actions involving the use of objects. Existing methods that use human-object interactions for anticipation require object affordance labels for every relevant object in the scene that match the ongoing action. Hence, we propose to represent every pairwise human-object (HO) interaction using only their visual features. Next, we use cross-correlation to capture the second-order statistics across human-object pairs in a frame. Cross-correlation produces a holistic representation of the frame that can also handle a variable number of human-object pairs in every frame of the observation period. We show that cross-correlation based frame representation is more suited for action anticipation than attention-based and other second-order approaches. Furthermore, we observe that using a transformer model for temporal aggregation of frame-wise HO representations results in better action anticipation than other temporal networks. So, we propose two approaches for constructing an end-to-end trainable multi-modal transformer (MM-Transformer; code at https://github.com/debadityaroy/MM-Transformer_ActAnt) model that combines the evidence across spatio-temporal, motion, and HO representations. We show the performance of MM-Transformer on procedural datasets like 50 Salads and Breakfast, and an unscripted dataset like EPIC-KITCHENS55. Finally, we demonstrate that the combination of human-object representation and MM-Transformers is effective even for long-term anticipation.  © 1992-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 24
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Sadeghzadeh, A.M.
AU  - Tajali, B.
AU  - Jalili, R.
TI  - AWA: Adversarial Website Adaptation
PY  - 2021
T2  - IEEE Transactions on Information Forensics and Security
VL  - 16
C7  - 9408630
SP  - 3109
EP  - 3122
DO  - 10.1109/TIFS.2021.3074295
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104621509&doi=10.1109%2fTIFS.2021.3074295&partnerID=40&md5=b41c067bdefe8e0d25cf85c9460e5339
AB  - One of the most important obligations of privacy-enhancing technologies is to bring confidentiality and privacy to users' browsing activities on the Internet. The website fingerprinting attack enables a local passive eavesdropper to predict the target user's browsing activities even she uses anonymous technologies, such as VPNs, IPsec, and Tor. Recently, the growth of deep learning empowers adversaries to conduct the website fingerprinting attack with higher accuracy. In this paper, we propose a new defense against website fingerprinting attack using adversarial deep learning approaches called Adversarial Website Adaptation (AWA). AWA creates a transformer set in each run so that each website has a unique transformer. Each transformer generates adversarial traces to evade the adversary's classifier. AWA has two versions, including Universal AWA (UAWA) and Non-Universal AWA (NUAWA). Unlike NUAWA, there is no need to access the entire trace of a website in order to generate an adversarial trace in UAWA. We accommodate secret random elements in the training phase of transformers in order for AWA to generate various sets of transformers in each run. We run AWA several times and create multiple sets of transformers. If an adversary and a target user select different sets of transformers, the accuracy of adversary's classifier is almost 19.52% and 31.94% with almost 22.28% and 26.28% bandwidth overhead in UAWA and NUAWA, respectively. If a more powerful adversary generates adversarial traces through multiple sets of transformers and trains a classifier on them, the accuracy of adversary's classifier is almost 49.10% and 25.93% with almost 62.52% and 64.33% bandwidth overhead in UAWA and NUAW, respectively. © 2005-2012 IEEE.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 19
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Su, J.
AU  - Zhang, X.
AU  - Lin, Q.
AU  - Qin, Y.
AU  - Yao, J.
AU  - Liu, Y.
TI  - Exploiting reverse target-side contexts for neural machine translation via asynchronous bidirectional decoding
PY  - 2019
T2  - Artificial Intelligence
VL  - 277
C7  - 103168
DO  - 10.1016/j.artint.2019.103168
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071920553&doi=10.1016%2fj.artint.2019.103168&partnerID=40&md5=2f574f3b83c26558e85f2595b2b8226d
AB  - Based on a unified encoder-decoder framework with attentional mechanism, neural machine translation (NMT) models have attracted much attention and become the mainstream in the community of machine translation. Generally, the NMT decoders produce translation in a left-to-right way. As a result, only left-to-right target-side contexts from the generated translations are exploited, while the right-to-left target-side contexts are completely unexploited for translation. In this paper, we extend the conventional attentional encoder-decoder NMT framework by introducing a backward decoder, in order to explore asynchronous bidirectional decoding for NMT. In the first step after encoding, our backward decoder learns to generate the target-side hidden states in a right-to-left manner. Next, in each timestep of translation prediction, our forward decoder concurrently considers both the source-side and the reverse target-side hidden states via two attention models. Compared with previous models, the innovation in this architecture enables our model to fully exploit contexts from both source side and target side, which improve translation quality altogether. We conducted experiments on NIST Chinese-English, WMT English-German and Finnish-English translation tasks to investigate the effectiveness of our model. Experimental results show that (1) our improved RNN-based NMT model achieves significant improvements over the conventional RNNSearch by 1.44/-3.02, 1.11/-1.01, and 1.23/-1.27 average BLEU and TER points, respectively; and (2) our enhanced Transformer outperforms the standard Transformer by 1.56/-1.49, 1.76/-2.49, and 1.29/-1.33 average BLEU and TER points, respectively. We released our code at https://github.com/DeepLearnXMU/ABD-NMT. © 2019 Elsevier B.V.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 34
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Blocker, W.
TI  - The Behavior of the Wide-Band Transmission-Line Transformer for Nonoptimum Line Impedance
PY  - 1978
T2  - Proceedings of the IEEE
VL  - 66
IS  - 4
SP  - 518
EP  - 519
DO  - 10.1109/PROC.1978.10948
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017955730&doi=10.1109%2fPROC.1978.10948&partnerID=40&md5=3d66c20795cd486d3d68bb891ed6ae60
AB  - The transmission-line transformer can perfectly match at a single frequency any resistance ratio greater than or equal to four. This requires an optimum choice for the impedance and length of the line. For short lines, the transformer behavior is insensitive to small variations from optimum of the line impedance. A simple easily applied theory predicts the transformer behavior as a function of line length, the ratio of resistances to be matched, and the departure of line impedance from the optimum value. Copyright © 1978 by The Institute of Electrical and Electronics Engineers, Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 12
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Allen, C.W.
AU  - Krauss, H.L.
TI  - Wide-Band Rotary Transformer—Unbalanced Current Analysis
PY  - 1977
T2  - Proceedings of the IEEE
VL  - 65
IS  - 2
SP  - 200
EP  - 206
DO  - 10.1109/PROC.1977.10457
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017455943&doi=10.1109%2fPROC.1977.10457&partnerID=40&md5=5bf48e5aff8a9264c1b768a73cd0a58d
AB  - Performance of a wide-band rotary transformer with approximately 1000:1 bandwidth (80 kHz–80 MHz) is described. A previous paper [1] gave theoretical analysis and experimental results based upon lossless balanced transmission line theory. The paper extends the results to the case of a shielded transformer, and uses a combination of balanced and unbalanced line theory to account for losses and predict frequency response. Theoretical results are confirmed by data measured on a prototype coupler. Copyright © 1977 by The Institute of Electrical and Electronics Engineers, Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 2
C2  - CCF:A期刊; 
ER  -

TY  - JOUR
AU  - Pitzalis, O.
AU  - Couse, T.P.M.
TI  - Practical Design Information for Broadband Transmission Line Transformers
PY  - 1968
T2  - Proceedings of the IEEE
VL  - 56
IS  - 4
SP  - 738
EP  - 739
DO  - 10.1109/PROC.1968.6362
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042611486&doi=10.1109%2fPROC.1968.6362&partnerID=40&md5=5dc787ddd49212481c0febb803fc381b
AB  - Normalized design curves useful in constructing broadband, 4-to-1 impedance matching, transmission line transformers of predictable performance are presented. Insertion loss plus the magnitude and phase angle of input impedance as functions of the length and characteristic impedance of the transmission line are included. Measurements demonstrate close agreement between measured and calculated performance. Copyright © 1968 by The Institute of Electrical and Electronics Engineers, Inc.
M3  - Article
DB  - Scopus
N1  - Export Date: 12 January 2025; Cited By: 23
C2  - CCF:A期刊; 
ER  -
